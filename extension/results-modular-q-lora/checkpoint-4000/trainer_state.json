{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.4194464158978,
  "eval_steps": 500,
  "global_step": 4000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0035486160397444995,
      "grad_norm": 49.820594787597656,
      "learning_rate": 9.995741660752308e-05,
      "loss": 31.8977,
      "step": 10
    },
    {
      "epoch": 0.007097232079488999,
      "grad_norm": 104.5453109741211,
      "learning_rate": 9.988644428672817e-05,
      "loss": 29.8361,
      "step": 20
    },
    {
      "epoch": 0.0106458481192335,
      "grad_norm": 133.81088256835938,
      "learning_rate": 9.981547196593329e-05,
      "loss": 24.4801,
      "step": 30
    },
    {
      "epoch": 0.014194464158977998,
      "grad_norm": 120.28369140625,
      "learning_rate": 9.97444996451384e-05,
      "loss": 17.7755,
      "step": 40
    },
    {
      "epoch": 0.017743080198722498,
      "grad_norm": 76.7419204711914,
      "learning_rate": 9.967352732434351e-05,
      "loss": 12.0278,
      "step": 50
    },
    {
      "epoch": 0.021291696238467,
      "grad_norm": 24.76036834716797,
      "learning_rate": 9.960255500354863e-05,
      "loss": 8.6561,
      "step": 60
    },
    {
      "epoch": 0.0248403122782115,
      "grad_norm": 13.155250549316406,
      "learning_rate": 9.953158268275374e-05,
      "loss": 7.4672,
      "step": 70
    },
    {
      "epoch": 0.028388928317955996,
      "grad_norm": 10.20486831665039,
      "learning_rate": 9.946061036195884e-05,
      "loss": 6.8697,
      "step": 80
    },
    {
      "epoch": 0.0319375443577005,
      "grad_norm": 9.68937873840332,
      "learning_rate": 9.938963804116395e-05,
      "loss": 6.4646,
      "step": 90
    },
    {
      "epoch": 0.035486160397444996,
      "grad_norm": 8.786203384399414,
      "learning_rate": 9.931866572036906e-05,
      "loss": 6.2636,
      "step": 100
    },
    {
      "epoch": 0.03903477643718949,
      "grad_norm": 8.477919578552246,
      "learning_rate": 9.924769339957418e-05,
      "loss": 5.9974,
      "step": 110
    },
    {
      "epoch": 0.042583392476934,
      "grad_norm": 8.365031242370605,
      "learning_rate": 9.917672107877927e-05,
      "loss": 5.7995,
      "step": 120
    },
    {
      "epoch": 0.046132008516678494,
      "grad_norm": 8.463397979736328,
      "learning_rate": 9.91057487579844e-05,
      "loss": 5.6183,
      "step": 130
    },
    {
      "epoch": 0.049680624556423,
      "grad_norm": 8.896711349487305,
      "learning_rate": 9.90347764371895e-05,
      "loss": 5.4603,
      "step": 140
    },
    {
      "epoch": 0.053229240596167494,
      "grad_norm": 8.272623062133789,
      "learning_rate": 9.896380411639461e-05,
      "loss": 5.442,
      "step": 150
    },
    {
      "epoch": 0.05677785663591199,
      "grad_norm": 9.03061580657959,
      "learning_rate": 9.889283179559971e-05,
      "loss": 5.224,
      "step": 160
    },
    {
      "epoch": 0.060326472675656495,
      "grad_norm": 8.912149429321289,
      "learning_rate": 9.882185947480484e-05,
      "loss": 5.2716,
      "step": 170
    },
    {
      "epoch": 0.063875088715401,
      "grad_norm": 8.358516693115234,
      "learning_rate": 9.875088715400994e-05,
      "loss": 5.0584,
      "step": 180
    },
    {
      "epoch": 0.06742370475514549,
      "grad_norm": 8.286345481872559,
      "learning_rate": 9.867991483321505e-05,
      "loss": 5.1561,
      "step": 190
    },
    {
      "epoch": 0.07097232079488999,
      "grad_norm": 9.325026512145996,
      "learning_rate": 9.860894251242016e-05,
      "loss": 4.9547,
      "step": 200
    },
    {
      "epoch": 0.0745209368346345,
      "grad_norm": 11.373086929321289,
      "learning_rate": 9.853797019162528e-05,
      "loss": 4.8545,
      "step": 210
    },
    {
      "epoch": 0.07806955287437899,
      "grad_norm": 9.541234970092773,
      "learning_rate": 9.846699787083037e-05,
      "loss": 4.878,
      "step": 220
    },
    {
      "epoch": 0.08161816891412349,
      "grad_norm": 9.412773132324219,
      "learning_rate": 9.839602555003549e-05,
      "loss": 4.8196,
      "step": 230
    },
    {
      "epoch": 0.085166784953868,
      "grad_norm": 8.143945693969727,
      "learning_rate": 9.83250532292406e-05,
      "loss": 4.7467,
      "step": 240
    },
    {
      "epoch": 0.0887154009936125,
      "grad_norm": 8.767878532409668,
      "learning_rate": 9.825408090844571e-05,
      "loss": 4.7758,
      "step": 250
    },
    {
      "epoch": 0.09226401703335699,
      "grad_norm": 10.330875396728516,
      "learning_rate": 9.818310858765081e-05,
      "loss": 4.75,
      "step": 260
    },
    {
      "epoch": 0.09581263307310149,
      "grad_norm": 8.680668830871582,
      "learning_rate": 9.811213626685594e-05,
      "loss": 4.6385,
      "step": 270
    },
    {
      "epoch": 0.099361249112846,
      "grad_norm": 8.899171829223633,
      "learning_rate": 9.804116394606104e-05,
      "loss": 4.6223,
      "step": 280
    },
    {
      "epoch": 0.10290986515259049,
      "grad_norm": 9.287422180175781,
      "learning_rate": 9.797019162526615e-05,
      "loss": 4.6192,
      "step": 290
    },
    {
      "epoch": 0.10645848119233499,
      "grad_norm": 9.550580978393555,
      "learning_rate": 9.789921930447126e-05,
      "loss": 4.6545,
      "step": 300
    },
    {
      "epoch": 0.11000709723207949,
      "grad_norm": 8.916820526123047,
      "learning_rate": 9.782824698367638e-05,
      "loss": 4.6067,
      "step": 310
    },
    {
      "epoch": 0.11355571327182398,
      "grad_norm": 10.422745704650879,
      "learning_rate": 9.775727466288149e-05,
      "loss": 4.5204,
      "step": 320
    },
    {
      "epoch": 0.11710432931156849,
      "grad_norm": 9.271795272827148,
      "learning_rate": 9.768630234208659e-05,
      "loss": 4.4858,
      "step": 330
    },
    {
      "epoch": 0.12065294535131299,
      "grad_norm": 8.60873794555664,
      "learning_rate": 9.76153300212917e-05,
      "loss": 4.4641,
      "step": 340
    },
    {
      "epoch": 0.1242015613910575,
      "grad_norm": 9.331103324890137,
      "learning_rate": 9.754435770049681e-05,
      "loss": 4.4977,
      "step": 350
    },
    {
      "epoch": 0.127750177430802,
      "grad_norm": 8.66601848602295,
      "learning_rate": 9.747338537970193e-05,
      "loss": 4.5007,
      "step": 360
    },
    {
      "epoch": 0.1312987934705465,
      "grad_norm": 9.339489936828613,
      "learning_rate": 9.740241305890702e-05,
      "loss": 4.4105,
      "step": 370
    },
    {
      "epoch": 0.13484740951029098,
      "grad_norm": 9.81830883026123,
      "learning_rate": 9.733144073811215e-05,
      "loss": 4.4346,
      "step": 380
    },
    {
      "epoch": 0.1383960255500355,
      "grad_norm": 10.404163360595703,
      "learning_rate": 9.726046841731725e-05,
      "loss": 4.4607,
      "step": 390
    },
    {
      "epoch": 0.14194464158977999,
      "grad_norm": 9.67337417602539,
      "learning_rate": 9.718949609652236e-05,
      "loss": 4.4175,
      "step": 400
    },
    {
      "epoch": 0.14549325762952448,
      "grad_norm": 9.737940788269043,
      "learning_rate": 9.711852377572746e-05,
      "loss": 4.3236,
      "step": 410
    },
    {
      "epoch": 0.149041873669269,
      "grad_norm": 8.434759140014648,
      "learning_rate": 9.704755145493259e-05,
      "loss": 4.3178,
      "step": 420
    },
    {
      "epoch": 0.15259048970901348,
      "grad_norm": 9.98868179321289,
      "learning_rate": 9.697657913413769e-05,
      "loss": 4.2753,
      "step": 430
    },
    {
      "epoch": 0.15613910574875797,
      "grad_norm": 11.436307907104492,
      "learning_rate": 9.69056068133428e-05,
      "loss": 4.2968,
      "step": 440
    },
    {
      "epoch": 0.1596877217885025,
      "grad_norm": 9.58381462097168,
      "learning_rate": 9.683463449254791e-05,
      "loss": 4.2659,
      "step": 450
    },
    {
      "epoch": 0.16323633782824698,
      "grad_norm": 10.08227825164795,
      "learning_rate": 9.676366217175303e-05,
      "loss": 4.2434,
      "step": 460
    },
    {
      "epoch": 0.16678495386799147,
      "grad_norm": 10.14919662475586,
      "learning_rate": 9.669268985095812e-05,
      "loss": 4.2526,
      "step": 470
    },
    {
      "epoch": 0.170333569907736,
      "grad_norm": 9.242413520812988,
      "learning_rate": 9.662171753016324e-05,
      "loss": 4.1747,
      "step": 480
    },
    {
      "epoch": 0.17388218594748048,
      "grad_norm": 8.693549156188965,
      "learning_rate": 9.655074520936835e-05,
      "loss": 4.1397,
      "step": 490
    },
    {
      "epoch": 0.177430801987225,
      "grad_norm": 10.974892616271973,
      "learning_rate": 9.647977288857346e-05,
      "loss": 4.2258,
      "step": 500
    },
    {
      "epoch": 0.18097941802696949,
      "grad_norm": 11.753127098083496,
      "learning_rate": 9.640880056777856e-05,
      "loss": 4.2055,
      "step": 510
    },
    {
      "epoch": 0.18452803406671398,
      "grad_norm": 9.41101360321045,
      "learning_rate": 9.633782824698369e-05,
      "loss": 4.1823,
      "step": 520
    },
    {
      "epoch": 0.1880766501064585,
      "grad_norm": 8.632255554199219,
      "learning_rate": 9.626685592618879e-05,
      "loss": 4.2209,
      "step": 530
    },
    {
      "epoch": 0.19162526614620298,
      "grad_norm": 10.161458969116211,
      "learning_rate": 9.61958836053939e-05,
      "loss": 4.1598,
      "step": 540
    },
    {
      "epoch": 0.19517388218594747,
      "grad_norm": 9.282312393188477,
      "learning_rate": 9.6124911284599e-05,
      "loss": 4.1434,
      "step": 550
    },
    {
      "epoch": 0.198722498225692,
      "grad_norm": 9.871757507324219,
      "learning_rate": 9.605393896380413e-05,
      "loss": 4.1155,
      "step": 560
    },
    {
      "epoch": 0.20227111426543648,
      "grad_norm": 10.641263008117676,
      "learning_rate": 9.598296664300923e-05,
      "loss": 4.1617,
      "step": 570
    },
    {
      "epoch": 0.20581973030518097,
      "grad_norm": 9.784392356872559,
      "learning_rate": 9.591199432221434e-05,
      "loss": 4.1185,
      "step": 580
    },
    {
      "epoch": 0.2093683463449255,
      "grad_norm": 8.996521949768066,
      "learning_rate": 9.584102200141945e-05,
      "loss": 4.154,
      "step": 590
    },
    {
      "epoch": 0.21291696238466998,
      "grad_norm": 12.114105224609375,
      "learning_rate": 9.577004968062456e-05,
      "loss": 4.1088,
      "step": 600
    },
    {
      "epoch": 0.21646557842441447,
      "grad_norm": 9.230057716369629,
      "learning_rate": 9.569907735982968e-05,
      "loss": 4.03,
      "step": 610
    },
    {
      "epoch": 0.22001419446415899,
      "grad_norm": 10.539926528930664,
      "learning_rate": 9.562810503903478e-05,
      "loss": 4.1057,
      "step": 620
    },
    {
      "epoch": 0.22356281050390348,
      "grad_norm": 9.394351959228516,
      "learning_rate": 9.555713271823989e-05,
      "loss": 3.9932,
      "step": 630
    },
    {
      "epoch": 0.22711142654364797,
      "grad_norm": 10.608275413513184,
      "learning_rate": 9.5486160397445e-05,
      "loss": 3.9786,
      "step": 640
    },
    {
      "epoch": 0.23066004258339248,
      "grad_norm": 9.45712947845459,
      "learning_rate": 9.541518807665011e-05,
      "loss": 3.9839,
      "step": 650
    },
    {
      "epoch": 0.23420865862313697,
      "grad_norm": 17.084213256835938,
      "learning_rate": 9.534421575585523e-05,
      "loss": 4.0158,
      "step": 660
    },
    {
      "epoch": 0.23775727466288146,
      "grad_norm": 10.009013175964355,
      "learning_rate": 9.527324343506034e-05,
      "loss": 3.9791,
      "step": 670
    },
    {
      "epoch": 0.24130589070262598,
      "grad_norm": 11.057402610778809,
      "learning_rate": 9.520227111426544e-05,
      "loss": 4.1293,
      "step": 680
    },
    {
      "epoch": 0.24485450674237047,
      "grad_norm": 8.64504337310791,
      "learning_rate": 9.513129879347055e-05,
      "loss": 4.0569,
      "step": 690
    },
    {
      "epoch": 0.248403122782115,
      "grad_norm": 11.240360260009766,
      "learning_rate": 9.506032647267566e-05,
      "loss": 4.0116,
      "step": 700
    },
    {
      "epoch": 0.25195173882185945,
      "grad_norm": 9.899913787841797,
      "learning_rate": 9.498935415188078e-05,
      "loss": 3.995,
      "step": 710
    },
    {
      "epoch": 0.255500354861604,
      "grad_norm": 10.170137405395508,
      "learning_rate": 9.491838183108588e-05,
      "loss": 3.9769,
      "step": 720
    },
    {
      "epoch": 0.2590489709013485,
      "grad_norm": 9.418708801269531,
      "learning_rate": 9.4847409510291e-05,
      "loss": 3.9242,
      "step": 730
    },
    {
      "epoch": 0.262597586941093,
      "grad_norm": 12.203594207763672,
      "learning_rate": 9.47764371894961e-05,
      "loss": 3.9857,
      "step": 740
    },
    {
      "epoch": 0.26614620298083747,
      "grad_norm": 10.088257789611816,
      "learning_rate": 9.470546486870121e-05,
      "loss": 3.9248,
      "step": 750
    },
    {
      "epoch": 0.26969481902058196,
      "grad_norm": 10.564580917358398,
      "learning_rate": 9.463449254790631e-05,
      "loss": 3.936,
      "step": 760
    },
    {
      "epoch": 0.27324343506032645,
      "grad_norm": 10.848008155822754,
      "learning_rate": 9.456352022711144e-05,
      "loss": 3.9179,
      "step": 770
    },
    {
      "epoch": 0.276792051100071,
      "grad_norm": 8.732433319091797,
      "learning_rate": 9.449254790631654e-05,
      "loss": 3.901,
      "step": 780
    },
    {
      "epoch": 0.2803406671398155,
      "grad_norm": 8.91549301147461,
      "learning_rate": 9.442157558552165e-05,
      "loss": 3.9927,
      "step": 790
    },
    {
      "epoch": 0.28388928317955997,
      "grad_norm": 9.885470390319824,
      "learning_rate": 9.435060326472675e-05,
      "loss": 3.9938,
      "step": 800
    },
    {
      "epoch": 0.28743789921930446,
      "grad_norm": 10.008208274841309,
      "learning_rate": 9.427963094393188e-05,
      "loss": 3.9722,
      "step": 810
    },
    {
      "epoch": 0.29098651525904895,
      "grad_norm": 8.625581741333008,
      "learning_rate": 9.420865862313698e-05,
      "loss": 3.9298,
      "step": 820
    },
    {
      "epoch": 0.2945351312987935,
      "grad_norm": 8.445854187011719,
      "learning_rate": 9.413768630234209e-05,
      "loss": 3.8975,
      "step": 830
    },
    {
      "epoch": 0.298083747338538,
      "grad_norm": 9.028387069702148,
      "learning_rate": 9.40667139815472e-05,
      "loss": 3.9093,
      "step": 840
    },
    {
      "epoch": 0.3016323633782825,
      "grad_norm": 10.061285018920898,
      "learning_rate": 9.399574166075231e-05,
      "loss": 3.9217,
      "step": 850
    },
    {
      "epoch": 0.30518097941802697,
      "grad_norm": 8.8305025100708,
      "learning_rate": 9.392476933995741e-05,
      "loss": 3.9481,
      "step": 860
    },
    {
      "epoch": 0.30872959545777146,
      "grad_norm": 11.105745315551758,
      "learning_rate": 9.385379701916253e-05,
      "loss": 3.8412,
      "step": 870
    },
    {
      "epoch": 0.31227821149751595,
      "grad_norm": 11.35622787475586,
      "learning_rate": 9.378282469836764e-05,
      "loss": 3.8258,
      "step": 880
    },
    {
      "epoch": 0.3158268275372605,
      "grad_norm": 10.460558891296387,
      "learning_rate": 9.371185237757275e-05,
      "loss": 3.8824,
      "step": 890
    },
    {
      "epoch": 0.319375443577005,
      "grad_norm": 11.424091339111328,
      "learning_rate": 9.364088005677786e-05,
      "loss": 3.7953,
      "step": 900
    },
    {
      "epoch": 0.32292405961674947,
      "grad_norm": 10.081424713134766,
      "learning_rate": 9.356990773598298e-05,
      "loss": 3.8465,
      "step": 910
    },
    {
      "epoch": 0.32647267565649396,
      "grad_norm": 9.729828834533691,
      "learning_rate": 9.349893541518809e-05,
      "loss": 3.7726,
      "step": 920
    },
    {
      "epoch": 0.33002129169623845,
      "grad_norm": 9.357264518737793,
      "learning_rate": 9.342796309439319e-05,
      "loss": 3.817,
      "step": 930
    },
    {
      "epoch": 0.33356990773598294,
      "grad_norm": 10.77834415435791,
      "learning_rate": 9.33569907735983e-05,
      "loss": 3.8455,
      "step": 940
    },
    {
      "epoch": 0.3371185237757275,
      "grad_norm": 10.135865211486816,
      "learning_rate": 9.328601845280341e-05,
      "loss": 3.804,
      "step": 950
    },
    {
      "epoch": 0.340667139815472,
      "grad_norm": 10.824522018432617,
      "learning_rate": 9.321504613200853e-05,
      "loss": 3.8531,
      "step": 960
    },
    {
      "epoch": 0.34421575585521647,
      "grad_norm": 9.854072570800781,
      "learning_rate": 9.314407381121363e-05,
      "loss": 3.9208,
      "step": 970
    },
    {
      "epoch": 0.34776437189496096,
      "grad_norm": 9.540932655334473,
      "learning_rate": 9.307310149041875e-05,
      "loss": 3.895,
      "step": 980
    },
    {
      "epoch": 0.35131298793470545,
      "grad_norm": 9.885319709777832,
      "learning_rate": 9.300212916962385e-05,
      "loss": 3.8587,
      "step": 990
    },
    {
      "epoch": 0.35486160397445,
      "grad_norm": 9.055562973022461,
      "learning_rate": 9.293115684882896e-05,
      "loss": 3.7791,
      "step": 1000
    },
    {
      "epoch": 0.3584102200141945,
      "grad_norm": 11.786453247070312,
      "learning_rate": 9.286018452803406e-05,
      "loss": 3.8165,
      "step": 1010
    },
    {
      "epoch": 0.36195883605393897,
      "grad_norm": 10.346324920654297,
      "learning_rate": 9.278921220723919e-05,
      "loss": 3.7899,
      "step": 1020
    },
    {
      "epoch": 0.36550745209368346,
      "grad_norm": 9.058032035827637,
      "learning_rate": 9.271823988644429e-05,
      "loss": 3.7651,
      "step": 1030
    },
    {
      "epoch": 0.36905606813342795,
      "grad_norm": 9.66750717163086,
      "learning_rate": 9.26472675656494e-05,
      "loss": 3.8605,
      "step": 1040
    },
    {
      "epoch": 0.37260468417317244,
      "grad_norm": 10.95009708404541,
      "learning_rate": 9.257629524485451e-05,
      "loss": 3.7825,
      "step": 1050
    },
    {
      "epoch": 0.376153300212917,
      "grad_norm": 9.229191780090332,
      "learning_rate": 9.250532292405963e-05,
      "loss": 3.6739,
      "step": 1060
    },
    {
      "epoch": 0.3797019162526615,
      "grad_norm": 9.404187202453613,
      "learning_rate": 9.243435060326473e-05,
      "loss": 3.8436,
      "step": 1070
    },
    {
      "epoch": 0.38325053229240597,
      "grad_norm": 9.94717025756836,
      "learning_rate": 9.236337828246984e-05,
      "loss": 3.6809,
      "step": 1080
    },
    {
      "epoch": 0.38679914833215046,
      "grad_norm": 8.628653526306152,
      "learning_rate": 9.229240596167495e-05,
      "loss": 3.8342,
      "step": 1090
    },
    {
      "epoch": 0.39034776437189495,
      "grad_norm": 8.788548469543457,
      "learning_rate": 9.222143364088006e-05,
      "loss": 3.7842,
      "step": 1100
    },
    {
      "epoch": 0.39389638041163944,
      "grad_norm": 9.532779693603516,
      "learning_rate": 9.215046132008516e-05,
      "loss": 3.7818,
      "step": 1110
    },
    {
      "epoch": 0.397444996451384,
      "grad_norm": 9.98658561706543,
      "learning_rate": 9.207948899929029e-05,
      "loss": 3.8596,
      "step": 1120
    },
    {
      "epoch": 0.40099361249112847,
      "grad_norm": 9.463139533996582,
      "learning_rate": 9.200851667849539e-05,
      "loss": 3.7375,
      "step": 1130
    },
    {
      "epoch": 0.40454222853087296,
      "grad_norm": 9.026130676269531,
      "learning_rate": 9.19375443577005e-05,
      "loss": 3.7715,
      "step": 1140
    },
    {
      "epoch": 0.40809084457061745,
      "grad_norm": 16.205684661865234,
      "learning_rate": 9.18665720369056e-05,
      "loss": 3.7939,
      "step": 1150
    },
    {
      "epoch": 0.41163946061036194,
      "grad_norm": 9.303296089172363,
      "learning_rate": 9.179559971611073e-05,
      "loss": 3.7528,
      "step": 1160
    },
    {
      "epoch": 0.4151880766501065,
      "grad_norm": 9.678827285766602,
      "learning_rate": 9.172462739531583e-05,
      "loss": 3.6828,
      "step": 1170
    },
    {
      "epoch": 0.418736692689851,
      "grad_norm": 9.284566879272461,
      "learning_rate": 9.165365507452094e-05,
      "loss": 3.7779,
      "step": 1180
    },
    {
      "epoch": 0.42228530872959547,
      "grad_norm": 10.108372688293457,
      "learning_rate": 9.158268275372605e-05,
      "loss": 3.6892,
      "step": 1190
    },
    {
      "epoch": 0.42583392476933996,
      "grad_norm": 9.073432922363281,
      "learning_rate": 9.151171043293116e-05,
      "loss": 3.7363,
      "step": 1200
    },
    {
      "epoch": 0.42938254080908445,
      "grad_norm": 8.549727439880371,
      "learning_rate": 9.144073811213628e-05,
      "loss": 3.8063,
      "step": 1210
    },
    {
      "epoch": 0.43293115684882894,
      "grad_norm": 9.121397972106934,
      "learning_rate": 9.136976579134138e-05,
      "loss": 3.738,
      "step": 1220
    },
    {
      "epoch": 0.4364797728885735,
      "grad_norm": 9.979776382446289,
      "learning_rate": 9.129879347054649e-05,
      "loss": 3.6118,
      "step": 1230
    },
    {
      "epoch": 0.44002838892831797,
      "grad_norm": 9.597269058227539,
      "learning_rate": 9.12278211497516e-05,
      "loss": 3.6828,
      "step": 1240
    },
    {
      "epoch": 0.44357700496806246,
      "grad_norm": 10.942949295043945,
      "learning_rate": 9.115684882895671e-05,
      "loss": 3.8225,
      "step": 1250
    },
    {
      "epoch": 0.44712562100780695,
      "grad_norm": 10.111490249633789,
      "learning_rate": 9.108587650816181e-05,
      "loss": 3.7878,
      "step": 1260
    },
    {
      "epoch": 0.45067423704755144,
      "grad_norm": 9.24775505065918,
      "learning_rate": 9.101490418736694e-05,
      "loss": 3.8161,
      "step": 1270
    },
    {
      "epoch": 0.45422285308729593,
      "grad_norm": 11.585188865661621,
      "learning_rate": 9.094393186657204e-05,
      "loss": 3.7366,
      "step": 1280
    },
    {
      "epoch": 0.4577714691270405,
      "grad_norm": 10.034201622009277,
      "learning_rate": 9.087295954577715e-05,
      "loss": 3.7439,
      "step": 1290
    },
    {
      "epoch": 0.46132008516678497,
      "grad_norm": 9.526439666748047,
      "learning_rate": 9.080198722498226e-05,
      "loss": 3.9178,
      "step": 1300
    },
    {
      "epoch": 0.46486870120652946,
      "grad_norm": 10.490854263305664,
      "learning_rate": 9.073101490418738e-05,
      "loss": 3.6486,
      "step": 1310
    },
    {
      "epoch": 0.46841731724627395,
      "grad_norm": 12.100886344909668,
      "learning_rate": 9.066004258339248e-05,
      "loss": 3.5983,
      "step": 1320
    },
    {
      "epoch": 0.47196593328601844,
      "grad_norm": 10.519795417785645,
      "learning_rate": 9.058907026259759e-05,
      "loss": 3.7354,
      "step": 1330
    },
    {
      "epoch": 0.4755145493257629,
      "grad_norm": 10.616394996643066,
      "learning_rate": 9.05180979418027e-05,
      "loss": 3.7801,
      "step": 1340
    },
    {
      "epoch": 0.47906316536550747,
      "grad_norm": 9.768937110900879,
      "learning_rate": 9.044712562100781e-05,
      "loss": 3.8341,
      "step": 1350
    },
    {
      "epoch": 0.48261178140525196,
      "grad_norm": 9.531604766845703,
      "learning_rate": 9.037615330021291e-05,
      "loss": 3.6929,
      "step": 1360
    },
    {
      "epoch": 0.48616039744499645,
      "grad_norm": 9.847573280334473,
      "learning_rate": 9.030518097941804e-05,
      "loss": 3.694,
      "step": 1370
    },
    {
      "epoch": 0.48970901348474094,
      "grad_norm": 11.087637901306152,
      "learning_rate": 9.023420865862314e-05,
      "loss": 3.6424,
      "step": 1380
    },
    {
      "epoch": 0.49325762952448543,
      "grad_norm": 8.401408195495605,
      "learning_rate": 9.016323633782825e-05,
      "loss": 3.7059,
      "step": 1390
    },
    {
      "epoch": 0.49680624556423,
      "grad_norm": 8.729101181030273,
      "learning_rate": 9.009226401703335e-05,
      "loss": 3.7411,
      "step": 1400
    },
    {
      "epoch": 0.5003548616039745,
      "grad_norm": 9.244491577148438,
      "learning_rate": 9.002129169623848e-05,
      "loss": 3.6712,
      "step": 1410
    },
    {
      "epoch": 0.5039034776437189,
      "grad_norm": 8.663081169128418,
      "learning_rate": 8.995031937544358e-05,
      "loss": 3.6679,
      "step": 1420
    },
    {
      "epoch": 0.5074520936834634,
      "grad_norm": 9.223261833190918,
      "learning_rate": 8.987934705464869e-05,
      "loss": 3.6967,
      "step": 1430
    },
    {
      "epoch": 0.511000709723208,
      "grad_norm": 8.629589080810547,
      "learning_rate": 8.98083747338538e-05,
      "loss": 3.62,
      "step": 1440
    },
    {
      "epoch": 0.5145493257629524,
      "grad_norm": 10.373592376708984,
      "learning_rate": 8.973740241305891e-05,
      "loss": 3.6802,
      "step": 1450
    },
    {
      "epoch": 0.518097941802697,
      "grad_norm": 8.684138298034668,
      "learning_rate": 8.966643009226401e-05,
      "loss": 3.6843,
      "step": 1460
    },
    {
      "epoch": 0.5216465578424414,
      "grad_norm": 11.6048583984375,
      "learning_rate": 8.959545777146913e-05,
      "loss": 3.663,
      "step": 1470
    },
    {
      "epoch": 0.525195173882186,
      "grad_norm": 9.789961814880371,
      "learning_rate": 8.952448545067424e-05,
      "loss": 3.759,
      "step": 1480
    },
    {
      "epoch": 0.5287437899219305,
      "grad_norm": 10.400968551635742,
      "learning_rate": 8.945351312987935e-05,
      "loss": 3.6349,
      "step": 1490
    },
    {
      "epoch": 0.5322924059616749,
      "grad_norm": 10.152787208557129,
      "learning_rate": 8.938254080908446e-05,
      "loss": 3.6383,
      "step": 1500
    },
    {
      "epoch": 0.5358410220014195,
      "grad_norm": 10.240814208984375,
      "learning_rate": 8.931156848828958e-05,
      "loss": 3.7606,
      "step": 1510
    },
    {
      "epoch": 0.5393896380411639,
      "grad_norm": 9.093767166137695,
      "learning_rate": 8.924059616749468e-05,
      "loss": 3.6601,
      "step": 1520
    },
    {
      "epoch": 0.5429382540809085,
      "grad_norm": 10.381497383117676,
      "learning_rate": 8.916962384669979e-05,
      "loss": 3.6582,
      "step": 1530
    },
    {
      "epoch": 0.5464868701206529,
      "grad_norm": 10.144776344299316,
      "learning_rate": 8.90986515259049e-05,
      "loss": 3.721,
      "step": 1540
    },
    {
      "epoch": 0.5500354861603974,
      "grad_norm": 8.380285263061523,
      "learning_rate": 8.902767920511001e-05,
      "loss": 3.5956,
      "step": 1550
    },
    {
      "epoch": 0.553584102200142,
      "grad_norm": 9.666349411010742,
      "learning_rate": 8.895670688431513e-05,
      "loss": 3.5807,
      "step": 1560
    },
    {
      "epoch": 0.5571327182398864,
      "grad_norm": 9.941278457641602,
      "learning_rate": 8.888573456352023e-05,
      "loss": 3.6665,
      "step": 1570
    },
    {
      "epoch": 0.560681334279631,
      "grad_norm": 9.181958198547363,
      "learning_rate": 8.881476224272535e-05,
      "loss": 3.6175,
      "step": 1580
    },
    {
      "epoch": 0.5642299503193754,
      "grad_norm": 10.020894050598145,
      "learning_rate": 8.874378992193045e-05,
      "loss": 3.7,
      "step": 1590
    },
    {
      "epoch": 0.5677785663591199,
      "grad_norm": 11.331542015075684,
      "learning_rate": 8.867281760113556e-05,
      "loss": 3.6456,
      "step": 1600
    },
    {
      "epoch": 0.5713271823988645,
      "grad_norm": 9.853934288024902,
      "learning_rate": 8.860184528034066e-05,
      "loss": 3.6186,
      "step": 1610
    },
    {
      "epoch": 0.5748757984386089,
      "grad_norm": 10.505566596984863,
      "learning_rate": 8.853087295954579e-05,
      "loss": 3.6223,
      "step": 1620
    },
    {
      "epoch": 0.5784244144783535,
      "grad_norm": 10.391867637634277,
      "learning_rate": 8.845990063875089e-05,
      "loss": 3.6322,
      "step": 1630
    },
    {
      "epoch": 0.5819730305180979,
      "grad_norm": 10.898919105529785,
      "learning_rate": 8.8388928317956e-05,
      "loss": 3.7041,
      "step": 1640
    },
    {
      "epoch": 0.5855216465578424,
      "grad_norm": 11.054080963134766,
      "learning_rate": 8.83179559971611e-05,
      "loss": 3.5716,
      "step": 1650
    },
    {
      "epoch": 0.589070262597587,
      "grad_norm": 9.768596649169922,
      "learning_rate": 8.824698367636623e-05,
      "loss": 3.6636,
      "step": 1660
    },
    {
      "epoch": 0.5926188786373314,
      "grad_norm": 9.610625267028809,
      "learning_rate": 8.817601135557133e-05,
      "loss": 3.5823,
      "step": 1670
    },
    {
      "epoch": 0.596167494677076,
      "grad_norm": 10.686138153076172,
      "learning_rate": 8.810503903477644e-05,
      "loss": 3.669,
      "step": 1680
    },
    {
      "epoch": 0.5997161107168204,
      "grad_norm": 9.424166679382324,
      "learning_rate": 8.803406671398155e-05,
      "loss": 3.623,
      "step": 1690
    },
    {
      "epoch": 0.603264726756565,
      "grad_norm": 9.052885055541992,
      "learning_rate": 8.796309439318667e-05,
      "loss": 3.641,
      "step": 1700
    },
    {
      "epoch": 0.6068133427963094,
      "grad_norm": 9.292540550231934,
      "learning_rate": 8.789212207239176e-05,
      "loss": 3.6197,
      "step": 1710
    },
    {
      "epoch": 0.6103619588360539,
      "grad_norm": 9.330850601196289,
      "learning_rate": 8.782114975159688e-05,
      "loss": 3.6121,
      "step": 1720
    },
    {
      "epoch": 0.6139105748757985,
      "grad_norm": 10.072600364685059,
      "learning_rate": 8.775017743080199e-05,
      "loss": 3.5741,
      "step": 1730
    },
    {
      "epoch": 0.6174591909155429,
      "grad_norm": 8.422746658325195,
      "learning_rate": 8.76792051100071e-05,
      "loss": 3.6369,
      "step": 1740
    },
    {
      "epoch": 0.6210078069552875,
      "grad_norm": 10.978137969970703,
      "learning_rate": 8.76082327892122e-05,
      "loss": 3.6081,
      "step": 1750
    },
    {
      "epoch": 0.6245564229950319,
      "grad_norm": 10.176301002502441,
      "learning_rate": 8.753726046841733e-05,
      "loss": 3.5677,
      "step": 1760
    },
    {
      "epoch": 0.6281050390347764,
      "grad_norm": 10.502670288085938,
      "learning_rate": 8.746628814762243e-05,
      "loss": 3.4655,
      "step": 1770
    },
    {
      "epoch": 0.631653655074521,
      "grad_norm": 10.230278015136719,
      "learning_rate": 8.739531582682754e-05,
      "loss": 3.6722,
      "step": 1780
    },
    {
      "epoch": 0.6352022711142654,
      "grad_norm": 10.114200592041016,
      "learning_rate": 8.732434350603265e-05,
      "loss": 3.5933,
      "step": 1790
    },
    {
      "epoch": 0.63875088715401,
      "grad_norm": 8.896600723266602,
      "learning_rate": 8.725337118523777e-05,
      "loss": 3.5338,
      "step": 1800
    },
    {
      "epoch": 0.6422995031937544,
      "grad_norm": 8.616841316223145,
      "learning_rate": 8.718239886444288e-05,
      "loss": 3.5582,
      "step": 1810
    },
    {
      "epoch": 0.6458481192334989,
      "grad_norm": 9.221946716308594,
      "learning_rate": 8.711142654364798e-05,
      "loss": 3.508,
      "step": 1820
    },
    {
      "epoch": 0.6493967352732435,
      "grad_norm": 9.886122703552246,
      "learning_rate": 8.704045422285309e-05,
      "loss": 3.5152,
      "step": 1830
    },
    {
      "epoch": 0.6529453513129879,
      "grad_norm": 10.345726013183594,
      "learning_rate": 8.69694819020582e-05,
      "loss": 3.5542,
      "step": 1840
    },
    {
      "epoch": 0.6564939673527325,
      "grad_norm": 9.490591049194336,
      "learning_rate": 8.689850958126332e-05,
      "loss": 3.5104,
      "step": 1850
    },
    {
      "epoch": 0.6600425833924769,
      "grad_norm": 10.439362525939941,
      "learning_rate": 8.682753726046841e-05,
      "loss": 3.4779,
      "step": 1860
    },
    {
      "epoch": 0.6635911994322214,
      "grad_norm": 10.206643104553223,
      "learning_rate": 8.675656493967354e-05,
      "loss": 3.5534,
      "step": 1870
    },
    {
      "epoch": 0.6671398154719659,
      "grad_norm": 9.730339050292969,
      "learning_rate": 8.668559261887864e-05,
      "loss": 3.5447,
      "step": 1880
    },
    {
      "epoch": 0.6706884315117104,
      "grad_norm": 9.684427261352539,
      "learning_rate": 8.661462029808375e-05,
      "loss": 3.5412,
      "step": 1890
    },
    {
      "epoch": 0.674237047551455,
      "grad_norm": 10.313204765319824,
      "learning_rate": 8.654364797728887e-05,
      "loss": 3.5672,
      "step": 1900
    },
    {
      "epoch": 0.6777856635911994,
      "grad_norm": 9.599074363708496,
      "learning_rate": 8.647267565649398e-05,
      "loss": 3.5311,
      "step": 1910
    },
    {
      "epoch": 0.681334279630944,
      "grad_norm": 9.102608680725098,
      "learning_rate": 8.640170333569908e-05,
      "loss": 3.6401,
      "step": 1920
    },
    {
      "epoch": 0.6848828956706884,
      "grad_norm": 10.827591896057129,
      "learning_rate": 8.633073101490419e-05,
      "loss": 3.5895,
      "step": 1930
    },
    {
      "epoch": 0.6884315117104329,
      "grad_norm": 8.628922462463379,
      "learning_rate": 8.62597586941093e-05,
      "loss": 3.5889,
      "step": 1940
    },
    {
      "epoch": 0.6919801277501775,
      "grad_norm": 10.251337051391602,
      "learning_rate": 8.618878637331442e-05,
      "loss": 3.485,
      "step": 1950
    },
    {
      "epoch": 0.6955287437899219,
      "grad_norm": 9.384461402893066,
      "learning_rate": 8.611781405251951e-05,
      "loss": 3.5857,
      "step": 1960
    },
    {
      "epoch": 0.6990773598296665,
      "grad_norm": 9.689079284667969,
      "learning_rate": 8.604684173172464e-05,
      "loss": 3.5481,
      "step": 1970
    },
    {
      "epoch": 0.7026259758694109,
      "grad_norm": 9.825276374816895,
      "learning_rate": 8.597586941092974e-05,
      "loss": 3.5599,
      "step": 1980
    },
    {
      "epoch": 0.7061745919091554,
      "grad_norm": 10.932038307189941,
      "learning_rate": 8.590489709013485e-05,
      "loss": 3.5535,
      "step": 1990
    },
    {
      "epoch": 0.7097232079489,
      "grad_norm": 8.669021606445312,
      "learning_rate": 8.583392476933995e-05,
      "loss": 3.6042,
      "step": 2000
    },
    {
      "epoch": 0.7132718239886444,
      "grad_norm": 9.587408065795898,
      "learning_rate": 8.576295244854508e-05,
      "loss": 3.5799,
      "step": 2010
    },
    {
      "epoch": 0.716820440028389,
      "grad_norm": 9.325510025024414,
      "learning_rate": 8.569198012775018e-05,
      "loss": 3.4905,
      "step": 2020
    },
    {
      "epoch": 0.7203690560681334,
      "grad_norm": 10.37441635131836,
      "learning_rate": 8.562100780695529e-05,
      "loss": 3.5096,
      "step": 2030
    },
    {
      "epoch": 0.7239176721078779,
      "grad_norm": 9.463478088378906,
      "learning_rate": 8.55500354861604e-05,
      "loss": 3.5791,
      "step": 2040
    },
    {
      "epoch": 0.7274662881476224,
      "grad_norm": 9.450739860534668,
      "learning_rate": 8.547906316536552e-05,
      "loss": 3.5587,
      "step": 2050
    },
    {
      "epoch": 0.7310149041873669,
      "grad_norm": 10.81802749633789,
      "learning_rate": 8.540809084457061e-05,
      "loss": 3.4473,
      "step": 2060
    },
    {
      "epoch": 0.7345635202271115,
      "grad_norm": 11.301196098327637,
      "learning_rate": 8.533711852377573e-05,
      "loss": 3.4727,
      "step": 2070
    },
    {
      "epoch": 0.7381121362668559,
      "grad_norm": 10.565696716308594,
      "learning_rate": 8.526614620298084e-05,
      "loss": 3.5416,
      "step": 2080
    },
    {
      "epoch": 0.7416607523066004,
      "grad_norm": 9.935070037841797,
      "learning_rate": 8.519517388218595e-05,
      "loss": 3.4486,
      "step": 2090
    },
    {
      "epoch": 0.7452093683463449,
      "grad_norm": 11.912386894226074,
      "learning_rate": 8.512420156139107e-05,
      "loss": 3.534,
      "step": 2100
    },
    {
      "epoch": 0.7487579843860894,
      "grad_norm": 9.696078300476074,
      "learning_rate": 8.505322924059616e-05,
      "loss": 3.6131,
      "step": 2110
    },
    {
      "epoch": 0.752306600425834,
      "grad_norm": 9.220535278320312,
      "learning_rate": 8.498225691980128e-05,
      "loss": 3.5357,
      "step": 2120
    },
    {
      "epoch": 0.7558552164655784,
      "grad_norm": 9.061857223510742,
      "learning_rate": 8.491128459900639e-05,
      "loss": 3.5453,
      "step": 2130
    },
    {
      "epoch": 0.759403832505323,
      "grad_norm": 10.236008644104004,
      "learning_rate": 8.48403122782115e-05,
      "loss": 3.5459,
      "step": 2140
    },
    {
      "epoch": 0.7629524485450674,
      "grad_norm": 9.88957405090332,
      "learning_rate": 8.476933995741662e-05,
      "loss": 3.5494,
      "step": 2150
    },
    {
      "epoch": 0.7665010645848119,
      "grad_norm": 8.963573455810547,
      "learning_rate": 8.469836763662173e-05,
      "loss": 3.4602,
      "step": 2160
    },
    {
      "epoch": 0.7700496806245565,
      "grad_norm": 8.916491508483887,
      "learning_rate": 8.462739531582683e-05,
      "loss": 3.4255,
      "step": 2170
    },
    {
      "epoch": 0.7735982966643009,
      "grad_norm": 9.835213661193848,
      "learning_rate": 8.455642299503194e-05,
      "loss": 3.569,
      "step": 2180
    },
    {
      "epoch": 0.7771469127040455,
      "grad_norm": 9.76822566986084,
      "learning_rate": 8.448545067423705e-05,
      "loss": 3.4546,
      "step": 2190
    },
    {
      "epoch": 0.7806955287437899,
      "grad_norm": 10.865945816040039,
      "learning_rate": 8.441447835344217e-05,
      "loss": 3.4631,
      "step": 2200
    },
    {
      "epoch": 0.7842441447835344,
      "grad_norm": 9.997344970703125,
      "learning_rate": 8.434350603264726e-05,
      "loss": 3.507,
      "step": 2210
    },
    {
      "epoch": 0.7877927608232789,
      "grad_norm": 9.896722793579102,
      "learning_rate": 8.427253371185239e-05,
      "loss": 3.4656,
      "step": 2220
    },
    {
      "epoch": 0.7913413768630234,
      "grad_norm": 9.95897388458252,
      "learning_rate": 8.420156139105749e-05,
      "loss": 3.4609,
      "step": 2230
    },
    {
      "epoch": 0.794889992902768,
      "grad_norm": 9.460432052612305,
      "learning_rate": 8.41305890702626e-05,
      "loss": 3.4648,
      "step": 2240
    },
    {
      "epoch": 0.7984386089425124,
      "grad_norm": 10.948718070983887,
      "learning_rate": 8.40596167494677e-05,
      "loss": 3.4541,
      "step": 2250
    },
    {
      "epoch": 0.8019872249822569,
      "grad_norm": 9.839218139648438,
      "learning_rate": 8.398864442867283e-05,
      "loss": 3.5383,
      "step": 2260
    },
    {
      "epoch": 0.8055358410220014,
      "grad_norm": 10.301072120666504,
      "learning_rate": 8.391767210787793e-05,
      "loss": 3.4599,
      "step": 2270
    },
    {
      "epoch": 0.8090844570617459,
      "grad_norm": 9.395825386047363,
      "learning_rate": 8.385379701916253e-05,
      "loss": 3.452,
      "step": 2280
    },
    {
      "epoch": 0.8126330731014905,
      "grad_norm": 9.949261665344238,
      "learning_rate": 8.378282469836764e-05,
      "loss": 3.5342,
      "step": 2290
    },
    {
      "epoch": 0.8161816891412349,
      "grad_norm": 9.321681022644043,
      "learning_rate": 8.371185237757275e-05,
      "loss": 3.3611,
      "step": 2300
    },
    {
      "epoch": 0.8197303051809794,
      "grad_norm": 10.173916816711426,
      "learning_rate": 8.364088005677786e-05,
      "loss": 3.4354,
      "step": 2310
    },
    {
      "epoch": 0.8232789212207239,
      "grad_norm": 10.713973999023438,
      "learning_rate": 8.356990773598296e-05,
      "loss": 3.3807,
      "step": 2320
    },
    {
      "epoch": 0.8268275372604684,
      "grad_norm": 11.048404693603516,
      "learning_rate": 8.349893541518809e-05,
      "loss": 3.3331,
      "step": 2330
    },
    {
      "epoch": 0.830376153300213,
      "grad_norm": 10.283257484436035,
      "learning_rate": 8.342796309439319e-05,
      "loss": 3.4482,
      "step": 2340
    },
    {
      "epoch": 0.8339247693399574,
      "grad_norm": 8.154019355773926,
      "learning_rate": 8.33569907735983e-05,
      "loss": 3.5496,
      "step": 2350
    },
    {
      "epoch": 0.837473385379702,
      "grad_norm": 9.565098762512207,
      "learning_rate": 8.328601845280341e-05,
      "loss": 3.3935,
      "step": 2360
    },
    {
      "epoch": 0.8410220014194464,
      "grad_norm": 9.996498107910156,
      "learning_rate": 8.321504613200853e-05,
      "loss": 3.3323,
      "step": 2370
    },
    {
      "epoch": 0.8445706174591909,
      "grad_norm": 10.94770336151123,
      "learning_rate": 8.314407381121363e-05,
      "loss": 3.4351,
      "step": 2380
    },
    {
      "epoch": 0.8481192334989354,
      "grad_norm": 9.533611297607422,
      "learning_rate": 8.307310149041874e-05,
      "loss": 3.5138,
      "step": 2390
    },
    {
      "epoch": 0.8516678495386799,
      "grad_norm": 8.869705200195312,
      "learning_rate": 8.300212916962385e-05,
      "loss": 3.5357,
      "step": 2400
    },
    {
      "epoch": 0.8552164655784245,
      "grad_norm": 10.582620620727539,
      "learning_rate": 8.293115684882896e-05,
      "loss": 3.5917,
      "step": 2410
    },
    {
      "epoch": 0.8587650816181689,
      "grad_norm": 9.02879810333252,
      "learning_rate": 8.286018452803406e-05,
      "loss": 3.5208,
      "step": 2420
    },
    {
      "epoch": 0.8623136976579134,
      "grad_norm": 10.214937210083008,
      "learning_rate": 8.278921220723918e-05,
      "loss": 3.3676,
      "step": 2430
    },
    {
      "epoch": 0.8658623136976579,
      "grad_norm": 10.193427085876465,
      "learning_rate": 8.271823988644429e-05,
      "loss": 3.4666,
      "step": 2440
    },
    {
      "epoch": 0.8694109297374024,
      "grad_norm": 9.963606834411621,
      "learning_rate": 8.26472675656494e-05,
      "loss": 3.5206,
      "step": 2450
    },
    {
      "epoch": 0.872959545777147,
      "grad_norm": 10.536102294921875,
      "learning_rate": 8.25762952448545e-05,
      "loss": 3.5037,
      "step": 2460
    },
    {
      "epoch": 0.8765081618168914,
      "grad_norm": 10.096513748168945,
      "learning_rate": 8.250532292405963e-05,
      "loss": 3.4817,
      "step": 2470
    },
    {
      "epoch": 0.8800567778566359,
      "grad_norm": 10.23276138305664,
      "learning_rate": 8.243435060326473e-05,
      "loss": 3.4933,
      "step": 2480
    },
    {
      "epoch": 0.8836053938963804,
      "grad_norm": 8.036561965942383,
      "learning_rate": 8.236337828246984e-05,
      "loss": 3.403,
      "step": 2490
    },
    {
      "epoch": 0.8871540099361249,
      "grad_norm": 10.38477611541748,
      "learning_rate": 8.229240596167494e-05,
      "loss": 3.4948,
      "step": 2500
    },
    {
      "epoch": 0.8907026259758695,
      "grad_norm": 9.013980865478516,
      "learning_rate": 8.222143364088006e-05,
      "loss": 3.5378,
      "step": 2510
    },
    {
      "epoch": 0.8942512420156139,
      "grad_norm": 9.627052307128906,
      "learning_rate": 8.215046132008516e-05,
      "loss": 3.4516,
      "step": 2520
    },
    {
      "epoch": 0.8977998580553584,
      "grad_norm": 11.43186092376709,
      "learning_rate": 8.207948899929028e-05,
      "loss": 3.4239,
      "step": 2530
    },
    {
      "epoch": 0.9013484740951029,
      "grad_norm": 9.300699234008789,
      "learning_rate": 8.200851667849539e-05,
      "loss": 3.3864,
      "step": 2540
    },
    {
      "epoch": 0.9048970901348474,
      "grad_norm": 9.558442115783691,
      "learning_rate": 8.19375443577005e-05,
      "loss": 3.4777,
      "step": 2550
    },
    {
      "epoch": 0.9084457061745919,
      "grad_norm": 9.258413314819336,
      "learning_rate": 8.186657203690562e-05,
      "loss": 3.3452,
      "step": 2560
    },
    {
      "epoch": 0.9119943222143364,
      "grad_norm": 9.12828254699707,
      "learning_rate": 8.179559971611071e-05,
      "loss": 3.3977,
      "step": 2570
    },
    {
      "epoch": 0.915542938254081,
      "grad_norm": 8.63705062866211,
      "learning_rate": 8.172462739531583e-05,
      "loss": 3.4757,
      "step": 2580
    },
    {
      "epoch": 0.9190915542938254,
      "grad_norm": 10.899974822998047,
      "learning_rate": 8.165365507452094e-05,
      "loss": 3.5158,
      "step": 2590
    },
    {
      "epoch": 0.9226401703335699,
      "grad_norm": 9.776798248291016,
      "learning_rate": 8.158268275372605e-05,
      "loss": 3.4386,
      "step": 2600
    },
    {
      "epoch": 0.9261887863733144,
      "grad_norm": 25.114442825317383,
      "learning_rate": 8.151171043293117e-05,
      "loss": 3.4225,
      "step": 2610
    },
    {
      "epoch": 0.9297374024130589,
      "grad_norm": 9.107666969299316,
      "learning_rate": 8.144073811213628e-05,
      "loss": 3.539,
      "step": 2620
    },
    {
      "epoch": 0.9332860184528035,
      "grad_norm": 10.465971946716309,
      "learning_rate": 8.136976579134138e-05,
      "loss": 3.4517,
      "step": 2630
    },
    {
      "epoch": 0.9368346344925479,
      "grad_norm": 10.666162490844727,
      "learning_rate": 8.129879347054649e-05,
      "loss": 3.3637,
      "step": 2640
    },
    {
      "epoch": 0.9403832505322924,
      "grad_norm": 9.570754051208496,
      "learning_rate": 8.12278211497516e-05,
      "loss": 3.3765,
      "step": 2650
    },
    {
      "epoch": 0.9439318665720369,
      "grad_norm": 9.39518928527832,
      "learning_rate": 8.115684882895672e-05,
      "loss": 3.4055,
      "step": 2660
    },
    {
      "epoch": 0.9474804826117814,
      "grad_norm": 11.25067138671875,
      "learning_rate": 8.108587650816181e-05,
      "loss": 3.4389,
      "step": 2670
    },
    {
      "epoch": 0.9510290986515259,
      "grad_norm": 10.425254821777344,
      "learning_rate": 8.101490418736694e-05,
      "loss": 3.3286,
      "step": 2680
    },
    {
      "epoch": 0.9545777146912704,
      "grad_norm": 9.593608856201172,
      "learning_rate": 8.094393186657204e-05,
      "loss": 3.4176,
      "step": 2690
    },
    {
      "epoch": 0.9581263307310149,
      "grad_norm": 9.692399024963379,
      "learning_rate": 8.087295954577715e-05,
      "loss": 3.4222,
      "step": 2700
    },
    {
      "epoch": 0.9616749467707594,
      "grad_norm": 9.014738082885742,
      "learning_rate": 8.080198722498225e-05,
      "loss": 3.4717,
      "step": 2710
    },
    {
      "epoch": 0.9652235628105039,
      "grad_norm": 10.535873413085938,
      "learning_rate": 8.073101490418738e-05,
      "loss": 3.5394,
      "step": 2720
    },
    {
      "epoch": 0.9687721788502484,
      "grad_norm": 9.754653930664062,
      "learning_rate": 8.066004258339248e-05,
      "loss": 3.3989,
      "step": 2730
    },
    {
      "epoch": 0.9723207948899929,
      "grad_norm": 10.958183288574219,
      "learning_rate": 8.058907026259759e-05,
      "loss": 3.3012,
      "step": 2740
    },
    {
      "epoch": 0.9758694109297374,
      "grad_norm": 9.090791702270508,
      "learning_rate": 8.05180979418027e-05,
      "loss": 3.33,
      "step": 2750
    },
    {
      "epoch": 0.9794180269694819,
      "grad_norm": 10.420994758605957,
      "learning_rate": 8.044712562100782e-05,
      "loss": 3.4142,
      "step": 2760
    },
    {
      "epoch": 0.9829666430092264,
      "grad_norm": 9.988271713256836,
      "learning_rate": 8.037615330021291e-05,
      "loss": 3.4206,
      "step": 2770
    },
    {
      "epoch": 0.9865152590489709,
      "grad_norm": 10.079995155334473,
      "learning_rate": 8.030518097941803e-05,
      "loss": 3.2929,
      "step": 2780
    },
    {
      "epoch": 0.9900638750887154,
      "grad_norm": 9.551318168640137,
      "learning_rate": 8.023420865862314e-05,
      "loss": 3.4099,
      "step": 2790
    },
    {
      "epoch": 0.99361249112846,
      "grad_norm": 9.306806564331055,
      "learning_rate": 8.016323633782825e-05,
      "loss": 3.3368,
      "step": 2800
    },
    {
      "epoch": 0.9971611071682044,
      "grad_norm": 10.547664642333984,
      "learning_rate": 8.009226401703335e-05,
      "loss": 3.3424,
      "step": 2810
    },
    {
      "epoch": 1.000709723207949,
      "grad_norm": 10.003096580505371,
      "learning_rate": 8.002129169623848e-05,
      "loss": 3.3947,
      "step": 2820
    },
    {
      "epoch": 1.0042583392476934,
      "grad_norm": 8.980180740356445,
      "learning_rate": 7.995031937544358e-05,
      "loss": 3.4107,
      "step": 2830
    },
    {
      "epoch": 1.0078069552874378,
      "grad_norm": 8.165224075317383,
      "learning_rate": 7.987934705464869e-05,
      "loss": 3.3767,
      "step": 2840
    },
    {
      "epoch": 1.0113555713271825,
      "grad_norm": 9.247590065002441,
      "learning_rate": 7.98083747338538e-05,
      "loss": 3.417,
      "step": 2850
    },
    {
      "epoch": 1.014904187366927,
      "grad_norm": 9.623479843139648,
      "learning_rate": 7.973740241305892e-05,
      "loss": 3.3502,
      "step": 2860
    },
    {
      "epoch": 1.0184528034066713,
      "grad_norm": 9.515925407409668,
      "learning_rate": 7.966643009226403e-05,
      "loss": 3.2988,
      "step": 2870
    },
    {
      "epoch": 1.022001419446416,
      "grad_norm": 9.850569725036621,
      "learning_rate": 7.959545777146913e-05,
      "loss": 3.4096,
      "step": 2880
    },
    {
      "epoch": 1.0255500354861604,
      "grad_norm": 9.443185806274414,
      "learning_rate": 7.952448545067424e-05,
      "loss": 3.4026,
      "step": 2890
    },
    {
      "epoch": 1.0290986515259049,
      "grad_norm": 12.649226188659668,
      "learning_rate": 7.945351312987935e-05,
      "loss": 3.3484,
      "step": 2900
    },
    {
      "epoch": 1.0326472675656495,
      "grad_norm": 9.745811462402344,
      "learning_rate": 7.938254080908447e-05,
      "loss": 3.4127,
      "step": 2910
    },
    {
      "epoch": 1.036195883605394,
      "grad_norm": 9.879714012145996,
      "learning_rate": 7.931156848828956e-05,
      "loss": 3.433,
      "step": 2920
    },
    {
      "epoch": 1.0397444996451384,
      "grad_norm": 9.802042007446289,
      "learning_rate": 7.924059616749469e-05,
      "loss": 3.3984,
      "step": 2930
    },
    {
      "epoch": 1.0432931156848828,
      "grad_norm": 9.9871826171875,
      "learning_rate": 7.916962384669979e-05,
      "loss": 3.3581,
      "step": 2940
    },
    {
      "epoch": 1.0468417317246275,
      "grad_norm": 9.851114273071289,
      "learning_rate": 7.90986515259049e-05,
      "loss": 3.3458,
      "step": 2950
    },
    {
      "epoch": 1.050390347764372,
      "grad_norm": 10.303963661193848,
      "learning_rate": 7.902767920511e-05,
      "loss": 3.4404,
      "step": 2960
    },
    {
      "epoch": 1.0539389638041163,
      "grad_norm": 10.32653522491455,
      "learning_rate": 7.895670688431513e-05,
      "loss": 3.3277,
      "step": 2970
    },
    {
      "epoch": 1.057487579843861,
      "grad_norm": 9.092658042907715,
      "learning_rate": 7.888573456352023e-05,
      "loss": 3.3707,
      "step": 2980
    },
    {
      "epoch": 1.0610361958836054,
      "grad_norm": 9.68780517578125,
      "learning_rate": 7.881476224272534e-05,
      "loss": 3.4204,
      "step": 2990
    },
    {
      "epoch": 1.0645848119233499,
      "grad_norm": 9.78197193145752,
      "learning_rate": 7.874378992193045e-05,
      "loss": 3.2597,
      "step": 3000
    },
    {
      "epoch": 1.0681334279630943,
      "grad_norm": 9.572417259216309,
      "learning_rate": 7.867281760113557e-05,
      "loss": 3.3904,
      "step": 3010
    },
    {
      "epoch": 1.071682044002839,
      "grad_norm": 11.293291091918945,
      "learning_rate": 7.860184528034066e-05,
      "loss": 3.3983,
      "step": 3020
    },
    {
      "epoch": 1.0752306600425834,
      "grad_norm": 9.124507904052734,
      "learning_rate": 7.853087295954578e-05,
      "loss": 3.3525,
      "step": 3030
    },
    {
      "epoch": 1.0787792760823278,
      "grad_norm": 9.357288360595703,
      "learning_rate": 7.845990063875089e-05,
      "loss": 3.3769,
      "step": 3040
    },
    {
      "epoch": 1.0823278921220725,
      "grad_norm": 9.0996732711792,
      "learning_rate": 7.8388928317956e-05,
      "loss": 3.4059,
      "step": 3050
    },
    {
      "epoch": 1.085876508161817,
      "grad_norm": 13.325597763061523,
      "learning_rate": 7.83179559971611e-05,
      "loss": 3.3901,
      "step": 3060
    },
    {
      "epoch": 1.0894251242015613,
      "grad_norm": 9.72962760925293,
      "learning_rate": 7.824698367636623e-05,
      "loss": 3.4335,
      "step": 3070
    },
    {
      "epoch": 1.0929737402413058,
      "grad_norm": 11.131255149841309,
      "learning_rate": 7.817601135557133e-05,
      "loss": 3.3349,
      "step": 3080
    },
    {
      "epoch": 1.0965223562810504,
      "grad_norm": 9.259538650512695,
      "learning_rate": 7.810503903477644e-05,
      "loss": 3.3513,
      "step": 3090
    },
    {
      "epoch": 1.1000709723207949,
      "grad_norm": 9.318735122680664,
      "learning_rate": 7.803406671398154e-05,
      "loss": 3.3794,
      "step": 3100
    },
    {
      "epoch": 1.1036195883605393,
      "grad_norm": 9.959529876708984,
      "learning_rate": 7.796309439318667e-05,
      "loss": 3.3484,
      "step": 3110
    },
    {
      "epoch": 1.107168204400284,
      "grad_norm": 9.348580360412598,
      "learning_rate": 7.789212207239177e-05,
      "loss": 3.3329,
      "step": 3120
    },
    {
      "epoch": 1.1107168204400284,
      "grad_norm": 9.927255630493164,
      "learning_rate": 7.782114975159688e-05,
      "loss": 3.2843,
      "step": 3130
    },
    {
      "epoch": 1.1142654364797728,
      "grad_norm": 10.003180503845215,
      "learning_rate": 7.775017743080199e-05,
      "loss": 3.4085,
      "step": 3140
    },
    {
      "epoch": 1.1178140525195175,
      "grad_norm": 9.06185531616211,
      "learning_rate": 7.76792051100071e-05,
      "loss": 3.311,
      "step": 3150
    },
    {
      "epoch": 1.121362668559262,
      "grad_norm": 10.112982749938965,
      "learning_rate": 7.760823278921222e-05,
      "loss": 3.3083,
      "step": 3160
    },
    {
      "epoch": 1.1249112845990064,
      "grad_norm": 9.616617202758789,
      "learning_rate": 7.753726046841732e-05,
      "loss": 3.3291,
      "step": 3170
    },
    {
      "epoch": 1.1284599006387508,
      "grad_norm": 11.518325805664062,
      "learning_rate": 7.746628814762243e-05,
      "loss": 3.3775,
      "step": 3180
    },
    {
      "epoch": 1.1320085166784954,
      "grad_norm": 9.29232120513916,
      "learning_rate": 7.739531582682754e-05,
      "loss": 3.3475,
      "step": 3190
    },
    {
      "epoch": 1.1355571327182399,
      "grad_norm": 9.8716402053833,
      "learning_rate": 7.732434350603265e-05,
      "loss": 3.2863,
      "step": 3200
    },
    {
      "epoch": 1.1391057487579843,
      "grad_norm": 11.628894805908203,
      "learning_rate": 7.725337118523777e-05,
      "loss": 3.3191,
      "step": 3210
    },
    {
      "epoch": 1.142654364797729,
      "grad_norm": 9.48122501373291,
      "learning_rate": 7.718239886444288e-05,
      "loss": 3.3904,
      "step": 3220
    },
    {
      "epoch": 1.1462029808374734,
      "grad_norm": 9.175207138061523,
      "learning_rate": 7.711142654364798e-05,
      "loss": 3.3124,
      "step": 3230
    },
    {
      "epoch": 1.1497515968772178,
      "grad_norm": 11.044869422912598,
      "learning_rate": 7.704045422285309e-05,
      "loss": 3.3449,
      "step": 3240
    },
    {
      "epoch": 1.1533002129169625,
      "grad_norm": 10.522117614746094,
      "learning_rate": 7.69694819020582e-05,
      "loss": 3.2674,
      "step": 3250
    },
    {
      "epoch": 1.156848828956707,
      "grad_norm": 9.873703002929688,
      "learning_rate": 7.689850958126332e-05,
      "loss": 3.3713,
      "step": 3260
    },
    {
      "epoch": 1.1603974449964514,
      "grad_norm": 12.14047622680664,
      "learning_rate": 7.682753726046842e-05,
      "loss": 3.3665,
      "step": 3270
    },
    {
      "epoch": 1.1639460610361958,
      "grad_norm": 11.853321075439453,
      "learning_rate": 7.675656493967354e-05,
      "loss": 3.3922,
      "step": 3280
    },
    {
      "epoch": 1.1674946770759405,
      "grad_norm": 9.81912899017334,
      "learning_rate": 7.668559261887864e-05,
      "loss": 3.2926,
      "step": 3290
    },
    {
      "epoch": 1.171043293115685,
      "grad_norm": 11.4719877243042,
      "learning_rate": 7.661462029808375e-05,
      "loss": 3.189,
      "step": 3300
    },
    {
      "epoch": 1.1745919091554293,
      "grad_norm": 9.295613288879395,
      "learning_rate": 7.654364797728885e-05,
      "loss": 3.3773,
      "step": 3310
    },
    {
      "epoch": 1.178140525195174,
      "grad_norm": 9.974660873413086,
      "learning_rate": 7.647267565649398e-05,
      "loss": 3.3518,
      "step": 3320
    },
    {
      "epoch": 1.1816891412349184,
      "grad_norm": 11.576545715332031,
      "learning_rate": 7.640170333569908e-05,
      "loss": 3.3354,
      "step": 3330
    },
    {
      "epoch": 1.1852377572746629,
      "grad_norm": 10.440716743469238,
      "learning_rate": 7.633073101490419e-05,
      "loss": 3.42,
      "step": 3340
    },
    {
      "epoch": 1.1887863733144073,
      "grad_norm": 9.286728858947754,
      "learning_rate": 7.625975869410929e-05,
      "loss": 3.3075,
      "step": 3350
    },
    {
      "epoch": 1.192334989354152,
      "grad_norm": 10.042699813842773,
      "learning_rate": 7.618878637331442e-05,
      "loss": 3.2986,
      "step": 3360
    },
    {
      "epoch": 1.1958836053938964,
      "grad_norm": 9.605705261230469,
      "learning_rate": 7.611781405251952e-05,
      "loss": 3.1679,
      "step": 3370
    },
    {
      "epoch": 1.1994322214336408,
      "grad_norm": 8.990912437438965,
      "learning_rate": 7.604684173172463e-05,
      "loss": 3.3967,
      "step": 3380
    },
    {
      "epoch": 1.2029808374733855,
      "grad_norm": 10.218306541442871,
      "learning_rate": 7.597586941092974e-05,
      "loss": 3.2604,
      "step": 3390
    },
    {
      "epoch": 1.20652945351313,
      "grad_norm": 9.229533195495605,
      "learning_rate": 7.590489709013485e-05,
      "loss": 3.3885,
      "step": 3400
    },
    {
      "epoch": 1.2100780695528743,
      "grad_norm": 9.775120735168457,
      "learning_rate": 7.583392476933995e-05,
      "loss": 3.2704,
      "step": 3410
    },
    {
      "epoch": 1.2136266855926188,
      "grad_norm": 9.18841552734375,
      "learning_rate": 7.576295244854507e-05,
      "loss": 3.3223,
      "step": 3420
    },
    {
      "epoch": 1.2171753016323634,
      "grad_norm": 10.006612777709961,
      "learning_rate": 7.569198012775018e-05,
      "loss": 3.3886,
      "step": 3430
    },
    {
      "epoch": 1.2207239176721079,
      "grad_norm": 10.064809799194336,
      "learning_rate": 7.562100780695529e-05,
      "loss": 3.2927,
      "step": 3440
    },
    {
      "epoch": 1.2242725337118523,
      "grad_norm": 11.390629768371582,
      "learning_rate": 7.55500354861604e-05,
      "loss": 3.3003,
      "step": 3450
    },
    {
      "epoch": 1.227821149751597,
      "grad_norm": 10.381447792053223,
      "learning_rate": 7.547906316536552e-05,
      "loss": 3.2818,
      "step": 3460
    },
    {
      "epoch": 1.2313697657913414,
      "grad_norm": 10.060993194580078,
      "learning_rate": 7.540809084457063e-05,
      "loss": 3.3302,
      "step": 3470
    },
    {
      "epoch": 1.2349183818310858,
      "grad_norm": 8.889031410217285,
      "learning_rate": 7.533711852377573e-05,
      "loss": 3.3126,
      "step": 3480
    },
    {
      "epoch": 1.2384669978708303,
      "grad_norm": 10.911044120788574,
      "learning_rate": 7.526614620298084e-05,
      "loss": 3.2681,
      "step": 3490
    },
    {
      "epoch": 1.242015613910575,
      "grad_norm": 8.770867347717285,
      "learning_rate": 7.519517388218595e-05,
      "loss": 3.2476,
      "step": 3500
    },
    {
      "epoch": 1.2455642299503193,
      "grad_norm": 9.36079216003418,
      "learning_rate": 7.512420156139107e-05,
      "loss": 3.2964,
      "step": 3510
    },
    {
      "epoch": 1.2491128459900638,
      "grad_norm": 9.314094543457031,
      "learning_rate": 7.505322924059617e-05,
      "loss": 3.2062,
      "step": 3520
    },
    {
      "epoch": 1.2526614620298084,
      "grad_norm": 10.138749122619629,
      "learning_rate": 7.498225691980129e-05,
      "loss": 3.2975,
      "step": 3530
    },
    {
      "epoch": 1.2562100780695529,
      "grad_norm": 8.75377082824707,
      "learning_rate": 7.491128459900639e-05,
      "loss": 3.3038,
      "step": 3540
    },
    {
      "epoch": 1.2597586941092973,
      "grad_norm": 9.847631454467773,
      "learning_rate": 7.48403122782115e-05,
      "loss": 3.3584,
      "step": 3550
    },
    {
      "epoch": 1.2633073101490417,
      "grad_norm": 10.759860038757324,
      "learning_rate": 7.47693399574166e-05,
      "loss": 3.3543,
      "step": 3560
    },
    {
      "epoch": 1.2668559261887864,
      "grad_norm": 9.777425765991211,
      "learning_rate": 7.469836763662173e-05,
      "loss": 3.3473,
      "step": 3570
    },
    {
      "epoch": 1.2704045422285308,
      "grad_norm": 9.621865272521973,
      "learning_rate": 7.462739531582683e-05,
      "loss": 3.3083,
      "step": 3580
    },
    {
      "epoch": 1.2739531582682755,
      "grad_norm": 10.481989860534668,
      "learning_rate": 7.455642299503194e-05,
      "loss": 3.3479,
      "step": 3590
    },
    {
      "epoch": 1.27750177430802,
      "grad_norm": 12.883609771728516,
      "learning_rate": 7.448545067423705e-05,
      "loss": 3.2501,
      "step": 3600
    },
    {
      "epoch": 1.2810503903477644,
      "grad_norm": 9.71389102935791,
      "learning_rate": 7.441447835344217e-05,
      "loss": 3.2246,
      "step": 3610
    },
    {
      "epoch": 1.2845990063875088,
      "grad_norm": 10.294133186340332,
      "learning_rate": 7.434350603264727e-05,
      "loss": 3.29,
      "step": 3620
    },
    {
      "epoch": 1.2881476224272534,
      "grad_norm": 22.12979507446289,
      "learning_rate": 7.427253371185238e-05,
      "loss": 3.2111,
      "step": 3630
    },
    {
      "epoch": 1.2916962384669979,
      "grad_norm": 8.401159286499023,
      "learning_rate": 7.420156139105749e-05,
      "loss": 3.2523,
      "step": 3640
    },
    {
      "epoch": 1.2952448545067423,
      "grad_norm": 11.75516128540039,
      "learning_rate": 7.41305890702626e-05,
      "loss": 3.3331,
      "step": 3650
    },
    {
      "epoch": 1.298793470546487,
      "grad_norm": 9.529879570007324,
      "learning_rate": 7.40596167494677e-05,
      "loss": 3.2652,
      "step": 3660
    },
    {
      "epoch": 1.3023420865862314,
      "grad_norm": 9.360501289367676,
      "learning_rate": 7.398864442867283e-05,
      "loss": 3.2539,
      "step": 3670
    },
    {
      "epoch": 1.3058907026259758,
      "grad_norm": 10.1300687789917,
      "learning_rate": 7.391767210787793e-05,
      "loss": 3.2544,
      "step": 3680
    },
    {
      "epoch": 1.3094393186657203,
      "grad_norm": 10.982176780700684,
      "learning_rate": 7.384669978708304e-05,
      "loss": 3.3181,
      "step": 3690
    },
    {
      "epoch": 1.312987934705465,
      "grad_norm": 10.452102661132812,
      "learning_rate": 7.377572746628814e-05,
      "loss": 3.3307,
      "step": 3700
    },
    {
      "epoch": 1.3165365507452094,
      "grad_norm": 10.127009391784668,
      "learning_rate": 7.370475514549327e-05,
      "loss": 3.3308,
      "step": 3710
    },
    {
      "epoch": 1.3200851667849538,
      "grad_norm": 9.635165214538574,
      "learning_rate": 7.363378282469837e-05,
      "loss": 3.3466,
      "step": 3720
    },
    {
      "epoch": 1.3236337828246985,
      "grad_norm": 9.182039260864258,
      "learning_rate": 7.356281050390348e-05,
      "loss": 3.2663,
      "step": 3730
    },
    {
      "epoch": 1.327182398864443,
      "grad_norm": 10.802266120910645,
      "learning_rate": 7.349183818310859e-05,
      "loss": 3.2486,
      "step": 3740
    },
    {
      "epoch": 1.3307310149041873,
      "grad_norm": 11.965949058532715,
      "learning_rate": 7.34208658623137e-05,
      "loss": 3.297,
      "step": 3750
    },
    {
      "epoch": 1.3342796309439318,
      "grad_norm": 9.362010955810547,
      "learning_rate": 7.334989354151882e-05,
      "loss": 3.2381,
      "step": 3760
    },
    {
      "epoch": 1.3378282469836764,
      "grad_norm": 9.172983169555664,
      "learning_rate": 7.327892122072392e-05,
      "loss": 3.2158,
      "step": 3770
    },
    {
      "epoch": 1.3413768630234209,
      "grad_norm": 11.673883438110352,
      "learning_rate": 7.320794889992903e-05,
      "loss": 3.2294,
      "step": 3780
    },
    {
      "epoch": 1.3449254790631653,
      "grad_norm": 10.99282169342041,
      "learning_rate": 7.313697657913414e-05,
      "loss": 3.3221,
      "step": 3790
    },
    {
      "epoch": 1.34847409510291,
      "grad_norm": 9.030714988708496,
      "learning_rate": 7.306600425833925e-05,
      "loss": 3.2761,
      "step": 3800
    },
    {
      "epoch": 1.3520227111426544,
      "grad_norm": 9.093748092651367,
      "learning_rate": 7.299503193754435e-05,
      "loss": 3.2079,
      "step": 3810
    },
    {
      "epoch": 1.3555713271823988,
      "grad_norm": 9.998172760009766,
      "learning_rate": 7.292405961674948e-05,
      "loss": 3.2946,
      "step": 3820
    },
    {
      "epoch": 1.3591199432221432,
      "grad_norm": 9.391044616699219,
      "learning_rate": 7.285308729595458e-05,
      "loss": 3.2675,
      "step": 3830
    },
    {
      "epoch": 1.362668559261888,
      "grad_norm": 9.272432327270508,
      "learning_rate": 7.278211497515969e-05,
      "loss": 3.1912,
      "step": 3840
    },
    {
      "epoch": 1.3662171753016323,
      "grad_norm": 9.682634353637695,
      "learning_rate": 7.27111426543648e-05,
      "loss": 3.0843,
      "step": 3850
    },
    {
      "epoch": 1.369765791341377,
      "grad_norm": 10.02372932434082,
      "learning_rate": 7.264017033356992e-05,
      "loss": 3.2926,
      "step": 3860
    },
    {
      "epoch": 1.3733144073811214,
      "grad_norm": 11.605681419372559,
      "learning_rate": 7.256919801277502e-05,
      "loss": 3.2821,
      "step": 3870
    },
    {
      "epoch": 1.3768630234208659,
      "grad_norm": 8.609366416931152,
      "learning_rate": 7.249822569198013e-05,
      "loss": 3.279,
      "step": 3880
    },
    {
      "epoch": 1.3804116394606103,
      "grad_norm": 10.421005249023438,
      "learning_rate": 7.242725337118524e-05,
      "loss": 3.2702,
      "step": 3890
    },
    {
      "epoch": 1.3839602555003547,
      "grad_norm": 9.827675819396973,
      "learning_rate": 7.235628105039035e-05,
      "loss": 3.2435,
      "step": 3900
    },
    {
      "epoch": 1.3875088715400994,
      "grad_norm": 9.448452949523926,
      "learning_rate": 7.228530872959545e-05,
      "loss": 3.2332,
      "step": 3910
    },
    {
      "epoch": 1.3910574875798438,
      "grad_norm": 9.732194900512695,
      "learning_rate": 7.221433640880058e-05,
      "loss": 3.2391,
      "step": 3920
    },
    {
      "epoch": 1.3946061036195885,
      "grad_norm": 8.424123764038086,
      "learning_rate": 7.214336408800568e-05,
      "loss": 3.1527,
      "step": 3930
    },
    {
      "epoch": 1.398154719659333,
      "grad_norm": 10.720234870910645,
      "learning_rate": 7.207239176721079e-05,
      "loss": 3.1889,
      "step": 3940
    },
    {
      "epoch": 1.4017033356990773,
      "grad_norm": 9.881061553955078,
      "learning_rate": 7.200141944641589e-05,
      "loss": 3.2454,
      "step": 3950
    },
    {
      "epoch": 1.4052519517388218,
      "grad_norm": 9.98976993560791,
      "learning_rate": 7.193044712562102e-05,
      "loss": 3.2456,
      "step": 3960
    },
    {
      "epoch": 1.4088005677785664,
      "grad_norm": 11.949178695678711,
      "learning_rate": 7.185947480482612e-05,
      "loss": 3.2047,
      "step": 3970
    },
    {
      "epoch": 1.4123491838183109,
      "grad_norm": 16.860395431518555,
      "learning_rate": 7.178850248403123e-05,
      "loss": 3.2968,
      "step": 3980
    },
    {
      "epoch": 1.4158977998580553,
      "grad_norm": 9.249663352966309,
      "learning_rate": 7.171753016323634e-05,
      "loss": 3.2618,
      "step": 3990
    },
    {
      "epoch": 1.4194464158978,
      "grad_norm": 9.65200424194336,
      "learning_rate": 7.164655784244145e-05,
      "loss": 3.2457,
      "step": 4000
    }
  ],
  "logging_steps": 10,
  "max_steps": 14090,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 2208787857408000.0,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
