{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 4.995555555555556,
  "eval_steps": 500,
  "global_step": 2810,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.017777777777777778,
      "grad_norm": 130.47959899902344,
      "learning_rate": 9.97864768683274e-05,
      "loss": 32.2992,
      "step": 10
    },
    {
      "epoch": 0.035555555555555556,
      "grad_norm": 127.93404388427734,
      "learning_rate": 9.943060498220641e-05,
      "loss": 30.9383,
      "step": 20
    },
    {
      "epoch": 0.05333333333333334,
      "grad_norm": 126.42113494873047,
      "learning_rate": 9.907473309608542e-05,
      "loss": 31.3993,
      "step": 30
    },
    {
      "epoch": 0.07111111111111111,
      "grad_norm": 127.86618041992188,
      "learning_rate": 9.871886120996443e-05,
      "loss": 30.656,
      "step": 40
    },
    {
      "epoch": 0.08888888888888889,
      "grad_norm": 124.04180908203125,
      "learning_rate": 9.836298932384342e-05,
      "loss": 29.6472,
      "step": 50
    },
    {
      "epoch": 0.10666666666666667,
      "grad_norm": 126.05084228515625,
      "learning_rate": 9.800711743772243e-05,
      "loss": 28.8011,
      "step": 60
    },
    {
      "epoch": 0.12444444444444444,
      "grad_norm": 121.87125396728516,
      "learning_rate": 9.765124555160144e-05,
      "loss": 28.6463,
      "step": 70
    },
    {
      "epoch": 0.14222222222222222,
      "grad_norm": 120.0839614868164,
      "learning_rate": 9.729537366548043e-05,
      "loss": 28.2829,
      "step": 80
    },
    {
      "epoch": 0.16,
      "grad_norm": 128.69825744628906,
      "learning_rate": 9.693950177935944e-05,
      "loss": 28.3218,
      "step": 90
    },
    {
      "epoch": 0.17777777777777778,
      "grad_norm": 120.32367706298828,
      "learning_rate": 9.658362989323843e-05,
      "loss": 27.3729,
      "step": 100
    },
    {
      "epoch": 0.19555555555555557,
      "grad_norm": 117.18570709228516,
      "learning_rate": 9.622775800711744e-05,
      "loss": 27.9221,
      "step": 110
    },
    {
      "epoch": 0.21333333333333335,
      "grad_norm": 120.34906005859375,
      "learning_rate": 9.587188612099645e-05,
      "loss": 27.0327,
      "step": 120
    },
    {
      "epoch": 0.2311111111111111,
      "grad_norm": 114.76557922363281,
      "learning_rate": 9.551601423487546e-05,
      "loss": 26.4885,
      "step": 130
    },
    {
      "epoch": 0.24888888888888888,
      "grad_norm": 99.99185180664062,
      "learning_rate": 9.516014234875445e-05,
      "loss": 25.3532,
      "step": 140
    },
    {
      "epoch": 0.26666666666666666,
      "grad_norm": 111.65331268310547,
      "learning_rate": 9.480427046263346e-05,
      "loss": 25.3531,
      "step": 150
    },
    {
      "epoch": 0.28444444444444444,
      "grad_norm": 99.39595031738281,
      "learning_rate": 9.444839857651247e-05,
      "loss": 24.671,
      "step": 160
    },
    {
      "epoch": 0.3022222222222222,
      "grad_norm": 107.93409729003906,
      "learning_rate": 9.409252669039146e-05,
      "loss": 25.154,
      "step": 170
    },
    {
      "epoch": 0.32,
      "grad_norm": 112.23552703857422,
      "learning_rate": 9.373665480427047e-05,
      "loss": 24.2119,
      "step": 180
    },
    {
      "epoch": 0.3377777777777778,
      "grad_norm": 112.16268157958984,
      "learning_rate": 9.338078291814946e-05,
      "loss": 23.8633,
      "step": 190
    },
    {
      "epoch": 0.35555555555555557,
      "grad_norm": 100.48796081542969,
      "learning_rate": 9.302491103202847e-05,
      "loss": 23.0518,
      "step": 200
    },
    {
      "epoch": 0.37333333333333335,
      "grad_norm": 89.44242095947266,
      "learning_rate": 9.266903914590748e-05,
      "loss": 22.8494,
      "step": 210
    },
    {
      "epoch": 0.39111111111111113,
      "grad_norm": 91.15386199951172,
      "learning_rate": 9.231316725978649e-05,
      "loss": 23.016,
      "step": 220
    },
    {
      "epoch": 0.4088888888888889,
      "grad_norm": 92.38347625732422,
      "learning_rate": 9.195729537366548e-05,
      "loss": 22.6661,
      "step": 230
    },
    {
      "epoch": 0.4266666666666667,
      "grad_norm": 91.51575469970703,
      "learning_rate": 9.160142348754449e-05,
      "loss": 22.7247,
      "step": 240
    },
    {
      "epoch": 0.4444444444444444,
      "grad_norm": 98.6125259399414,
      "learning_rate": 9.12455516014235e-05,
      "loss": 21.8544,
      "step": 250
    },
    {
      "epoch": 0.4622222222222222,
      "grad_norm": 75.78423309326172,
      "learning_rate": 9.08896797153025e-05,
      "loss": 21.4784,
      "step": 260
    },
    {
      "epoch": 0.48,
      "grad_norm": 83.44158935546875,
      "learning_rate": 9.05338078291815e-05,
      "loss": 21.6506,
      "step": 270
    },
    {
      "epoch": 0.49777777777777776,
      "grad_norm": 73.52904510498047,
      "learning_rate": 9.01779359430605e-05,
      "loss": 21.2033,
      "step": 280
    },
    {
      "epoch": 0.5155555555555555,
      "grad_norm": 74.22520446777344,
      "learning_rate": 8.98220640569395e-05,
      "loss": 21.1733,
      "step": 290
    },
    {
      "epoch": 0.5333333333333333,
      "grad_norm": 82.70878601074219,
      "learning_rate": 8.946619217081851e-05,
      "loss": 20.9291,
      "step": 300
    },
    {
      "epoch": 0.5511111111111111,
      "grad_norm": 61.97577667236328,
      "learning_rate": 8.911032028469752e-05,
      "loss": 20.1412,
      "step": 310
    },
    {
      "epoch": 0.5688888888888889,
      "grad_norm": 53.26360321044922,
      "learning_rate": 8.875444839857651e-05,
      "loss": 20.1124,
      "step": 320
    },
    {
      "epoch": 0.5866666666666667,
      "grad_norm": 57.0326042175293,
      "learning_rate": 8.839857651245552e-05,
      "loss": 20.1666,
      "step": 330
    },
    {
      "epoch": 0.6044444444444445,
      "grad_norm": 59.19154357910156,
      "learning_rate": 8.804270462633453e-05,
      "loss": 20.2395,
      "step": 340
    },
    {
      "epoch": 0.6222222222222222,
      "grad_norm": 54.00483322143555,
      "learning_rate": 8.768683274021354e-05,
      "loss": 19.5397,
      "step": 350
    },
    {
      "epoch": 0.64,
      "grad_norm": 55.251094818115234,
      "learning_rate": 8.733096085409253e-05,
      "loss": 18.8208,
      "step": 360
    },
    {
      "epoch": 0.6577777777777778,
      "grad_norm": 61.321292877197266,
      "learning_rate": 8.697508896797153e-05,
      "loss": 19.2662,
      "step": 370
    },
    {
      "epoch": 0.6755555555555556,
      "grad_norm": 58.21697998046875,
      "learning_rate": 8.661921708185053e-05,
      "loss": 18.5204,
      "step": 380
    },
    {
      "epoch": 0.6933333333333334,
      "grad_norm": 46.37819290161133,
      "learning_rate": 8.626334519572954e-05,
      "loss": 18.5968,
      "step": 390
    },
    {
      "epoch": 0.7111111111111111,
      "grad_norm": 64.71602630615234,
      "learning_rate": 8.590747330960855e-05,
      "loss": 18.2746,
      "step": 400
    },
    {
      "epoch": 0.7288888888888889,
      "grad_norm": 61.06301498413086,
      "learning_rate": 8.555160142348754e-05,
      "loss": 18.4807,
      "step": 410
    },
    {
      "epoch": 0.7466666666666667,
      "grad_norm": 36.2509765625,
      "learning_rate": 8.519572953736655e-05,
      "loss": 18.3914,
      "step": 420
    },
    {
      "epoch": 0.7644444444444445,
      "grad_norm": 36.98108673095703,
      "learning_rate": 8.483985765124556e-05,
      "loss": 18.2512,
      "step": 430
    },
    {
      "epoch": 0.7822222222222223,
      "grad_norm": 44.056732177734375,
      "learning_rate": 8.448398576512457e-05,
      "loss": 17.2027,
      "step": 440
    },
    {
      "epoch": 0.8,
      "grad_norm": 40.091129302978516,
      "learning_rate": 8.412811387900356e-05,
      "loss": 17.7298,
      "step": 450
    },
    {
      "epoch": 0.8177777777777778,
      "grad_norm": 46.040550231933594,
      "learning_rate": 8.377224199288256e-05,
      "loss": 17.2657,
      "step": 460
    },
    {
      "epoch": 0.8355555555555556,
      "grad_norm": 35.65753936767578,
      "learning_rate": 8.341637010676157e-05,
      "loss": 17.3776,
      "step": 470
    },
    {
      "epoch": 0.8533333333333334,
      "grad_norm": 46.55960464477539,
      "learning_rate": 8.306049822064057e-05,
      "loss": 17.053,
      "step": 480
    },
    {
      "epoch": 0.8711111111111111,
      "grad_norm": 32.86063766479492,
      "learning_rate": 8.270462633451958e-05,
      "loss": 17.3993,
      "step": 490
    },
    {
      "epoch": 0.8888888888888888,
      "grad_norm": 36.03329086303711,
      "learning_rate": 8.234875444839858e-05,
      "loss": 17.0751,
      "step": 500
    },
    {
      "epoch": 0.9066666666666666,
      "grad_norm": 49.6606330871582,
      "learning_rate": 8.199288256227758e-05,
      "loss": 16.8154,
      "step": 510
    },
    {
      "epoch": 0.9244444444444444,
      "grad_norm": 39.240638732910156,
      "learning_rate": 8.163701067615659e-05,
      "loss": 16.6249,
      "step": 520
    },
    {
      "epoch": 0.9422222222222222,
      "grad_norm": 37.31173324584961,
      "learning_rate": 8.12811387900356e-05,
      "loss": 16.4826,
      "step": 530
    },
    {
      "epoch": 0.96,
      "grad_norm": 53.84469223022461,
      "learning_rate": 8.09252669039146e-05,
      "loss": 16.5295,
      "step": 540
    },
    {
      "epoch": 0.9777777777777777,
      "grad_norm": 43.719905853271484,
      "learning_rate": 8.05693950177936e-05,
      "loss": 16.2283,
      "step": 550
    },
    {
      "epoch": 0.9955555555555555,
      "grad_norm": 43.117340087890625,
      "learning_rate": 8.02135231316726e-05,
      "loss": 16.2876,
      "step": 560
    },
    {
      "epoch": 0.9991111111111111,
      "eval_bp": 1.0,
      "eval_counts": [
        859,
        68,
        16,
        7
      ],
      "eval_loss": 16.295804977416992,
      "eval_precisions": [
        0.34403623794972027,
        0.02728918104365065,
        0.006433893752287048,
        0.0028204993895633466
      ],
      "eval_ref_len": 14293,
      "eval_runtime": 1707.6936,
      "eval_samples_per_second": 0.293,
      "eval_score": 0.020316488508132374,
      "eval_steps_per_second": 0.146,
      "eval_sys_len": 249683,
      "eval_totals": [
        249683,
        249183,
        248683,
        248183
      ],
      "step": 562
    },
    {
      "epoch": 1.0133333333333334,
      "grad_norm": 30.70945167541504,
      "learning_rate": 7.98576512455516e-05,
      "loss": 15.862,
      "step": 570
    },
    {
      "epoch": 1.031111111111111,
      "grad_norm": 44.14080810546875,
      "learning_rate": 7.950177935943061e-05,
      "loss": 15.8799,
      "step": 580
    },
    {
      "epoch": 1.048888888888889,
      "grad_norm": 32.376461029052734,
      "learning_rate": 7.91459074733096e-05,
      "loss": 16.0257,
      "step": 590
    },
    {
      "epoch": 1.0666666666666667,
      "grad_norm": 43.73392105102539,
      "learning_rate": 7.879003558718861e-05,
      "loss": 15.809,
      "step": 600
    },
    {
      "epoch": 1.0844444444444445,
      "grad_norm": 30.274051666259766,
      "learning_rate": 7.843416370106762e-05,
      "loss": 15.6931,
      "step": 610
    },
    {
      "epoch": 1.1022222222222222,
      "grad_norm": 41.53611755371094,
      "learning_rate": 7.807829181494663e-05,
      "loss": 15.7271,
      "step": 620
    },
    {
      "epoch": 1.12,
      "grad_norm": 32.62147903442383,
      "learning_rate": 7.772241992882563e-05,
      "loss": 15.4418,
      "step": 630
    },
    {
      "epoch": 1.1377777777777778,
      "grad_norm": 33.20610046386719,
      "learning_rate": 7.736654804270463e-05,
      "loss": 15.0443,
      "step": 640
    },
    {
      "epoch": 1.1555555555555554,
      "grad_norm": 34.334590911865234,
      "learning_rate": 7.701067615658364e-05,
      "loss": 15.8596,
      "step": 650
    },
    {
      "epoch": 1.1733333333333333,
      "grad_norm": 36.34880447387695,
      "learning_rate": 7.665480427046264e-05,
      "loss": 14.8373,
      "step": 660
    },
    {
      "epoch": 1.1911111111111112,
      "grad_norm": 34.56939697265625,
      "learning_rate": 7.629893238434164e-05,
      "loss": 15.0733,
      "step": 670
    },
    {
      "epoch": 1.208888888888889,
      "grad_norm": 34.792640686035156,
      "learning_rate": 7.594306049822064e-05,
      "loss": 14.9426,
      "step": 680
    },
    {
      "epoch": 1.2266666666666666,
      "grad_norm": 34.12553024291992,
      "learning_rate": 7.558718861209965e-05,
      "loss": 14.9018,
      "step": 690
    },
    {
      "epoch": 1.2444444444444445,
      "grad_norm": 38.77985763549805,
      "learning_rate": 7.523131672597865e-05,
      "loss": 14.7144,
      "step": 700
    },
    {
      "epoch": 1.2622222222222224,
      "grad_norm": 50.16450881958008,
      "learning_rate": 7.487544483985766e-05,
      "loss": 14.575,
      "step": 710
    },
    {
      "epoch": 1.28,
      "grad_norm": 39.543800354003906,
      "learning_rate": 7.451957295373666e-05,
      "loss": 14.5527,
      "step": 720
    },
    {
      "epoch": 1.2977777777777777,
      "grad_norm": 27.691679000854492,
      "learning_rate": 7.416370106761566e-05,
      "loss": 14.6319,
      "step": 730
    },
    {
      "epoch": 1.3155555555555556,
      "grad_norm": 29.27785873413086,
      "learning_rate": 7.380782918149467e-05,
      "loss": 14.4605,
      "step": 740
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 44.7338981628418,
      "learning_rate": 7.345195729537368e-05,
      "loss": 14.3893,
      "step": 750
    },
    {
      "epoch": 1.3511111111111112,
      "grad_norm": 32.87884521484375,
      "learning_rate": 7.309608540925267e-05,
      "loss": 14.3246,
      "step": 760
    },
    {
      "epoch": 1.3688888888888888,
      "grad_norm": 30.083282470703125,
      "learning_rate": 7.274021352313167e-05,
      "loss": 13.7904,
      "step": 770
    },
    {
      "epoch": 1.3866666666666667,
      "grad_norm": 38.37968063354492,
      "learning_rate": 7.238434163701068e-05,
      "loss": 13.7711,
      "step": 780
    },
    {
      "epoch": 1.4044444444444444,
      "grad_norm": 40.73179626464844,
      "learning_rate": 7.202846975088968e-05,
      "loss": 13.9175,
      "step": 790
    },
    {
      "epoch": 1.4222222222222223,
      "grad_norm": 31.985736846923828,
      "learning_rate": 7.167259786476869e-05,
      "loss": 14.0452,
      "step": 800
    },
    {
      "epoch": 1.44,
      "grad_norm": 28.62208366394043,
      "learning_rate": 7.131672597864769e-05,
      "loss": 13.4762,
      "step": 810
    },
    {
      "epoch": 1.4577777777777778,
      "grad_norm": 31.6624755859375,
      "learning_rate": 7.09608540925267e-05,
      "loss": 13.4435,
      "step": 820
    },
    {
      "epoch": 1.4755555555555555,
      "grad_norm": 28.531965255737305,
      "learning_rate": 7.06049822064057e-05,
      "loss": 13.6356,
      "step": 830
    },
    {
      "epoch": 1.4933333333333334,
      "grad_norm": 35.76803970336914,
      "learning_rate": 7.024911032028471e-05,
      "loss": 13.6668,
      "step": 840
    },
    {
      "epoch": 1.511111111111111,
      "grad_norm": 25.498554229736328,
      "learning_rate": 6.98932384341637e-05,
      "loss": 13.5015,
      "step": 850
    },
    {
      "epoch": 1.528888888888889,
      "grad_norm": 32.088218688964844,
      "learning_rate": 6.95373665480427e-05,
      "loss": 13.4866,
      "step": 860
    },
    {
      "epoch": 1.5466666666666666,
      "grad_norm": 45.24491882324219,
      "learning_rate": 6.918149466192171e-05,
      "loss": 13.1388,
      "step": 870
    },
    {
      "epoch": 1.5644444444444443,
      "grad_norm": 28.462419509887695,
      "learning_rate": 6.882562277580072e-05,
      "loss": 13.1429,
      "step": 880
    },
    {
      "epoch": 1.5822222222222222,
      "grad_norm": 32.07488250732422,
      "learning_rate": 6.846975088967972e-05,
      "loss": 13.3046,
      "step": 890
    },
    {
      "epoch": 1.6,
      "grad_norm": 29.026945114135742,
      "learning_rate": 6.811387900355872e-05,
      "loss": 13.1356,
      "step": 900
    },
    {
      "epoch": 1.6177777777777778,
      "grad_norm": 29.95383644104004,
      "learning_rate": 6.775800711743773e-05,
      "loss": 13.3301,
      "step": 910
    },
    {
      "epoch": 1.6355555555555554,
      "grad_norm": 27.711240768432617,
      "learning_rate": 6.740213523131673e-05,
      "loss": 12.9448,
      "step": 920
    },
    {
      "epoch": 1.6533333333333333,
      "grad_norm": 31.174863815307617,
      "learning_rate": 6.704626334519574e-05,
      "loss": 12.7871,
      "step": 930
    },
    {
      "epoch": 1.6711111111111112,
      "grad_norm": 26.116024017333984,
      "learning_rate": 6.669039145907474e-05,
      "loss": 12.8309,
      "step": 940
    },
    {
      "epoch": 1.6888888888888889,
      "grad_norm": 25.336580276489258,
      "learning_rate": 6.633451957295373e-05,
      "loss": 12.6127,
      "step": 950
    },
    {
      "epoch": 1.7066666666666666,
      "grad_norm": 42.235721588134766,
      "learning_rate": 6.597864768683274e-05,
      "loss": 12.7133,
      "step": 960
    },
    {
      "epoch": 1.7244444444444444,
      "grad_norm": 30.217313766479492,
      "learning_rate": 6.562277580071175e-05,
      "loss": 12.4555,
      "step": 970
    },
    {
      "epoch": 1.7422222222222223,
      "grad_norm": 36.01628112792969,
      "learning_rate": 6.526690391459076e-05,
      "loss": 12.4069,
      "step": 980
    },
    {
      "epoch": 1.76,
      "grad_norm": 30.245038986206055,
      "learning_rate": 6.491103202846975e-05,
      "loss": 12.4716,
      "step": 990
    },
    {
      "epoch": 1.7777777777777777,
      "grad_norm": 25.597431182861328,
      "learning_rate": 6.455516014234876e-05,
      "loss": 12.3109,
      "step": 1000
    },
    {
      "epoch": 1.7955555555555556,
      "grad_norm": 28.502784729003906,
      "learning_rate": 6.419928825622777e-05,
      "loss": 12.3755,
      "step": 1010
    },
    {
      "epoch": 1.8133333333333335,
      "grad_norm": 29.47715187072754,
      "learning_rate": 6.384341637010677e-05,
      "loss": 12.467,
      "step": 1020
    },
    {
      "epoch": 1.8311111111111111,
      "grad_norm": 24.317432403564453,
      "learning_rate": 6.348754448398577e-05,
      "loss": 11.8942,
      "step": 1030
    },
    {
      "epoch": 1.8488888888888888,
      "grad_norm": 25.8261661529541,
      "learning_rate": 6.313167259786478e-05,
      "loss": 12.2077,
      "step": 1040
    },
    {
      "epoch": 1.8666666666666667,
      "grad_norm": 34.02305603027344,
      "learning_rate": 6.277580071174377e-05,
      "loss": 12.1064,
      "step": 1050
    },
    {
      "epoch": 1.8844444444444446,
      "grad_norm": 55.30485534667969,
      "learning_rate": 6.241992882562278e-05,
      "loss": 12.3817,
      "step": 1060
    },
    {
      "epoch": 1.9022222222222223,
      "grad_norm": 26.50566291809082,
      "learning_rate": 6.206405693950179e-05,
      "loss": 12.0281,
      "step": 1070
    },
    {
      "epoch": 1.92,
      "grad_norm": 22.593517303466797,
      "learning_rate": 6.170818505338078e-05,
      "loss": 11.8277,
      "step": 1080
    },
    {
      "epoch": 1.9377777777777778,
      "grad_norm": 23.12999725341797,
      "learning_rate": 6.135231316725979e-05,
      "loss": 12.0048,
      "step": 1090
    },
    {
      "epoch": 1.9555555555555557,
      "grad_norm": 36.51508331298828,
      "learning_rate": 6.09964412811388e-05,
      "loss": 11.8965,
      "step": 1100
    },
    {
      "epoch": 1.9733333333333334,
      "grad_norm": 31.320663452148438,
      "learning_rate": 6.06405693950178e-05,
      "loss": 11.9779,
      "step": 1110
    },
    {
      "epoch": 1.991111111111111,
      "grad_norm": 27.30075454711914,
      "learning_rate": 6.0284697508896806e-05,
      "loss": 11.8158,
      "step": 1120
    },
    {
      "epoch": 2.0,
      "eval_bp": 1.0,
      "eval_counts": [
        696,
        60,
        14,
        5
      ],
      "eval_loss": 11.676773071289062,
      "eval_precisions": [
        0.2900108754078278,
        0.0250531335206751,
        0.005857936667336145,
        0.0020964888005568275
      ],
      "eval_ref_len": 14293,
      "eval_runtime": 1704.5269,
      "eval_samples_per_second": 0.293,
      "eval_score": 0.017283368473348607,
      "eval_steps_per_second": 0.147,
      "eval_sys_len": 239991,
      "eval_totals": [
        239991,
        239491,
        238992,
        238494
      ],
      "step": 1125
    },
    {
      "epoch": 2.008888888888889,
      "grad_norm": 26.081085205078125,
      "learning_rate": 5.992882562277581e-05,
      "loss": 11.6076,
      "step": 1130
    },
    {
      "epoch": 2.026666666666667,
      "grad_norm": 23.35144805908203,
      "learning_rate": 5.95729537366548e-05,
      "loss": 11.6327,
      "step": 1140
    },
    {
      "epoch": 2.0444444444444443,
      "grad_norm": 22.673608779907227,
      "learning_rate": 5.921708185053381e-05,
      "loss": 11.613,
      "step": 1150
    },
    {
      "epoch": 2.062222222222222,
      "grad_norm": 25.43562889099121,
      "learning_rate": 5.886120996441281e-05,
      "loss": 11.5486,
      "step": 1160
    },
    {
      "epoch": 2.08,
      "grad_norm": 22.45682716369629,
      "learning_rate": 5.850533807829182e-05,
      "loss": 11.3369,
      "step": 1170
    },
    {
      "epoch": 2.097777777777778,
      "grad_norm": 24.830516815185547,
      "learning_rate": 5.814946619217082e-05,
      "loss": 11.4969,
      "step": 1180
    },
    {
      "epoch": 2.1155555555555554,
      "grad_norm": 27.013526916503906,
      "learning_rate": 5.779359430604983e-05,
      "loss": 11.6557,
      "step": 1190
    },
    {
      "epoch": 2.1333333333333333,
      "grad_norm": 37.980167388916016,
      "learning_rate": 5.743772241992883e-05,
      "loss": 11.3683,
      "step": 1200
    },
    {
      "epoch": 2.151111111111111,
      "grad_norm": 28.010059356689453,
      "learning_rate": 5.708185053380784e-05,
      "loss": 11.1911,
      "step": 1210
    },
    {
      "epoch": 2.168888888888889,
      "grad_norm": 29.75848388671875,
      "learning_rate": 5.672597864768684e-05,
      "loss": 11.5407,
      "step": 1220
    },
    {
      "epoch": 2.1866666666666665,
      "grad_norm": 25.789941787719727,
      "learning_rate": 5.6370106761565846e-05,
      "loss": 11.4433,
      "step": 1230
    },
    {
      "epoch": 2.2044444444444444,
      "grad_norm": 30.373371124267578,
      "learning_rate": 5.601423487544484e-05,
      "loss": 11.1953,
      "step": 1240
    },
    {
      "epoch": 2.2222222222222223,
      "grad_norm": 28.108182907104492,
      "learning_rate": 5.565836298932384e-05,
      "loss": 10.9022,
      "step": 1250
    },
    {
      "epoch": 2.24,
      "grad_norm": 26.731624603271484,
      "learning_rate": 5.530249110320285e-05,
      "loss": 11.0,
      "step": 1260
    },
    {
      "epoch": 2.2577777777777777,
      "grad_norm": 20.21868324279785,
      "learning_rate": 5.494661921708185e-05,
      "loss": 11.0862,
      "step": 1270
    },
    {
      "epoch": 2.2755555555555556,
      "grad_norm": 22.74455451965332,
      "learning_rate": 5.459074733096086e-05,
      "loss": 11.0413,
      "step": 1280
    },
    {
      "epoch": 2.2933333333333334,
      "grad_norm": 20.647296905517578,
      "learning_rate": 5.423487544483986e-05,
      "loss": 10.8093,
      "step": 1290
    },
    {
      "epoch": 2.311111111111111,
      "grad_norm": 24.390762329101562,
      "learning_rate": 5.387900355871887e-05,
      "loss": 11.0617,
      "step": 1300
    },
    {
      "epoch": 2.328888888888889,
      "grad_norm": 21.854280471801758,
      "learning_rate": 5.352313167259787e-05,
      "loss": 10.9158,
      "step": 1310
    },
    {
      "epoch": 2.3466666666666667,
      "grad_norm": 26.85816764831543,
      "learning_rate": 5.316725978647688e-05,
      "loss": 10.7009,
      "step": 1320
    },
    {
      "epoch": 2.3644444444444446,
      "grad_norm": 22.80715560913086,
      "learning_rate": 5.281138790035588e-05,
      "loss": 10.8981,
      "step": 1330
    },
    {
      "epoch": 2.3822222222222225,
      "grad_norm": 22.91512680053711,
      "learning_rate": 5.245551601423487e-05,
      "loss": 10.7303,
      "step": 1340
    },
    {
      "epoch": 2.4,
      "grad_norm": 21.093833923339844,
      "learning_rate": 5.209964412811388e-05,
      "loss": 10.7904,
      "step": 1350
    },
    {
      "epoch": 2.417777777777778,
      "grad_norm": 24.081096649169922,
      "learning_rate": 5.174377224199288e-05,
      "loss": 10.655,
      "step": 1360
    },
    {
      "epoch": 2.4355555555555557,
      "grad_norm": 21.591337203979492,
      "learning_rate": 5.138790035587189e-05,
      "loss": 10.8431,
      "step": 1370
    },
    {
      "epoch": 2.453333333333333,
      "grad_norm": 18.922271728515625,
      "learning_rate": 5.103202846975089e-05,
      "loss": 10.4371,
      "step": 1380
    },
    {
      "epoch": 2.471111111111111,
      "grad_norm": 27.466554641723633,
      "learning_rate": 5.06761565836299e-05,
      "loss": 10.548,
      "step": 1390
    },
    {
      "epoch": 2.488888888888889,
      "grad_norm": 19.389677047729492,
      "learning_rate": 5.03202846975089e-05,
      "loss": 10.7253,
      "step": 1400
    },
    {
      "epoch": 2.506666666666667,
      "grad_norm": 24.094785690307617,
      "learning_rate": 4.99644128113879e-05,
      "loss": 10.7169,
      "step": 1410
    },
    {
      "epoch": 2.5244444444444447,
      "grad_norm": 31.927640914916992,
      "learning_rate": 4.96085409252669e-05,
      "loss": 10.6516,
      "step": 1420
    },
    {
      "epoch": 2.542222222222222,
      "grad_norm": 17.715194702148438,
      "learning_rate": 4.925266903914591e-05,
      "loss": 10.7043,
      "step": 1430
    },
    {
      "epoch": 2.56,
      "grad_norm": 23.34453773498535,
      "learning_rate": 4.889679715302491e-05,
      "loss": 10.5027,
      "step": 1440
    },
    {
      "epoch": 2.5777777777777775,
      "grad_norm": 18.64597511291504,
      "learning_rate": 4.854092526690392e-05,
      "loss": 10.5142,
      "step": 1450
    },
    {
      "epoch": 2.5955555555555554,
      "grad_norm": 24.191999435424805,
      "learning_rate": 4.818505338078292e-05,
      "loss": 10.7708,
      "step": 1460
    },
    {
      "epoch": 2.6133333333333333,
      "grad_norm": 34.007938385009766,
      "learning_rate": 4.782918149466192e-05,
      "loss": 10.3163,
      "step": 1470
    },
    {
      "epoch": 2.631111111111111,
      "grad_norm": 21.847881317138672,
      "learning_rate": 4.747330960854093e-05,
      "loss": 10.5406,
      "step": 1480
    },
    {
      "epoch": 2.648888888888889,
      "grad_norm": 20.16474723815918,
      "learning_rate": 4.711743772241993e-05,
      "loss": 10.0566,
      "step": 1490
    },
    {
      "epoch": 2.6666666666666665,
      "grad_norm": 17.76719093322754,
      "learning_rate": 4.676156583629894e-05,
      "loss": 10.0407,
      "step": 1500
    },
    {
      "epoch": 2.6844444444444444,
      "grad_norm": 19.77895736694336,
      "learning_rate": 4.6405693950177934e-05,
      "loss": 10.2135,
      "step": 1510
    },
    {
      "epoch": 2.7022222222222223,
      "grad_norm": 17.49154281616211,
      "learning_rate": 4.604982206405694e-05,
      "loss": 10.255,
      "step": 1520
    },
    {
      "epoch": 2.7199999999999998,
      "grad_norm": 20.431486129760742,
      "learning_rate": 4.569395017793594e-05,
      "loss": 10.1935,
      "step": 1530
    },
    {
      "epoch": 2.7377777777777776,
      "grad_norm": 20.05307960510254,
      "learning_rate": 4.533807829181495e-05,
      "loss": 10.1237,
      "step": 1540
    },
    {
      "epoch": 2.7555555555555555,
      "grad_norm": 21.37385368347168,
      "learning_rate": 4.498220640569395e-05,
      "loss": 10.2663,
      "step": 1550
    },
    {
      "epoch": 2.7733333333333334,
      "grad_norm": 16.912012100219727,
      "learning_rate": 4.4626334519572954e-05,
      "loss": 10.3814,
      "step": 1560
    },
    {
      "epoch": 2.7911111111111113,
      "grad_norm": 19.613693237304688,
      "learning_rate": 4.427046263345196e-05,
      "loss": 10.0102,
      "step": 1570
    },
    {
      "epoch": 2.8088888888888888,
      "grad_norm": 20.72358512878418,
      "learning_rate": 4.391459074733096e-05,
      "loss": 9.9775,
      "step": 1580
    },
    {
      "epoch": 2.8266666666666667,
      "grad_norm": 18.846492767333984,
      "learning_rate": 4.355871886120997e-05,
      "loss": 9.9395,
      "step": 1590
    },
    {
      "epoch": 2.8444444444444446,
      "grad_norm": 20.266128540039062,
      "learning_rate": 4.3202846975088965e-05,
      "loss": 10.073,
      "step": 1600
    },
    {
      "epoch": 2.862222222222222,
      "grad_norm": 16.470975875854492,
      "learning_rate": 4.284697508896797e-05,
      "loss": 9.9616,
      "step": 1610
    },
    {
      "epoch": 2.88,
      "grad_norm": 20.903926849365234,
      "learning_rate": 4.2491103202846975e-05,
      "loss": 9.8008,
      "step": 1620
    },
    {
      "epoch": 2.897777777777778,
      "grad_norm": 30.373239517211914,
      "learning_rate": 4.213523131672598e-05,
      "loss": 9.7408,
      "step": 1630
    },
    {
      "epoch": 2.9155555555555557,
      "grad_norm": 26.13499641418457,
      "learning_rate": 4.1779359430604984e-05,
      "loss": 9.925,
      "step": 1640
    },
    {
      "epoch": 2.9333333333333336,
      "grad_norm": 15.93917179107666,
      "learning_rate": 4.1423487544483985e-05,
      "loss": 9.7155,
      "step": 1650
    },
    {
      "epoch": 2.951111111111111,
      "grad_norm": 15.732205390930176,
      "learning_rate": 4.106761565836299e-05,
      "loss": 9.7103,
      "step": 1660
    },
    {
      "epoch": 2.968888888888889,
      "grad_norm": 16.54331398010254,
      "learning_rate": 4.0711743772241994e-05,
      "loss": 9.8722,
      "step": 1670
    },
    {
      "epoch": 2.986666666666667,
      "grad_norm": 18.16838264465332,
      "learning_rate": 4.0355871886121e-05,
      "loss": 9.9008,
      "step": 1680
    },
    {
      "epoch": 2.999111111111111,
      "eval_bp": 1.0,
      "eval_counts": [
        865,
        77,
        15,
        5
      ],
      "eval_loss": 9.606534957885742,
      "eval_precisions": [
        0.38010616606904307,
        0.03391054662039565,
        0.006620499715318512,
        0.0022116945561350197
      ],
      "eval_ref_len": 14293,
      "eval_runtime": 1691.7496,
      "eval_samples_per_second": 0.296,
      "eval_score": 0.020843176509667837,
      "eval_steps_per_second": 0.148,
      "eval_sys_len": 227568,
      "eval_totals": [
        227568,
        227068,
        226569,
        226071
      ],
      "step": 1687
    },
    {
      "epoch": 3.0044444444444443,
      "grad_norm": 17.27091407775879,
      "learning_rate": 4e-05,
      "loss": 9.8332,
      "step": 1690
    },
    {
      "epoch": 3.022222222222222,
      "grad_norm": 19.664743423461914,
      "learning_rate": 3.9644128113879004e-05,
      "loss": 9.7775,
      "step": 1700
    },
    {
      "epoch": 3.04,
      "grad_norm": 16.852853775024414,
      "learning_rate": 3.9288256227758006e-05,
      "loss": 9.7985,
      "step": 1710
    },
    {
      "epoch": 3.057777777777778,
      "grad_norm": 15.8228120803833,
      "learning_rate": 3.8932384341637014e-05,
      "loss": 9.9347,
      "step": 1720
    },
    {
      "epoch": 3.0755555555555554,
      "grad_norm": 17.065279006958008,
      "learning_rate": 3.8576512455516015e-05,
      "loss": 9.6494,
      "step": 1730
    },
    {
      "epoch": 3.0933333333333333,
      "grad_norm": 23.148611068725586,
      "learning_rate": 3.822064056939502e-05,
      "loss": 9.6418,
      "step": 1740
    },
    {
      "epoch": 3.111111111111111,
      "grad_norm": 15.360904693603516,
      "learning_rate": 3.7864768683274024e-05,
      "loss": 9.2756,
      "step": 1750
    },
    {
      "epoch": 3.128888888888889,
      "grad_norm": 15.899922370910645,
      "learning_rate": 3.7508896797153025e-05,
      "loss": 9.6314,
      "step": 1760
    },
    {
      "epoch": 3.1466666666666665,
      "grad_norm": 16.714378356933594,
      "learning_rate": 3.715302491103203e-05,
      "loss": 9.7665,
      "step": 1770
    },
    {
      "epoch": 3.1644444444444444,
      "grad_norm": 21.827638626098633,
      "learning_rate": 3.6797153024911034e-05,
      "loss": 9.5939,
      "step": 1780
    },
    {
      "epoch": 3.1822222222222223,
      "grad_norm": 17.277259826660156,
      "learning_rate": 3.644128113879004e-05,
      "loss": 9.785,
      "step": 1790
    },
    {
      "epoch": 3.2,
      "grad_norm": 21.476505279541016,
      "learning_rate": 3.608540925266904e-05,
      "loss": 9.4871,
      "step": 1800
    },
    {
      "epoch": 3.2177777777777776,
      "grad_norm": 16.94980239868164,
      "learning_rate": 3.5729537366548045e-05,
      "loss": 9.3466,
      "step": 1810
    },
    {
      "epoch": 3.2355555555555555,
      "grad_norm": 18.241491317749023,
      "learning_rate": 3.5373665480427046e-05,
      "loss": 9.421,
      "step": 1820
    },
    {
      "epoch": 3.2533333333333334,
      "grad_norm": 17.136402130126953,
      "learning_rate": 3.5017793594306054e-05,
      "loss": 9.3959,
      "step": 1830
    },
    {
      "epoch": 3.2711111111111113,
      "grad_norm": 21.993812561035156,
      "learning_rate": 3.4661921708185055e-05,
      "loss": 9.4154,
      "step": 1840
    },
    {
      "epoch": 3.2888888888888888,
      "grad_norm": 14.181504249572754,
      "learning_rate": 3.4306049822064056e-05,
      "loss": 9.2516,
      "step": 1850
    },
    {
      "epoch": 3.3066666666666666,
      "grad_norm": 16.70870018005371,
      "learning_rate": 3.3950177935943064e-05,
      "loss": 9.4736,
      "step": 1860
    },
    {
      "epoch": 3.3244444444444445,
      "grad_norm": 15.097347259521484,
      "learning_rate": 3.3594306049822066e-05,
      "loss": 9.2426,
      "step": 1870
    },
    {
      "epoch": 3.3422222222222224,
      "grad_norm": 17.031147003173828,
      "learning_rate": 3.3238434163701074e-05,
      "loss": 9.4239,
      "step": 1880
    },
    {
      "epoch": 3.36,
      "grad_norm": 16.24903106689453,
      "learning_rate": 3.288256227758007e-05,
      "loss": 9.17,
      "step": 1890
    },
    {
      "epoch": 3.3777777777777778,
      "grad_norm": 14.824139595031738,
      "learning_rate": 3.2526690391459076e-05,
      "loss": 9.2408,
      "step": 1900
    },
    {
      "epoch": 3.3955555555555557,
      "grad_norm": 20.40363883972168,
      "learning_rate": 3.217081850533808e-05,
      "loss": 9.1558,
      "step": 1910
    },
    {
      "epoch": 3.413333333333333,
      "grad_norm": 20.95057487487793,
      "learning_rate": 3.1814946619217085e-05,
      "loss": 9.2284,
      "step": 1920
    },
    {
      "epoch": 3.431111111111111,
      "grad_norm": 19.985090255737305,
      "learning_rate": 3.1459074733096086e-05,
      "loss": 9.3101,
      "step": 1930
    },
    {
      "epoch": 3.448888888888889,
      "grad_norm": 16.350156784057617,
      "learning_rate": 3.110320284697509e-05,
      "loss": 9.1408,
      "step": 1940
    },
    {
      "epoch": 3.466666666666667,
      "grad_norm": 22.0856876373291,
      "learning_rate": 3.0747330960854096e-05,
      "loss": 9.2902,
      "step": 1950
    },
    {
      "epoch": 3.4844444444444447,
      "grad_norm": 14.306203842163086,
      "learning_rate": 3.0391459074733097e-05,
      "loss": 9.2321,
      "step": 1960
    },
    {
      "epoch": 3.502222222222222,
      "grad_norm": 19.873376846313477,
      "learning_rate": 3.00355871886121e-05,
      "loss": 9.2275,
      "step": 1970
    },
    {
      "epoch": 3.52,
      "grad_norm": 16.645076751708984,
      "learning_rate": 2.9679715302491106e-05,
      "loss": 9.0731,
      "step": 1980
    },
    {
      "epoch": 3.537777777777778,
      "grad_norm": 15.266921043395996,
      "learning_rate": 2.9323843416370107e-05,
      "loss": 9.0299,
      "step": 1990
    },
    {
      "epoch": 3.5555555555555554,
      "grad_norm": 16.071773529052734,
      "learning_rate": 2.8967971530249112e-05,
      "loss": 9.1527,
      "step": 2000
    },
    {
      "epoch": 3.5733333333333333,
      "grad_norm": 15.145052909851074,
      "learning_rate": 2.8612099644128116e-05,
      "loss": 9.0945,
      "step": 2010
    },
    {
      "epoch": 3.591111111111111,
      "grad_norm": 12.357473373413086,
      "learning_rate": 2.825622775800712e-05,
      "loss": 9.0556,
      "step": 2020
    },
    {
      "epoch": 3.608888888888889,
      "grad_norm": 14.598387718200684,
      "learning_rate": 2.7900355871886125e-05,
      "loss": 9.1481,
      "step": 2030
    },
    {
      "epoch": 3.626666666666667,
      "grad_norm": 15.191657066345215,
      "learning_rate": 2.7544483985765123e-05,
      "loss": 9.1265,
      "step": 2040
    },
    {
      "epoch": 3.6444444444444444,
      "grad_norm": 13.132023811340332,
      "learning_rate": 2.7188612099644128e-05,
      "loss": 9.1225,
      "step": 2050
    },
    {
      "epoch": 3.6622222222222223,
      "grad_norm": 13.784072875976562,
      "learning_rate": 2.6832740213523132e-05,
      "loss": 8.9068,
      "step": 2060
    },
    {
      "epoch": 3.68,
      "grad_norm": 19.632753372192383,
      "learning_rate": 2.6476868327402137e-05,
      "loss": 8.961,
      "step": 2070
    },
    {
      "epoch": 3.6977777777777776,
      "grad_norm": 13.830997467041016,
      "learning_rate": 2.612099644128114e-05,
      "loss": 8.8723,
      "step": 2080
    },
    {
      "epoch": 3.7155555555555555,
      "grad_norm": 22.242691040039062,
      "learning_rate": 2.5765124555160143e-05,
      "loss": 9.0367,
      "step": 2090
    },
    {
      "epoch": 3.7333333333333334,
      "grad_norm": 20.521074295043945,
      "learning_rate": 2.5409252669039147e-05,
      "loss": 9.2225,
      "step": 2100
    },
    {
      "epoch": 3.7511111111111113,
      "grad_norm": 16.15113067626953,
      "learning_rate": 2.5053380782918152e-05,
      "loss": 8.957,
      "step": 2110
    },
    {
      "epoch": 3.7688888888888887,
      "grad_norm": 19.43100929260254,
      "learning_rate": 2.4697508896797153e-05,
      "loss": 8.8458,
      "step": 2120
    },
    {
      "epoch": 3.7866666666666666,
      "grad_norm": 14.216830253601074,
      "learning_rate": 2.4341637010676158e-05,
      "loss": 8.8881,
      "step": 2130
    },
    {
      "epoch": 3.8044444444444445,
      "grad_norm": 14.87242603302002,
      "learning_rate": 2.3985765124555162e-05,
      "loss": 9.3,
      "step": 2140
    },
    {
      "epoch": 3.822222222222222,
      "grad_norm": 19.05760383605957,
      "learning_rate": 2.3629893238434164e-05,
      "loss": 9.1566,
      "step": 2150
    },
    {
      "epoch": 3.84,
      "grad_norm": 14.460160255432129,
      "learning_rate": 2.3274021352313168e-05,
      "loss": 8.8513,
      "step": 2160
    },
    {
      "epoch": 3.8577777777777778,
      "grad_norm": 16.846033096313477,
      "learning_rate": 2.291814946619217e-05,
      "loss": 8.8809,
      "step": 2170
    },
    {
      "epoch": 3.8755555555555556,
      "grad_norm": 19.826486587524414,
      "learning_rate": 2.2562277580071174e-05,
      "loss": 8.9388,
      "step": 2180
    },
    {
      "epoch": 3.8933333333333335,
      "grad_norm": 14.808138847351074,
      "learning_rate": 2.220640569395018e-05,
      "loss": 9.0723,
      "step": 2190
    },
    {
      "epoch": 3.911111111111111,
      "grad_norm": 18.662738800048828,
      "learning_rate": 2.1850533807829183e-05,
      "loss": 8.828,
      "step": 2200
    },
    {
      "epoch": 3.928888888888889,
      "grad_norm": 21.152130126953125,
      "learning_rate": 2.1494661921708188e-05,
      "loss": 9.0065,
      "step": 2210
    },
    {
      "epoch": 3.9466666666666668,
      "grad_norm": 13.66653060913086,
      "learning_rate": 2.113879003558719e-05,
      "loss": 8.959,
      "step": 2220
    },
    {
      "epoch": 3.964444444444444,
      "grad_norm": 19.534204483032227,
      "learning_rate": 2.0782918149466194e-05,
      "loss": 8.8521,
      "step": 2230
    },
    {
      "epoch": 3.982222222222222,
      "grad_norm": 26.346067428588867,
      "learning_rate": 2.0427046263345195e-05,
      "loss": 9.0091,
      "step": 2240
    },
    {
      "epoch": 4.0,
      "grad_norm": 13.97669506072998,
      "learning_rate": 2.00711743772242e-05,
      "loss": 8.9806,
      "step": 2250
    },
    {
      "epoch": 4.0,
      "eval_bp": 1.0,
      "eval_counts": [
        1014,
        94,
        14,
        4
      ],
      "eval_loss": 8.711722373962402,
      "eval_precisions": [
        0.46397554748199465,
        0.04311016941379342,
        0.006435390972985148,
        0.001842910323983635
      ],
      "eval_ref_len": 14293,
      "eval_runtime": 1673.6197,
      "eval_samples_per_second": 0.299,
      "eval_score": 0.02206929655899044,
      "eval_steps_per_second": 0.149,
      "eval_sys_len": 218546,
      "eval_totals": [
        218546,
        218046,
        217547,
        217048
      ],
      "step": 2250
    },
    {
      "epoch": 4.017777777777778,
      "grad_norm": 16.44791603088379,
      "learning_rate": 1.9715302491103204e-05,
      "loss": 8.965,
      "step": 2260
    },
    {
      "epoch": 4.035555555555556,
      "grad_norm": 18.73784637451172,
      "learning_rate": 1.9359430604982205e-05,
      "loss": 9.0271,
      "step": 2270
    },
    {
      "epoch": 4.053333333333334,
      "grad_norm": 16.071945190429688,
      "learning_rate": 1.900355871886121e-05,
      "loss": 8.8565,
      "step": 2280
    },
    {
      "epoch": 4.071111111111111,
      "grad_norm": 26.367755889892578,
      "learning_rate": 1.8647686832740214e-05,
      "loss": 8.7288,
      "step": 2290
    },
    {
      "epoch": 4.088888888888889,
      "grad_norm": 16.351404190063477,
      "learning_rate": 1.829181494661922e-05,
      "loss": 8.907,
      "step": 2300
    },
    {
      "epoch": 4.1066666666666665,
      "grad_norm": 22.65779685974121,
      "learning_rate": 1.7935943060498224e-05,
      "loss": 8.971,
      "step": 2310
    },
    {
      "epoch": 4.124444444444444,
      "grad_norm": 16.50248908996582,
      "learning_rate": 1.7580071174377225e-05,
      "loss": 8.8757,
      "step": 2320
    },
    {
      "epoch": 4.142222222222222,
      "grad_norm": 18.50773811340332,
      "learning_rate": 1.722419928825623e-05,
      "loss": 8.7283,
      "step": 2330
    },
    {
      "epoch": 4.16,
      "grad_norm": 16.712360382080078,
      "learning_rate": 1.686832740213523e-05,
      "loss": 8.7609,
      "step": 2340
    },
    {
      "epoch": 4.177777777777778,
      "grad_norm": 18.990942001342773,
      "learning_rate": 1.6512455516014235e-05,
      "loss": 8.7067,
      "step": 2350
    },
    {
      "epoch": 4.195555555555556,
      "grad_norm": 14.904505729675293,
      "learning_rate": 1.615658362989324e-05,
      "loss": 8.6999,
      "step": 2360
    },
    {
      "epoch": 4.213333333333333,
      "grad_norm": 17.975801467895508,
      "learning_rate": 1.580071174377224e-05,
      "loss": 8.7723,
      "step": 2370
    },
    {
      "epoch": 4.231111111111111,
      "grad_norm": 19.583877563476562,
      "learning_rate": 1.5444839857651245e-05,
      "loss": 8.6169,
      "step": 2380
    },
    {
      "epoch": 4.248888888888889,
      "grad_norm": 14.503534317016602,
      "learning_rate": 1.5088967971530248e-05,
      "loss": 8.6328,
      "step": 2390
    },
    {
      "epoch": 4.266666666666667,
      "grad_norm": 14.16258716583252,
      "learning_rate": 1.4733096085409253e-05,
      "loss": 8.735,
      "step": 2400
    },
    {
      "epoch": 4.2844444444444445,
      "grad_norm": 12.923346519470215,
      "learning_rate": 1.4377224199288256e-05,
      "loss": 8.5419,
      "step": 2410
    },
    {
      "epoch": 4.302222222222222,
      "grad_norm": 19.641178131103516,
      "learning_rate": 1.402135231316726e-05,
      "loss": 8.7415,
      "step": 2420
    },
    {
      "epoch": 4.32,
      "grad_norm": 14.21917724609375,
      "learning_rate": 1.3665480427046265e-05,
      "loss": 8.8363,
      "step": 2430
    },
    {
      "epoch": 4.337777777777778,
      "grad_norm": 13.406925201416016,
      "learning_rate": 1.3309608540925266e-05,
      "loss": 8.8092,
      "step": 2440
    },
    {
      "epoch": 4.355555555555555,
      "grad_norm": 12.588117599487305,
      "learning_rate": 1.2953736654804271e-05,
      "loss": 8.7534,
      "step": 2450
    },
    {
      "epoch": 4.373333333333333,
      "grad_norm": 12.249086380004883,
      "learning_rate": 1.2597864768683274e-05,
      "loss": 8.5288,
      "step": 2460
    },
    {
      "epoch": 4.391111111111111,
      "grad_norm": 17.214221954345703,
      "learning_rate": 1.2241992882562278e-05,
      "loss": 8.822,
      "step": 2470
    },
    {
      "epoch": 4.408888888888889,
      "grad_norm": 12.768430709838867,
      "learning_rate": 1.1886120996441281e-05,
      "loss": 8.7024,
      "step": 2480
    },
    {
      "epoch": 4.426666666666667,
      "grad_norm": 20.68282699584961,
      "learning_rate": 1.1530249110320284e-05,
      "loss": 8.8182,
      "step": 2490
    },
    {
      "epoch": 4.444444444444445,
      "grad_norm": 18.801118850708008,
      "learning_rate": 1.1174377224199289e-05,
      "loss": 8.6099,
      "step": 2500
    },
    {
      "epoch": 4.4622222222222225,
      "grad_norm": 12.751763343811035,
      "learning_rate": 1.0818505338078293e-05,
      "loss": 8.6341,
      "step": 2510
    },
    {
      "epoch": 4.48,
      "grad_norm": 16.144697189331055,
      "learning_rate": 1.0462633451957296e-05,
      "loss": 8.7926,
      "step": 2520
    },
    {
      "epoch": 4.497777777777777,
      "grad_norm": 23.903114318847656,
      "learning_rate": 1.0106761565836299e-05,
      "loss": 8.7758,
      "step": 2530
    },
    {
      "epoch": 4.515555555555555,
      "grad_norm": 12.981973648071289,
      "learning_rate": 9.750889679715302e-06,
      "loss": 8.5511,
      "step": 2540
    },
    {
      "epoch": 4.533333333333333,
      "grad_norm": 13.349087715148926,
      "learning_rate": 9.395017793594307e-06,
      "loss": 8.6307,
      "step": 2550
    },
    {
      "epoch": 4.551111111111111,
      "grad_norm": 13.256478309631348,
      "learning_rate": 9.039145907473311e-06,
      "loss": 8.9038,
      "step": 2560
    },
    {
      "epoch": 4.568888888888889,
      "grad_norm": 12.51484203338623,
      "learning_rate": 8.683274021352314e-06,
      "loss": 8.48,
      "step": 2570
    },
    {
      "epoch": 4.586666666666667,
      "grad_norm": 15.43681526184082,
      "learning_rate": 8.327402135231317e-06,
      "loss": 8.6891,
      "step": 2580
    },
    {
      "epoch": 4.604444444444445,
      "grad_norm": 12.725565910339355,
      "learning_rate": 7.97153024911032e-06,
      "loss": 8.7892,
      "step": 2590
    },
    {
      "epoch": 4.622222222222222,
      "grad_norm": 16.222448348999023,
      "learning_rate": 7.615658362989324e-06,
      "loss": 8.5782,
      "step": 2600
    },
    {
      "epoch": 4.64,
      "grad_norm": 12.360997200012207,
      "learning_rate": 7.259786476868327e-06,
      "loss": 8.5754,
      "step": 2610
    },
    {
      "epoch": 4.657777777777778,
      "grad_norm": 12.384862899780273,
      "learning_rate": 6.903914590747332e-06,
      "loss": 8.659,
      "step": 2620
    },
    {
      "epoch": 4.6755555555555555,
      "grad_norm": 14.744399070739746,
      "learning_rate": 6.548042704626335e-06,
      "loss": 8.5583,
      "step": 2630
    },
    {
      "epoch": 4.693333333333333,
      "grad_norm": 19.340974807739258,
      "learning_rate": 6.192170818505339e-06,
      "loss": 8.7163,
      "step": 2640
    },
    {
      "epoch": 4.711111111111111,
      "grad_norm": 15.288058280944824,
      "learning_rate": 5.8362989323843415e-06,
      "loss": 8.6269,
      "step": 2650
    },
    {
      "epoch": 4.728888888888889,
      "grad_norm": 12.953660011291504,
      "learning_rate": 5.480427046263346e-06,
      "loss": 8.6496,
      "step": 2660
    },
    {
      "epoch": 4.746666666666667,
      "grad_norm": 13.44503116607666,
      "learning_rate": 5.124555160142349e-06,
      "loss": 8.5066,
      "step": 2670
    },
    {
      "epoch": 4.764444444444445,
      "grad_norm": 14.402015686035156,
      "learning_rate": 4.768683274021353e-06,
      "loss": 8.6807,
      "step": 2680
    },
    {
      "epoch": 4.782222222222222,
      "grad_norm": 29.008764266967773,
      "learning_rate": 4.4128113879003565e-06,
      "loss": 8.6127,
      "step": 2690
    },
    {
      "epoch": 4.8,
      "grad_norm": 14.083955764770508,
      "learning_rate": 4.056939501779359e-06,
      "loss": 8.6812,
      "step": 2700
    },
    {
      "epoch": 4.817777777777778,
      "grad_norm": 15.139504432678223,
      "learning_rate": 3.701067615658363e-06,
      "loss": 8.6781,
      "step": 2710
    },
    {
      "epoch": 4.835555555555556,
      "grad_norm": 12.735730171203613,
      "learning_rate": 3.345195729537367e-06,
      "loss": 8.459,
      "step": 2720
    },
    {
      "epoch": 4.8533333333333335,
      "grad_norm": 15.249246597290039,
      "learning_rate": 2.98932384341637e-06,
      "loss": 8.5508,
      "step": 2730
    },
    {
      "epoch": 4.871111111111111,
      "grad_norm": 14.701935768127441,
      "learning_rate": 2.633451957295374e-06,
      "loss": 8.677,
      "step": 2740
    },
    {
      "epoch": 4.888888888888889,
      "grad_norm": 16.677453994750977,
      "learning_rate": 2.2775800711743777e-06,
      "loss": 8.6334,
      "step": 2750
    },
    {
      "epoch": 4.906666666666666,
      "grad_norm": 12.265366554260254,
      "learning_rate": 1.921708185053381e-06,
      "loss": 8.5946,
      "step": 2760
    },
    {
      "epoch": 4.924444444444444,
      "grad_norm": 14.117170333862305,
      "learning_rate": 1.5658362989323845e-06,
      "loss": 8.67,
      "step": 2770
    },
    {
      "epoch": 4.942222222222222,
      "grad_norm": 14.037461280822754,
      "learning_rate": 1.2099644128113878e-06,
      "loss": 8.6184,
      "step": 2780
    },
    {
      "epoch": 4.96,
      "grad_norm": 11.137404441833496,
      "learning_rate": 8.540925266903916e-07,
      "loss": 8.593,
      "step": 2790
    },
    {
      "epoch": 4.977777777777778,
      "grad_norm": 16.563865661621094,
      "learning_rate": 4.98220640569395e-07,
      "loss": 8.4744,
      "step": 2800
    },
    {
      "epoch": 4.995555555555556,
      "grad_norm": 14.289170265197754,
      "learning_rate": 1.423487544483986e-07,
      "loss": 8.8045,
      "step": 2810
    }
  ],
  "logging_steps": 10,
  "max_steps": 2810,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1099738002346368.0,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
