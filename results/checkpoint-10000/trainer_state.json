{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.9857072449482503,
  "eval_steps": 500,
  "global_step": 10000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0009857072449482504,
      "grad_norm": 4.2071943283081055,
      "learning_rate": 1.9996451453918187e-05,
      "loss": 7.7538,
      "step": 10
    },
    {
      "epoch": 0.001971414489896501,
      "grad_norm": 4.334127426147461,
      "learning_rate": 1.9992508624938395e-05,
      "loss": 7.5174,
      "step": 20
    },
    {
      "epoch": 0.002957121734844751,
      "grad_norm": 3.3350348472595215,
      "learning_rate": 1.99885657959586e-05,
      "loss": 7.305,
      "step": 30
    },
    {
      "epoch": 0.003942828979793002,
      "grad_norm": 2.80643367767334,
      "learning_rate": 1.9984622966978808e-05,
      "loss": 7.031,
      "step": 40
    },
    {
      "epoch": 0.0049285362247412515,
      "grad_norm": 2.5470080375671387,
      "learning_rate": 1.9980680137999016e-05,
      "loss": 6.9477,
      "step": 50
    },
    {
      "epoch": 0.005914243469689502,
      "grad_norm": 3.229693651199341,
      "learning_rate": 1.9976737309019224e-05,
      "loss": 6.7985,
      "step": 60
    },
    {
      "epoch": 0.006899950714637753,
      "grad_norm": 2.788766622543335,
      "learning_rate": 1.997279448003943e-05,
      "loss": 6.8024,
      "step": 70
    },
    {
      "epoch": 0.007885657959586003,
      "grad_norm": 2.718690872192383,
      "learning_rate": 1.9968851651059637e-05,
      "loss": 6.5783,
      "step": 80
    },
    {
      "epoch": 0.008871365204534253,
      "grad_norm": 2.9308743476867676,
      "learning_rate": 1.9964908822079846e-05,
      "loss": 6.6518,
      "step": 90
    },
    {
      "epoch": 0.009857072449482503,
      "grad_norm": 2.6104564666748047,
      "learning_rate": 1.996096599310005e-05,
      "loss": 6.5534,
      "step": 100
    },
    {
      "epoch": 0.010842779694430755,
      "grad_norm": 2.740851879119873,
      "learning_rate": 1.995702316412026e-05,
      "loss": 6.4156,
      "step": 110
    },
    {
      "epoch": 0.011828486939379004,
      "grad_norm": 3.4412572383880615,
      "learning_rate": 1.9953080335140463e-05,
      "loss": 6.4685,
      "step": 120
    },
    {
      "epoch": 0.012814194184327254,
      "grad_norm": 3.7520487308502197,
      "learning_rate": 1.994913750616067e-05,
      "loss": 6.5005,
      "step": 130
    },
    {
      "epoch": 0.013799901429275506,
      "grad_norm": 4.536940097808838,
      "learning_rate": 1.994519467718088e-05,
      "loss": 6.4171,
      "step": 140
    },
    {
      "epoch": 0.014785608674223755,
      "grad_norm": 3.2635183334350586,
      "learning_rate": 1.9941251848201088e-05,
      "loss": 6.2916,
      "step": 150
    },
    {
      "epoch": 0.015771315919172007,
      "grad_norm": 3.5012054443359375,
      "learning_rate": 1.9937309019221293e-05,
      "loss": 6.3336,
      "step": 160
    },
    {
      "epoch": 0.016757023164120255,
      "grad_norm": 2.8411576747894287,
      "learning_rate": 1.99333661902415e-05,
      "loss": 6.1915,
      "step": 170
    },
    {
      "epoch": 0.017742730409068506,
      "grad_norm": 2.606210470199585,
      "learning_rate": 1.9929423361261706e-05,
      "loss": 6.2121,
      "step": 180
    },
    {
      "epoch": 0.018728437654016758,
      "grad_norm": 2.8503921031951904,
      "learning_rate": 1.9925480532281914e-05,
      "loss": 6.1283,
      "step": 190
    },
    {
      "epoch": 0.019714144898965006,
      "grad_norm": 3.774988889694214,
      "learning_rate": 1.9921537703302122e-05,
      "loss": 6.2482,
      "step": 200
    },
    {
      "epoch": 0.020699852143913258,
      "grad_norm": 2.848660469055176,
      "learning_rate": 1.9917594874322327e-05,
      "loss": 6.2306,
      "step": 210
    },
    {
      "epoch": 0.02168555938886151,
      "grad_norm": 2.8185348510742188,
      "learning_rate": 1.9913652045342535e-05,
      "loss": 6.211,
      "step": 220
    },
    {
      "epoch": 0.022671266633809757,
      "grad_norm": 3.2242085933685303,
      "learning_rate": 1.9909709216362743e-05,
      "loss": 6.0948,
      "step": 230
    },
    {
      "epoch": 0.02365697387875801,
      "grad_norm": 3.1622211933135986,
      "learning_rate": 1.990576638738295e-05,
      "loss": 6.2045,
      "step": 240
    },
    {
      "epoch": 0.02464268112370626,
      "grad_norm": 3.230454921722412,
      "learning_rate": 1.9901823558403156e-05,
      "loss": 6.1957,
      "step": 250
    },
    {
      "epoch": 0.025628388368654508,
      "grad_norm": 3.5957045555114746,
      "learning_rate": 1.989788072942336e-05,
      "loss": 6.1555,
      "step": 260
    },
    {
      "epoch": 0.02661409561360276,
      "grad_norm": 3.8208224773406982,
      "learning_rate": 1.989393790044357e-05,
      "loss": 5.9389,
      "step": 270
    },
    {
      "epoch": 0.02759980285855101,
      "grad_norm": 2.9637045860290527,
      "learning_rate": 1.9889995071463777e-05,
      "loss": 6.0498,
      "step": 280
    },
    {
      "epoch": 0.02858551010349926,
      "grad_norm": 3.7913410663604736,
      "learning_rate": 1.9886052242483986e-05,
      "loss": 6.2051,
      "step": 290
    },
    {
      "epoch": 0.02957121734844751,
      "grad_norm": 3.340480327606201,
      "learning_rate": 1.988210941350419e-05,
      "loss": 5.9259,
      "step": 300
    },
    {
      "epoch": 0.030556924593395762,
      "grad_norm": 2.9217910766601562,
      "learning_rate": 1.98781665845244e-05,
      "loss": 5.9652,
      "step": 310
    },
    {
      "epoch": 0.031542631838344014,
      "grad_norm": 3.213602304458618,
      "learning_rate": 1.9874223755544603e-05,
      "loss": 6.0016,
      "step": 320
    },
    {
      "epoch": 0.032528339083292265,
      "grad_norm": 3.0202090740203857,
      "learning_rate": 1.987028092656481e-05,
      "loss": 6.0443,
      "step": 330
    },
    {
      "epoch": 0.03351404632824051,
      "grad_norm": 3.499906301498413,
      "learning_rate": 1.986633809758502e-05,
      "loss": 5.9323,
      "step": 340
    },
    {
      "epoch": 0.03449975357318876,
      "grad_norm": 2.947138547897339,
      "learning_rate": 1.9862395268605224e-05,
      "loss": 6.0686,
      "step": 350
    },
    {
      "epoch": 0.03548546081813701,
      "grad_norm": 3.9578211307525635,
      "learning_rate": 1.9858452439625433e-05,
      "loss": 6.0162,
      "step": 360
    },
    {
      "epoch": 0.036471168063085264,
      "grad_norm": 3.722649574279785,
      "learning_rate": 1.985450961064564e-05,
      "loss": 5.9604,
      "step": 370
    },
    {
      "epoch": 0.037456875308033516,
      "grad_norm": 3.215576410293579,
      "learning_rate": 1.985056678166585e-05,
      "loss": 5.9985,
      "step": 380
    },
    {
      "epoch": 0.03844258255298177,
      "grad_norm": 3.5097310543060303,
      "learning_rate": 1.9846623952686054e-05,
      "loss": 5.9372,
      "step": 390
    },
    {
      "epoch": 0.03942828979793001,
      "grad_norm": 3.482250213623047,
      "learning_rate": 1.984268112370626e-05,
      "loss": 5.9202,
      "step": 400
    },
    {
      "epoch": 0.040413997042878264,
      "grad_norm": 3.085341453552246,
      "learning_rate": 1.9838738294726467e-05,
      "loss": 5.9156,
      "step": 410
    },
    {
      "epoch": 0.041399704287826515,
      "grad_norm": 4.528897762298584,
      "learning_rate": 1.9834795465746675e-05,
      "loss": 5.9913,
      "step": 420
    },
    {
      "epoch": 0.04238541153277477,
      "grad_norm": 3.126417398452759,
      "learning_rate": 1.9830852636766883e-05,
      "loss": 5.8804,
      "step": 430
    },
    {
      "epoch": 0.04337111877772302,
      "grad_norm": 3.030817747116089,
      "learning_rate": 1.9826909807787088e-05,
      "loss": 5.9676,
      "step": 440
    },
    {
      "epoch": 0.04435682602267127,
      "grad_norm": 3.6961495876312256,
      "learning_rate": 1.9822966978807296e-05,
      "loss": 5.7523,
      "step": 450
    },
    {
      "epoch": 0.045342533267619514,
      "grad_norm": 3.040203332901001,
      "learning_rate": 1.9819024149827504e-05,
      "loss": 5.8301,
      "step": 460
    },
    {
      "epoch": 0.046328240512567766,
      "grad_norm": 2.892976999282837,
      "learning_rate": 1.981508132084771e-05,
      "loss": 6.0086,
      "step": 470
    },
    {
      "epoch": 0.04731394775751602,
      "grad_norm": 3.3245439529418945,
      "learning_rate": 1.9811138491867917e-05,
      "loss": 5.8642,
      "step": 480
    },
    {
      "epoch": 0.04829965500246427,
      "grad_norm": 3.166661262512207,
      "learning_rate": 1.9807195662888122e-05,
      "loss": 5.9123,
      "step": 490
    },
    {
      "epoch": 0.04928536224741252,
      "grad_norm": 2.7375237941741943,
      "learning_rate": 1.980325283390833e-05,
      "loss": 5.8662,
      "step": 500
    },
    {
      "epoch": 0.05027106949236077,
      "grad_norm": 3.3979697227478027,
      "learning_rate": 1.979931000492854e-05,
      "loss": 5.8285,
      "step": 510
    },
    {
      "epoch": 0.051256776737309016,
      "grad_norm": 3.518695592880249,
      "learning_rate": 1.9795367175948747e-05,
      "loss": 5.8729,
      "step": 520
    },
    {
      "epoch": 0.05224248398225727,
      "grad_norm": 3.1101651191711426,
      "learning_rate": 1.979142434696895e-05,
      "loss": 5.8489,
      "step": 530
    },
    {
      "epoch": 0.05322819122720552,
      "grad_norm": 3.7135517597198486,
      "learning_rate": 1.9787481517989156e-05,
      "loss": 5.7832,
      "step": 540
    },
    {
      "epoch": 0.05421389847215377,
      "grad_norm": 3.2167422771453857,
      "learning_rate": 1.9783538689009364e-05,
      "loss": 5.8428,
      "step": 550
    },
    {
      "epoch": 0.05519960571710202,
      "grad_norm": 3.391479015350342,
      "learning_rate": 1.9779595860029573e-05,
      "loss": 5.713,
      "step": 560
    },
    {
      "epoch": 0.056185312962050274,
      "grad_norm": 4.233640193939209,
      "learning_rate": 1.977565303104978e-05,
      "loss": 5.8247,
      "step": 570
    },
    {
      "epoch": 0.05717102020699852,
      "grad_norm": 4.344638824462891,
      "learning_rate": 1.9771710202069986e-05,
      "loss": 5.6958,
      "step": 580
    },
    {
      "epoch": 0.05815672745194677,
      "grad_norm": 3.1059036254882812,
      "learning_rate": 1.9767767373090194e-05,
      "loss": 5.844,
      "step": 590
    },
    {
      "epoch": 0.05914243469689502,
      "grad_norm": 3.9478771686553955,
      "learning_rate": 1.9763824544110402e-05,
      "loss": 5.8433,
      "step": 600
    },
    {
      "epoch": 0.06012814194184327,
      "grad_norm": 3.504347801208496,
      "learning_rate": 1.975988171513061e-05,
      "loss": 5.7878,
      "step": 610
    },
    {
      "epoch": 0.061113849186791525,
      "grad_norm": 2.7703890800476074,
      "learning_rate": 1.9755938886150815e-05,
      "loss": 5.7208,
      "step": 620
    },
    {
      "epoch": 0.062099556431739776,
      "grad_norm": 3.8714120388031006,
      "learning_rate": 1.975199605717102e-05,
      "loss": 5.6948,
      "step": 630
    },
    {
      "epoch": 0.06308526367668803,
      "grad_norm": 3.104275703430176,
      "learning_rate": 1.9748053228191228e-05,
      "loss": 5.728,
      "step": 640
    },
    {
      "epoch": 0.06407097092163627,
      "grad_norm": 3.465498447418213,
      "learning_rate": 1.9744110399211436e-05,
      "loss": 5.7371,
      "step": 650
    },
    {
      "epoch": 0.06505667816658453,
      "grad_norm": 4.808828830718994,
      "learning_rate": 1.9740167570231644e-05,
      "loss": 5.6285,
      "step": 660
    },
    {
      "epoch": 0.06604238541153278,
      "grad_norm": 3.8450767993927,
      "learning_rate": 1.973622474125185e-05,
      "loss": 5.7005,
      "step": 670
    },
    {
      "epoch": 0.06702809265648102,
      "grad_norm": 4.185912609100342,
      "learning_rate": 1.9732281912272057e-05,
      "loss": 5.678,
      "step": 680
    },
    {
      "epoch": 0.06801379990142928,
      "grad_norm": 3.0943753719329834,
      "learning_rate": 1.9728339083292262e-05,
      "loss": 5.6988,
      "step": 690
    },
    {
      "epoch": 0.06899950714637752,
      "grad_norm": 3.897627830505371,
      "learning_rate": 1.972439625431247e-05,
      "loss": 5.6254,
      "step": 700
    },
    {
      "epoch": 0.06998521439132578,
      "grad_norm": 3.3699545860290527,
      "learning_rate": 1.972045342533268e-05,
      "loss": 5.64,
      "step": 710
    },
    {
      "epoch": 0.07097092163627403,
      "grad_norm": 4.397217273712158,
      "learning_rate": 1.9716510596352883e-05,
      "loss": 5.5855,
      "step": 720
    },
    {
      "epoch": 0.07195662888122227,
      "grad_norm": 3.8252696990966797,
      "learning_rate": 1.971256776737309e-05,
      "loss": 5.5785,
      "step": 730
    },
    {
      "epoch": 0.07294233612617053,
      "grad_norm": 4.3695478439331055,
      "learning_rate": 1.97086249383933e-05,
      "loss": 5.648,
      "step": 740
    },
    {
      "epoch": 0.07392804337111877,
      "grad_norm": 4.60300350189209,
      "learning_rate": 1.9704682109413508e-05,
      "loss": 5.6145,
      "step": 750
    },
    {
      "epoch": 0.07491375061606703,
      "grad_norm": 3.906719923019409,
      "learning_rate": 1.9700739280433713e-05,
      "loss": 5.6514,
      "step": 760
    },
    {
      "epoch": 0.07589945786101528,
      "grad_norm": 3.839170217514038,
      "learning_rate": 1.9696796451453917e-05,
      "loss": 5.6329,
      "step": 770
    },
    {
      "epoch": 0.07688516510596353,
      "grad_norm": 2.8632500171661377,
      "learning_rate": 1.9692853622474125e-05,
      "loss": 5.7083,
      "step": 780
    },
    {
      "epoch": 0.07787087235091178,
      "grad_norm": 3.330740213394165,
      "learning_rate": 1.9688910793494334e-05,
      "loss": 5.6072,
      "step": 790
    },
    {
      "epoch": 0.07885657959586002,
      "grad_norm": 3.619572639465332,
      "learning_rate": 1.9684967964514542e-05,
      "loss": 5.6574,
      "step": 800
    },
    {
      "epoch": 0.07984228684080828,
      "grad_norm": 3.270291805267334,
      "learning_rate": 1.9681025135534747e-05,
      "loss": 5.631,
      "step": 810
    },
    {
      "epoch": 0.08082799408575653,
      "grad_norm": 4.278512477874756,
      "learning_rate": 1.9677082306554955e-05,
      "loss": 5.7452,
      "step": 820
    },
    {
      "epoch": 0.08181370133070479,
      "grad_norm": 3.915419578552246,
      "learning_rate": 1.9673139477575163e-05,
      "loss": 5.5647,
      "step": 830
    },
    {
      "epoch": 0.08279940857565303,
      "grad_norm": 4.286830902099609,
      "learning_rate": 1.9669196648595368e-05,
      "loss": 5.6077,
      "step": 840
    },
    {
      "epoch": 0.08378511582060127,
      "grad_norm": 3.217972755432129,
      "learning_rate": 1.9665253819615576e-05,
      "loss": 5.5612,
      "step": 850
    },
    {
      "epoch": 0.08477082306554953,
      "grad_norm": 2.886080026626587,
      "learning_rate": 1.966131099063578e-05,
      "loss": 5.566,
      "step": 860
    },
    {
      "epoch": 0.08575653031049778,
      "grad_norm": 3.7586569786071777,
      "learning_rate": 1.965736816165599e-05,
      "loss": 5.6746,
      "step": 870
    },
    {
      "epoch": 0.08674223755544604,
      "grad_norm": 4.415459156036377,
      "learning_rate": 1.9653425332676197e-05,
      "loss": 5.6703,
      "step": 880
    },
    {
      "epoch": 0.08772794480039428,
      "grad_norm": 4.007958889007568,
      "learning_rate": 1.9649482503696405e-05,
      "loss": 5.5164,
      "step": 890
    },
    {
      "epoch": 0.08871365204534254,
      "grad_norm": 3.357131004333496,
      "learning_rate": 1.964553967471661e-05,
      "loss": 5.6293,
      "step": 900
    },
    {
      "epoch": 0.08969935929029078,
      "grad_norm": 3.2743868827819824,
      "learning_rate": 1.9641596845736818e-05,
      "loss": 5.5588,
      "step": 910
    },
    {
      "epoch": 0.09068506653523903,
      "grad_norm": 3.568176031112671,
      "learning_rate": 1.9637654016757023e-05,
      "loss": 5.4336,
      "step": 920
    },
    {
      "epoch": 0.09167077378018729,
      "grad_norm": 3.801987409591675,
      "learning_rate": 1.963371118777723e-05,
      "loss": 5.6123,
      "step": 930
    },
    {
      "epoch": 0.09265648102513553,
      "grad_norm": 5.375143527984619,
      "learning_rate": 1.962976835879744e-05,
      "loss": 5.53,
      "step": 940
    },
    {
      "epoch": 0.09364218827008379,
      "grad_norm": 4.201223373413086,
      "learning_rate": 1.9625825529817644e-05,
      "loss": 5.5569,
      "step": 950
    },
    {
      "epoch": 0.09462789551503203,
      "grad_norm": 4.604939937591553,
      "learning_rate": 1.9621882700837852e-05,
      "loss": 5.5733,
      "step": 960
    },
    {
      "epoch": 0.09561360275998028,
      "grad_norm": 3.339496374130249,
      "learning_rate": 1.961793987185806e-05,
      "loss": 5.4983,
      "step": 970
    },
    {
      "epoch": 0.09659931000492854,
      "grad_norm": 3.5235960483551025,
      "learning_rate": 1.961399704287827e-05,
      "loss": 5.5188,
      "step": 980
    },
    {
      "epoch": 0.09758501724987678,
      "grad_norm": 3.9835314750671387,
      "learning_rate": 1.9610054213898474e-05,
      "loss": 5.6366,
      "step": 990
    },
    {
      "epoch": 0.09857072449482504,
      "grad_norm": 3.5320258140563965,
      "learning_rate": 1.9606111384918682e-05,
      "loss": 5.4504,
      "step": 1000
    },
    {
      "epoch": 0.09955643173977329,
      "grad_norm": 4.337118625640869,
      "learning_rate": 1.9602168555938887e-05,
      "loss": 5.6317,
      "step": 1010
    },
    {
      "epoch": 0.10054213898472154,
      "grad_norm": 3.4289932250976562,
      "learning_rate": 1.9598225726959095e-05,
      "loss": 5.2505,
      "step": 1020
    },
    {
      "epoch": 0.10152784622966979,
      "grad_norm": 4.787432670593262,
      "learning_rate": 1.9594282897979303e-05,
      "loss": 5.447,
      "step": 1030
    },
    {
      "epoch": 0.10251355347461803,
      "grad_norm": 3.4014737606048584,
      "learning_rate": 1.9590340068999508e-05,
      "loss": 5.5523,
      "step": 1040
    },
    {
      "epoch": 0.10349926071956629,
      "grad_norm": 4.014778137207031,
      "learning_rate": 1.9586397240019716e-05,
      "loss": 5.5067,
      "step": 1050
    },
    {
      "epoch": 0.10448496796451454,
      "grad_norm": 3.667937755584717,
      "learning_rate": 1.958245441103992e-05,
      "loss": 5.3615,
      "step": 1060
    },
    {
      "epoch": 0.1054706752094628,
      "grad_norm": 3.512363910675049,
      "learning_rate": 1.957851158206013e-05,
      "loss": 5.5539,
      "step": 1070
    },
    {
      "epoch": 0.10645638245441104,
      "grad_norm": 4.549253463745117,
      "learning_rate": 1.9574568753080337e-05,
      "loss": 5.4401,
      "step": 1080
    },
    {
      "epoch": 0.1074420896993593,
      "grad_norm": 3.89021372795105,
      "learning_rate": 1.9570625924100545e-05,
      "loss": 5.5107,
      "step": 1090
    },
    {
      "epoch": 0.10842779694430754,
      "grad_norm": 4.048833847045898,
      "learning_rate": 1.956668309512075e-05,
      "loss": 5.3743,
      "step": 1100
    },
    {
      "epoch": 0.10941350418925579,
      "grad_norm": 4.183438301086426,
      "learning_rate": 1.9562740266140958e-05,
      "loss": 5.5024,
      "step": 1110
    },
    {
      "epoch": 0.11039921143420404,
      "grad_norm": 4.251394748687744,
      "learning_rate": 1.9558797437161166e-05,
      "loss": 5.4901,
      "step": 1120
    },
    {
      "epoch": 0.11138491867915229,
      "grad_norm": 4.118979454040527,
      "learning_rate": 1.955485460818137e-05,
      "loss": 5.3199,
      "step": 1130
    },
    {
      "epoch": 0.11237062592410055,
      "grad_norm": 3.4635446071624756,
      "learning_rate": 1.955091177920158e-05,
      "loss": 5.4443,
      "step": 1140
    },
    {
      "epoch": 0.11335633316904879,
      "grad_norm": 4.03232479095459,
      "learning_rate": 1.9546968950221784e-05,
      "loss": 5.4356,
      "step": 1150
    },
    {
      "epoch": 0.11434204041399704,
      "grad_norm": 3.4660117626190186,
      "learning_rate": 1.9543026121241992e-05,
      "loss": 5.4996,
      "step": 1160
    },
    {
      "epoch": 0.1153277476589453,
      "grad_norm": 4.318796634674072,
      "learning_rate": 1.95390832922622e-05,
      "loss": 5.4321,
      "step": 1170
    },
    {
      "epoch": 0.11631345490389354,
      "grad_norm": 4.0861310958862305,
      "learning_rate": 1.953514046328241e-05,
      "loss": 5.3719,
      "step": 1180
    },
    {
      "epoch": 0.1172991621488418,
      "grad_norm": 3.848459482192993,
      "learning_rate": 1.9531197634302614e-05,
      "loss": 5.4716,
      "step": 1190
    },
    {
      "epoch": 0.11828486939379004,
      "grad_norm": 4.611841678619385,
      "learning_rate": 1.952725480532282e-05,
      "loss": 5.4004,
      "step": 1200
    },
    {
      "epoch": 0.1192705766387383,
      "grad_norm": 3.7132630348205566,
      "learning_rate": 1.9523311976343026e-05,
      "loss": 5.3213,
      "step": 1210
    },
    {
      "epoch": 0.12025628388368655,
      "grad_norm": 4.717936038970947,
      "learning_rate": 1.9519369147363235e-05,
      "loss": 5.3316,
      "step": 1220
    },
    {
      "epoch": 0.12124199112863479,
      "grad_norm": 4.082310676574707,
      "learning_rate": 1.9515426318383443e-05,
      "loss": 5.3407,
      "step": 1230
    },
    {
      "epoch": 0.12222769837358305,
      "grad_norm": 4.201888084411621,
      "learning_rate": 1.9511483489403648e-05,
      "loss": 5.425,
      "step": 1240
    },
    {
      "epoch": 0.1232134056185313,
      "grad_norm": 4.213758945465088,
      "learning_rate": 1.9507540660423856e-05,
      "loss": 5.4872,
      "step": 1250
    },
    {
      "epoch": 0.12419911286347955,
      "grad_norm": 4.500340938568115,
      "learning_rate": 1.9503597831444064e-05,
      "loss": 5.44,
      "step": 1260
    },
    {
      "epoch": 0.1251848201084278,
      "grad_norm": 3.7857155799865723,
      "learning_rate": 1.9499655002464272e-05,
      "loss": 5.3817,
      "step": 1270
    },
    {
      "epoch": 0.12617052735337606,
      "grad_norm": 4.046384334564209,
      "learning_rate": 1.9495712173484477e-05,
      "loss": 5.4828,
      "step": 1280
    },
    {
      "epoch": 0.12715623459832429,
      "grad_norm": 3.4027998447418213,
      "learning_rate": 1.9491769344504682e-05,
      "loss": 5.3152,
      "step": 1290
    },
    {
      "epoch": 0.12814194184327254,
      "grad_norm": 3.9639430046081543,
      "learning_rate": 1.948782651552489e-05,
      "loss": 5.3129,
      "step": 1300
    },
    {
      "epoch": 0.1291276490882208,
      "grad_norm": 4.842648983001709,
      "learning_rate": 1.9483883686545098e-05,
      "loss": 5.3918,
      "step": 1310
    },
    {
      "epoch": 0.13011335633316906,
      "grad_norm": 4.193469047546387,
      "learning_rate": 1.9479940857565306e-05,
      "loss": 5.4593,
      "step": 1320
    },
    {
      "epoch": 0.1310990635781173,
      "grad_norm": 4.714646816253662,
      "learning_rate": 1.947599802858551e-05,
      "loss": 5.4305,
      "step": 1330
    },
    {
      "epoch": 0.13208477082306555,
      "grad_norm": 3.8559987545013428,
      "learning_rate": 1.947205519960572e-05,
      "loss": 5.4793,
      "step": 1340
    },
    {
      "epoch": 0.1330704780680138,
      "grad_norm": 4.573672294616699,
      "learning_rate": 1.9468112370625927e-05,
      "loss": 5.4279,
      "step": 1350
    },
    {
      "epoch": 0.13405618531296204,
      "grad_norm": 4.359956741333008,
      "learning_rate": 1.9464169541646132e-05,
      "loss": 5.4548,
      "step": 1360
    },
    {
      "epoch": 0.1350418925579103,
      "grad_norm": 3.7387542724609375,
      "learning_rate": 1.946022671266634e-05,
      "loss": 5.2834,
      "step": 1370
    },
    {
      "epoch": 0.13602759980285856,
      "grad_norm": 4.288345813751221,
      "learning_rate": 1.9456283883686545e-05,
      "loss": 5.4922,
      "step": 1380
    },
    {
      "epoch": 0.13701330704780681,
      "grad_norm": 3.5755856037139893,
      "learning_rate": 1.9452341054706753e-05,
      "loss": 5.3507,
      "step": 1390
    },
    {
      "epoch": 0.13799901429275505,
      "grad_norm": 4.882948875427246,
      "learning_rate": 1.944839822572696e-05,
      "loss": 5.5006,
      "step": 1400
    },
    {
      "epoch": 0.1389847215377033,
      "grad_norm": 3.9797048568725586,
      "learning_rate": 1.944445539674717e-05,
      "loss": 5.3027,
      "step": 1410
    },
    {
      "epoch": 0.13997042878265156,
      "grad_norm": 3.234346389770508,
      "learning_rate": 1.9440512567767375e-05,
      "loss": 5.3673,
      "step": 1420
    },
    {
      "epoch": 0.1409561360275998,
      "grad_norm": 4.159039497375488,
      "learning_rate": 1.943656973878758e-05,
      "loss": 5.4355,
      "step": 1430
    },
    {
      "epoch": 0.14194184327254805,
      "grad_norm": 3.8361997604370117,
      "learning_rate": 1.9432626909807788e-05,
      "loss": 5.2902,
      "step": 1440
    },
    {
      "epoch": 0.1429275505174963,
      "grad_norm": 4.157810688018799,
      "learning_rate": 1.9428684080827996e-05,
      "loss": 5.3898,
      "step": 1450
    },
    {
      "epoch": 0.14391325776244454,
      "grad_norm": 3.2378015518188477,
      "learning_rate": 1.9424741251848204e-05,
      "loss": 5.3197,
      "step": 1460
    },
    {
      "epoch": 0.1448989650073928,
      "grad_norm": 3.7217049598693848,
      "learning_rate": 1.942079842286841e-05,
      "loss": 5.334,
      "step": 1470
    },
    {
      "epoch": 0.14588467225234106,
      "grad_norm": 3.284860372543335,
      "learning_rate": 1.9416855593888617e-05,
      "loss": 5.2845,
      "step": 1480
    },
    {
      "epoch": 0.14687037949728932,
      "grad_norm": 3.8555893898010254,
      "learning_rate": 1.9412912764908825e-05,
      "loss": 5.3317,
      "step": 1490
    },
    {
      "epoch": 0.14785608674223755,
      "grad_norm": 3.894564628601074,
      "learning_rate": 1.940896993592903e-05,
      "loss": 5.2953,
      "step": 1500
    },
    {
      "epoch": 0.1488417939871858,
      "grad_norm": 4.204304218292236,
      "learning_rate": 1.9405027106949238e-05,
      "loss": 5.4157,
      "step": 1510
    },
    {
      "epoch": 0.14982750123213406,
      "grad_norm": 3.6546483039855957,
      "learning_rate": 1.9401084277969443e-05,
      "loss": 5.257,
      "step": 1520
    },
    {
      "epoch": 0.1508132084770823,
      "grad_norm": 4.699069023132324,
      "learning_rate": 1.939714144898965e-05,
      "loss": 5.1763,
      "step": 1530
    },
    {
      "epoch": 0.15179891572203055,
      "grad_norm": 4.429602146148682,
      "learning_rate": 1.939319862000986e-05,
      "loss": 5.2642,
      "step": 1540
    },
    {
      "epoch": 0.1527846229669788,
      "grad_norm": 5.035827159881592,
      "learning_rate": 1.9389255791030067e-05,
      "loss": 5.3953,
      "step": 1550
    },
    {
      "epoch": 0.15377033021192707,
      "grad_norm": 3.8013598918914795,
      "learning_rate": 1.9385312962050272e-05,
      "loss": 5.3239,
      "step": 1560
    },
    {
      "epoch": 0.1547560374568753,
      "grad_norm": 3.2844841480255127,
      "learning_rate": 1.938137013307048e-05,
      "loss": 5.1787,
      "step": 1570
    },
    {
      "epoch": 0.15574174470182356,
      "grad_norm": 3.5309243202209473,
      "learning_rate": 1.9377427304090685e-05,
      "loss": 5.4397,
      "step": 1580
    },
    {
      "epoch": 0.15672745194677182,
      "grad_norm": 3.6816012859344482,
      "learning_rate": 1.9373484475110893e-05,
      "loss": 5.2779,
      "step": 1590
    },
    {
      "epoch": 0.15771315919172005,
      "grad_norm": 4.006493091583252,
      "learning_rate": 1.93695416461311e-05,
      "loss": 5.3563,
      "step": 1600
    },
    {
      "epoch": 0.1586988664366683,
      "grad_norm": 3.777585029602051,
      "learning_rate": 1.9365598817151306e-05,
      "loss": 5.2037,
      "step": 1610
    },
    {
      "epoch": 0.15968457368161657,
      "grad_norm": 3.093514919281006,
      "learning_rate": 1.9361655988171514e-05,
      "loss": 5.4241,
      "step": 1620
    },
    {
      "epoch": 0.16067028092656482,
      "grad_norm": 5.006655693054199,
      "learning_rate": 1.9357713159191723e-05,
      "loss": 5.1926,
      "step": 1630
    },
    {
      "epoch": 0.16165598817151305,
      "grad_norm": 4.720107555389404,
      "learning_rate": 1.935377033021193e-05,
      "loss": 5.2762,
      "step": 1640
    },
    {
      "epoch": 0.1626416954164613,
      "grad_norm": 4.843969821929932,
      "learning_rate": 1.9349827501232136e-05,
      "loss": 5.2304,
      "step": 1650
    },
    {
      "epoch": 0.16362740266140957,
      "grad_norm": 4.079070091247559,
      "learning_rate": 1.934588467225234e-05,
      "loss": 5.2676,
      "step": 1660
    },
    {
      "epoch": 0.1646131099063578,
      "grad_norm": 3.853501319885254,
      "learning_rate": 1.934194184327255e-05,
      "loss": 5.3182,
      "step": 1670
    },
    {
      "epoch": 0.16559881715130606,
      "grad_norm": 3.466581106185913,
      "learning_rate": 1.9337999014292757e-05,
      "loss": 5.1591,
      "step": 1680
    },
    {
      "epoch": 0.16658452439625432,
      "grad_norm": 5.3212080001831055,
      "learning_rate": 1.9334056185312965e-05,
      "loss": 5.3421,
      "step": 1690
    },
    {
      "epoch": 0.16757023164120255,
      "grad_norm": 3.6445469856262207,
      "learning_rate": 1.933011335633317e-05,
      "loss": 5.2597,
      "step": 1700
    },
    {
      "epoch": 0.1685559388861508,
      "grad_norm": 4.564301013946533,
      "learning_rate": 1.9326170527353378e-05,
      "loss": 5.3044,
      "step": 1710
    },
    {
      "epoch": 0.16954164613109907,
      "grad_norm": 3.562758445739746,
      "learning_rate": 1.9322227698373586e-05,
      "loss": 5.2892,
      "step": 1720
    },
    {
      "epoch": 0.17052735337604732,
      "grad_norm": 4.431614875793457,
      "learning_rate": 1.931828486939379e-05,
      "loss": 5.2678,
      "step": 1730
    },
    {
      "epoch": 0.17151306062099556,
      "grad_norm": 4.417904376983643,
      "learning_rate": 1.9314342040414e-05,
      "loss": 5.0727,
      "step": 1740
    },
    {
      "epoch": 0.17249876786594381,
      "grad_norm": 3.6816954612731934,
      "learning_rate": 1.9310399211434204e-05,
      "loss": 5.3215,
      "step": 1750
    },
    {
      "epoch": 0.17348447511089207,
      "grad_norm": 4.5426859855651855,
      "learning_rate": 1.9306456382454412e-05,
      "loss": 5.0017,
      "step": 1760
    },
    {
      "epoch": 0.1744701823558403,
      "grad_norm": 4.415105819702148,
      "learning_rate": 1.930251355347462e-05,
      "loss": 5.2186,
      "step": 1770
    },
    {
      "epoch": 0.17545588960078856,
      "grad_norm": 4.776877403259277,
      "learning_rate": 1.929857072449483e-05,
      "loss": 5.2389,
      "step": 1780
    },
    {
      "epoch": 0.17644159684573682,
      "grad_norm": 4.013283729553223,
      "learning_rate": 1.9294627895515033e-05,
      "loss": 5.1799,
      "step": 1790
    },
    {
      "epoch": 0.17742730409068508,
      "grad_norm": 4.338168144226074,
      "learning_rate": 1.9290685066535238e-05,
      "loss": 5.2945,
      "step": 1800
    },
    {
      "epoch": 0.1784130113356333,
      "grad_norm": 4.183053970336914,
      "learning_rate": 1.9286742237555446e-05,
      "loss": 5.1565,
      "step": 1810
    },
    {
      "epoch": 0.17939871858058157,
      "grad_norm": 3.9468834400177,
      "learning_rate": 1.9282799408575654e-05,
      "loss": 5.227,
      "step": 1820
    },
    {
      "epoch": 0.18038442582552983,
      "grad_norm": 4.323674201965332,
      "learning_rate": 1.9278856579595863e-05,
      "loss": 5.259,
      "step": 1830
    },
    {
      "epoch": 0.18137013307047806,
      "grad_norm": 5.088396072387695,
      "learning_rate": 1.9274913750616067e-05,
      "loss": 5.2747,
      "step": 1840
    },
    {
      "epoch": 0.18235584031542632,
      "grad_norm": 3.9866995811462402,
      "learning_rate": 1.9270970921636276e-05,
      "loss": 5.2791,
      "step": 1850
    },
    {
      "epoch": 0.18334154756037457,
      "grad_norm": 3.8988869190216064,
      "learning_rate": 1.9267028092656484e-05,
      "loss": 5.1892,
      "step": 1860
    },
    {
      "epoch": 0.18432725480532283,
      "grad_norm": 4.4642558097839355,
      "learning_rate": 1.926308526367669e-05,
      "loss": 5.1722,
      "step": 1870
    },
    {
      "epoch": 0.18531296205027106,
      "grad_norm": 4.9126763343811035,
      "learning_rate": 1.9259142434696897e-05,
      "loss": 5.1846,
      "step": 1880
    },
    {
      "epoch": 0.18629866929521932,
      "grad_norm": 5.0724687576293945,
      "learning_rate": 1.92551996057171e-05,
      "loss": 5.1017,
      "step": 1890
    },
    {
      "epoch": 0.18728437654016758,
      "grad_norm": 3.6010208129882812,
      "learning_rate": 1.925125677673731e-05,
      "loss": 5.1399,
      "step": 1900
    },
    {
      "epoch": 0.1882700837851158,
      "grad_norm": 3.9119222164154053,
      "learning_rate": 1.9247313947757518e-05,
      "loss": 5.3003,
      "step": 1910
    },
    {
      "epoch": 0.18925579103006407,
      "grad_norm": 3.6532554626464844,
      "learning_rate": 1.9243371118777726e-05,
      "loss": 5.14,
      "step": 1920
    },
    {
      "epoch": 0.19024149827501233,
      "grad_norm": 4.6073737144470215,
      "learning_rate": 1.923942828979793e-05,
      "loss": 5.228,
      "step": 1930
    },
    {
      "epoch": 0.19122720551996056,
      "grad_norm": 4.024796009063721,
      "learning_rate": 1.923548546081814e-05,
      "loss": 5.1884,
      "step": 1940
    },
    {
      "epoch": 0.19221291276490882,
      "grad_norm": 3.837658166885376,
      "learning_rate": 1.9231542631838344e-05,
      "loss": 5.214,
      "step": 1950
    },
    {
      "epoch": 0.19319862000985707,
      "grad_norm": 3.7461884021759033,
      "learning_rate": 1.9227599802858552e-05,
      "loss": 5.0367,
      "step": 1960
    },
    {
      "epoch": 0.19418432725480533,
      "grad_norm": 3.9400296211242676,
      "learning_rate": 1.922365697387876e-05,
      "loss": 5.1296,
      "step": 1970
    },
    {
      "epoch": 0.19517003449975356,
      "grad_norm": 3.6653804779052734,
      "learning_rate": 1.9219714144898965e-05,
      "loss": 5.239,
      "step": 1980
    },
    {
      "epoch": 0.19615574174470182,
      "grad_norm": 4.846629619598389,
      "learning_rate": 1.9215771315919173e-05,
      "loss": 5.3208,
      "step": 1990
    },
    {
      "epoch": 0.19714144898965008,
      "grad_norm": 4.541869640350342,
      "learning_rate": 1.921182848693938e-05,
      "loss": 5.1096,
      "step": 2000
    },
    {
      "epoch": 0.1981271562345983,
      "grad_norm": 4.620683670043945,
      "learning_rate": 1.920788565795959e-05,
      "loss": 5.1553,
      "step": 2010
    },
    {
      "epoch": 0.19911286347954657,
      "grad_norm": 4.78650426864624,
      "learning_rate": 1.9203942828979794e-05,
      "loss": 5.0405,
      "step": 2020
    },
    {
      "epoch": 0.20009857072449483,
      "grad_norm": 4.456504821777344,
      "learning_rate": 1.9200000000000003e-05,
      "loss": 5.24,
      "step": 2030
    },
    {
      "epoch": 0.2010842779694431,
      "grad_norm": 3.8557019233703613,
      "learning_rate": 1.9196057171020207e-05,
      "loss": 5.063,
      "step": 2040
    },
    {
      "epoch": 0.20206998521439132,
      "grad_norm": 3.827954053878784,
      "learning_rate": 1.9192114342040415e-05,
      "loss": 5.1815,
      "step": 2050
    },
    {
      "epoch": 0.20305569245933958,
      "grad_norm": 3.6588900089263916,
      "learning_rate": 1.9188171513060624e-05,
      "loss": 5.1007,
      "step": 2060
    },
    {
      "epoch": 0.20404139970428783,
      "grad_norm": 3.769592046737671,
      "learning_rate": 1.918422868408083e-05,
      "loss": 5.2245,
      "step": 2070
    },
    {
      "epoch": 0.20502710694923607,
      "grad_norm": 3.2744855880737305,
      "learning_rate": 1.9180285855101037e-05,
      "loss": 5.0736,
      "step": 2080
    },
    {
      "epoch": 0.20601281419418432,
      "grad_norm": 3.7230684757232666,
      "learning_rate": 1.9176343026121245e-05,
      "loss": 5.1192,
      "step": 2090
    },
    {
      "epoch": 0.20699852143913258,
      "grad_norm": 3.8147590160369873,
      "learning_rate": 1.917240019714145e-05,
      "loss": 5.1677,
      "step": 2100
    },
    {
      "epoch": 0.20798422868408084,
      "grad_norm": 4.8959856033325195,
      "learning_rate": 1.9168457368161658e-05,
      "loss": 5.1134,
      "step": 2110
    },
    {
      "epoch": 0.20896993592902907,
      "grad_norm": 4.4910101890563965,
      "learning_rate": 1.9164514539181866e-05,
      "loss": 5.0823,
      "step": 2120
    },
    {
      "epoch": 0.20995564317397733,
      "grad_norm": 4.533167839050293,
      "learning_rate": 1.916057171020207e-05,
      "loss": 5.1671,
      "step": 2130
    },
    {
      "epoch": 0.2109413504189256,
      "grad_norm": 3.5213704109191895,
      "learning_rate": 1.915662888122228e-05,
      "loss": 5.1609,
      "step": 2140
    },
    {
      "epoch": 0.21192705766387382,
      "grad_norm": 4.69868278503418,
      "learning_rate": 1.9152686052242487e-05,
      "loss": 5.0295,
      "step": 2150
    },
    {
      "epoch": 0.21291276490882208,
      "grad_norm": 5.32515811920166,
      "learning_rate": 1.9148743223262692e-05,
      "loss": 5.0459,
      "step": 2160
    },
    {
      "epoch": 0.21389847215377034,
      "grad_norm": 4.757569313049316,
      "learning_rate": 1.91448003942829e-05,
      "loss": 5.2363,
      "step": 2170
    },
    {
      "epoch": 0.2148841793987186,
      "grad_norm": 3.679492712020874,
      "learning_rate": 1.9140857565303105e-05,
      "loss": 5.1233,
      "step": 2180
    },
    {
      "epoch": 0.21586988664366683,
      "grad_norm": 4.813538074493408,
      "learning_rate": 1.9136914736323313e-05,
      "loss": 5.2663,
      "step": 2190
    },
    {
      "epoch": 0.21685559388861508,
      "grad_norm": 4.3244404792785645,
      "learning_rate": 1.913297190734352e-05,
      "loss": 4.9797,
      "step": 2200
    },
    {
      "epoch": 0.21784130113356334,
      "grad_norm": 5.020352363586426,
      "learning_rate": 1.912902907836373e-05,
      "loss": 5.0834,
      "step": 2210
    },
    {
      "epoch": 0.21882700837851157,
      "grad_norm": 3.827620506286621,
      "learning_rate": 1.9125086249383934e-05,
      "loss": 5.1922,
      "step": 2220
    },
    {
      "epoch": 0.21981271562345983,
      "grad_norm": 4.528537750244141,
      "learning_rate": 1.9121143420404142e-05,
      "loss": 5.0973,
      "step": 2230
    },
    {
      "epoch": 0.2207984228684081,
      "grad_norm": 4.087676048278809,
      "learning_rate": 1.9117200591424347e-05,
      "loss": 5.0937,
      "step": 2240
    },
    {
      "epoch": 0.22178413011335632,
      "grad_norm": 4.304400444030762,
      "learning_rate": 1.9113257762444555e-05,
      "loss": 5.1746,
      "step": 2250
    },
    {
      "epoch": 0.22276983735830458,
      "grad_norm": 3.5971248149871826,
      "learning_rate": 1.9109314933464764e-05,
      "loss": 5.2076,
      "step": 2260
    },
    {
      "epoch": 0.22375554460325284,
      "grad_norm": 4.558318614959717,
      "learning_rate": 1.910537210448497e-05,
      "loss": 5.0658,
      "step": 2270
    },
    {
      "epoch": 0.2247412518482011,
      "grad_norm": 4.38222074508667,
      "learning_rate": 1.9101429275505177e-05,
      "loss": 5.075,
      "step": 2280
    },
    {
      "epoch": 0.22572695909314933,
      "grad_norm": 5.310416221618652,
      "learning_rate": 1.9097486446525385e-05,
      "loss": 5.1796,
      "step": 2290
    },
    {
      "epoch": 0.22671266633809758,
      "grad_norm": 3.9002835750579834,
      "learning_rate": 1.9093543617545593e-05,
      "loss": 5.0214,
      "step": 2300
    },
    {
      "epoch": 0.22769837358304584,
      "grad_norm": 5.0066914558410645,
      "learning_rate": 1.9089600788565798e-05,
      "loss": 5.1347,
      "step": 2310
    },
    {
      "epoch": 0.22868408082799407,
      "grad_norm": 4.393829822540283,
      "learning_rate": 1.9085657959586003e-05,
      "loss": 5.2044,
      "step": 2320
    },
    {
      "epoch": 0.22966978807294233,
      "grad_norm": 5.642798900604248,
      "learning_rate": 1.908171513060621e-05,
      "loss": 5.0412,
      "step": 2330
    },
    {
      "epoch": 0.2306554953178906,
      "grad_norm": 3.5801842212677,
      "learning_rate": 1.907777230162642e-05,
      "loss": 5.1356,
      "step": 2340
    },
    {
      "epoch": 0.23164120256283885,
      "grad_norm": 3.483518600463867,
      "learning_rate": 1.9073829472646627e-05,
      "loss": 5.1462,
      "step": 2350
    },
    {
      "epoch": 0.23262690980778708,
      "grad_norm": 3.574849843978882,
      "learning_rate": 1.9069886643666832e-05,
      "loss": 5.1537,
      "step": 2360
    },
    {
      "epoch": 0.23361261705273534,
      "grad_norm": 3.0858798027038574,
      "learning_rate": 1.906594381468704e-05,
      "loss": 5.0564,
      "step": 2370
    },
    {
      "epoch": 0.2345983242976836,
      "grad_norm": 4.7649970054626465,
      "learning_rate": 1.9062000985707248e-05,
      "loss": 4.9873,
      "step": 2380
    },
    {
      "epoch": 0.23558403154263183,
      "grad_norm": 3.625507354736328,
      "learning_rate": 1.9058058156727453e-05,
      "loss": 4.9766,
      "step": 2390
    },
    {
      "epoch": 0.23656973878758009,
      "grad_norm": 3.4025521278381348,
      "learning_rate": 1.905411532774766e-05,
      "loss": 5.1251,
      "step": 2400
    },
    {
      "epoch": 0.23755544603252834,
      "grad_norm": 3.892366647720337,
      "learning_rate": 1.9050172498767866e-05,
      "loss": 5.1294,
      "step": 2410
    },
    {
      "epoch": 0.2385411532774766,
      "grad_norm": 3.674830198287964,
      "learning_rate": 1.9046229669788074e-05,
      "loss": 5.0585,
      "step": 2420
    },
    {
      "epoch": 0.23952686052242483,
      "grad_norm": 3.937887191772461,
      "learning_rate": 1.9042286840808282e-05,
      "loss": 5.0728,
      "step": 2430
    },
    {
      "epoch": 0.2405125677673731,
      "grad_norm": 3.874204158782959,
      "learning_rate": 1.903834401182849e-05,
      "loss": 5.1854,
      "step": 2440
    },
    {
      "epoch": 0.24149827501232135,
      "grad_norm": 4.941960334777832,
      "learning_rate": 1.9034401182848695e-05,
      "loss": 4.9475,
      "step": 2450
    },
    {
      "epoch": 0.24248398225726958,
      "grad_norm": 5.94901180267334,
      "learning_rate": 1.90304583538689e-05,
      "loss": 5.1367,
      "step": 2460
    },
    {
      "epoch": 0.24346968950221784,
      "grad_norm": 3.9936859607696533,
      "learning_rate": 1.902651552488911e-05,
      "loss": 5.0481,
      "step": 2470
    },
    {
      "epoch": 0.2444553967471661,
      "grad_norm": 4.317012310028076,
      "learning_rate": 1.9022572695909316e-05,
      "loss": 4.9773,
      "step": 2480
    },
    {
      "epoch": 0.24544110399211433,
      "grad_norm": 3.4493346214294434,
      "learning_rate": 1.9018629866929525e-05,
      "loss": 5.1428,
      "step": 2490
    },
    {
      "epoch": 0.2464268112370626,
      "grad_norm": 6.347537994384766,
      "learning_rate": 1.901468703794973e-05,
      "loss": 5.1947,
      "step": 2500
    },
    {
      "epoch": 0.24741251848201085,
      "grad_norm": 4.352140426635742,
      "learning_rate": 1.9010744208969938e-05,
      "loss": 5.0472,
      "step": 2510
    },
    {
      "epoch": 0.2483982257269591,
      "grad_norm": 3.5525972843170166,
      "learning_rate": 1.9006801379990146e-05,
      "loss": 4.9529,
      "step": 2520
    },
    {
      "epoch": 0.24938393297190733,
      "grad_norm": 3.9407029151916504,
      "learning_rate": 1.9002858551010354e-05,
      "loss": 5.1297,
      "step": 2530
    },
    {
      "epoch": 0.2503696402168556,
      "grad_norm": 4.505748748779297,
      "learning_rate": 1.899891572203056e-05,
      "loss": 5.0914,
      "step": 2540
    },
    {
      "epoch": 0.2513553474618038,
      "grad_norm": 3.927056312561035,
      "learning_rate": 1.8994972893050764e-05,
      "loss": 5.1228,
      "step": 2550
    },
    {
      "epoch": 0.2523410547067521,
      "grad_norm": 3.8470568656921387,
      "learning_rate": 1.8991030064070972e-05,
      "loss": 5.165,
      "step": 2560
    },
    {
      "epoch": 0.25332676195170034,
      "grad_norm": 3.325352191925049,
      "learning_rate": 1.898708723509118e-05,
      "loss": 5.1278,
      "step": 2570
    },
    {
      "epoch": 0.25431246919664857,
      "grad_norm": 4.4628777503967285,
      "learning_rate": 1.8983144406111388e-05,
      "loss": 5.098,
      "step": 2580
    },
    {
      "epoch": 0.25529817644159686,
      "grad_norm": 3.6601834297180176,
      "learning_rate": 1.8979201577131593e-05,
      "loss": 4.9747,
      "step": 2590
    },
    {
      "epoch": 0.2562838836865451,
      "grad_norm": 4.138744354248047,
      "learning_rate": 1.89752587481518e-05,
      "loss": 5.0232,
      "step": 2600
    },
    {
      "epoch": 0.2572695909314933,
      "grad_norm": 3.7160520553588867,
      "learning_rate": 1.8971315919172006e-05,
      "loss": 5.0514,
      "step": 2610
    },
    {
      "epoch": 0.2582552981764416,
      "grad_norm": 3.601896047592163,
      "learning_rate": 1.8967373090192214e-05,
      "loss": 5.0137,
      "step": 2620
    },
    {
      "epoch": 0.25924100542138984,
      "grad_norm": 4.558172702789307,
      "learning_rate": 1.8963430261212422e-05,
      "loss": 5.0858,
      "step": 2630
    },
    {
      "epoch": 0.2602267126663381,
      "grad_norm": 3.639451503753662,
      "learning_rate": 1.8959487432232627e-05,
      "loss": 5.0789,
      "step": 2640
    },
    {
      "epoch": 0.26121241991128635,
      "grad_norm": 3.2281882762908936,
      "learning_rate": 1.8955544603252835e-05,
      "loss": 5.117,
      "step": 2650
    },
    {
      "epoch": 0.2621981271562346,
      "grad_norm": 4.096938610076904,
      "learning_rate": 1.8951601774273043e-05,
      "loss": 4.9655,
      "step": 2660
    },
    {
      "epoch": 0.26318383440118287,
      "grad_norm": 3.9095442295074463,
      "learning_rate": 1.894765894529325e-05,
      "loss": 4.9935,
      "step": 2670
    },
    {
      "epoch": 0.2641695416461311,
      "grad_norm": 3.2046663761138916,
      "learning_rate": 1.8943716116313456e-05,
      "loss": 4.9901,
      "step": 2680
    },
    {
      "epoch": 0.26515524889107933,
      "grad_norm": 3.423222303390503,
      "learning_rate": 1.893977328733366e-05,
      "loss": 4.9756,
      "step": 2690
    },
    {
      "epoch": 0.2661409561360276,
      "grad_norm": 3.871673583984375,
      "learning_rate": 1.893583045835387e-05,
      "loss": 4.8786,
      "step": 2700
    },
    {
      "epoch": 0.26712666338097585,
      "grad_norm": 4.246686935424805,
      "learning_rate": 1.8931887629374078e-05,
      "loss": 4.9421,
      "step": 2710
    },
    {
      "epoch": 0.2681123706259241,
      "grad_norm": 3.3295278549194336,
      "learning_rate": 1.8927944800394286e-05,
      "loss": 5.1115,
      "step": 2720
    },
    {
      "epoch": 0.26909807787087237,
      "grad_norm": 5.015183448791504,
      "learning_rate": 1.892400197141449e-05,
      "loss": 5.037,
      "step": 2730
    },
    {
      "epoch": 0.2700837851158206,
      "grad_norm": 4.946474075317383,
      "learning_rate": 1.89200591424347e-05,
      "loss": 5.0965,
      "step": 2740
    },
    {
      "epoch": 0.2710694923607688,
      "grad_norm": 4.775393486022949,
      "learning_rate": 1.8916116313454907e-05,
      "loss": 5.0923,
      "step": 2750
    },
    {
      "epoch": 0.2720551996057171,
      "grad_norm": 3.170860767364502,
      "learning_rate": 1.8912173484475112e-05,
      "loss": 4.9993,
      "step": 2760
    },
    {
      "epoch": 0.27304090685066534,
      "grad_norm": 3.672071933746338,
      "learning_rate": 1.890823065549532e-05,
      "loss": 4.9363,
      "step": 2770
    },
    {
      "epoch": 0.27402661409561363,
      "grad_norm": 3.824965476989746,
      "learning_rate": 1.8904287826515525e-05,
      "loss": 5.0491,
      "step": 2780
    },
    {
      "epoch": 0.27501232134056186,
      "grad_norm": 3.386601209640503,
      "learning_rate": 1.8900344997535733e-05,
      "loss": 4.8697,
      "step": 2790
    },
    {
      "epoch": 0.2759980285855101,
      "grad_norm": 6.170783519744873,
      "learning_rate": 1.889640216855594e-05,
      "loss": 4.9044,
      "step": 2800
    },
    {
      "epoch": 0.2769837358304584,
      "grad_norm": 3.469191312789917,
      "learning_rate": 1.889245933957615e-05,
      "loss": 4.9991,
      "step": 2810
    },
    {
      "epoch": 0.2779694430754066,
      "grad_norm": 4.68353271484375,
      "learning_rate": 1.8888516510596354e-05,
      "loss": 4.9634,
      "step": 2820
    },
    {
      "epoch": 0.27895515032035484,
      "grad_norm": 3.9555907249450684,
      "learning_rate": 1.888457368161656e-05,
      "loss": 4.9785,
      "step": 2830
    },
    {
      "epoch": 0.2799408575653031,
      "grad_norm": 3.150251865386963,
      "learning_rate": 1.8880630852636767e-05,
      "loss": 5.0674,
      "step": 2840
    },
    {
      "epoch": 0.28092656481025136,
      "grad_norm": 3.5115246772766113,
      "learning_rate": 1.8876688023656975e-05,
      "loss": 5.143,
      "step": 2850
    },
    {
      "epoch": 0.2819122720551996,
      "grad_norm": 4.192617893218994,
      "learning_rate": 1.8872745194677183e-05,
      "loss": 4.9331,
      "step": 2860
    },
    {
      "epoch": 0.2828979793001479,
      "grad_norm": 4.680755138397217,
      "learning_rate": 1.8868802365697388e-05,
      "loss": 5.0106,
      "step": 2870
    },
    {
      "epoch": 0.2838836865450961,
      "grad_norm": 3.883308172225952,
      "learning_rate": 1.8864859536717596e-05,
      "loss": 4.8414,
      "step": 2880
    },
    {
      "epoch": 0.28486939379004433,
      "grad_norm": 4.52553653717041,
      "learning_rate": 1.8860916707737805e-05,
      "loss": 5.0317,
      "step": 2890
    },
    {
      "epoch": 0.2858551010349926,
      "grad_norm": 3.1756253242492676,
      "learning_rate": 1.8856973878758013e-05,
      "loss": 4.9004,
      "step": 2900
    },
    {
      "epoch": 0.28684080827994085,
      "grad_norm": 3.7106852531433105,
      "learning_rate": 1.8853031049778217e-05,
      "loss": 4.8821,
      "step": 2910
    },
    {
      "epoch": 0.2878265155248891,
      "grad_norm": 3.9011871814727783,
      "learning_rate": 1.8849088220798422e-05,
      "loss": 4.9044,
      "step": 2920
    },
    {
      "epoch": 0.28881222276983737,
      "grad_norm": 4.088690757751465,
      "learning_rate": 1.884514539181863e-05,
      "loss": 4.9583,
      "step": 2930
    },
    {
      "epoch": 0.2897979300147856,
      "grad_norm": 4.876376152038574,
      "learning_rate": 1.884120256283884e-05,
      "loss": 5.0732,
      "step": 2940
    },
    {
      "epoch": 0.2907836372597339,
      "grad_norm": 3.275421619415283,
      "learning_rate": 1.8837259733859047e-05,
      "loss": 4.9613,
      "step": 2950
    },
    {
      "epoch": 0.2917693445046821,
      "grad_norm": 3.977086305618286,
      "learning_rate": 1.883331690487925e-05,
      "loss": 5.0721,
      "step": 2960
    },
    {
      "epoch": 0.29275505174963035,
      "grad_norm": 4.645820617675781,
      "learning_rate": 1.882937407589946e-05,
      "loss": 4.8922,
      "step": 2970
    },
    {
      "epoch": 0.29374075899457863,
      "grad_norm": 5.804163932800293,
      "learning_rate": 1.8825431246919665e-05,
      "loss": 4.9862,
      "step": 2980
    },
    {
      "epoch": 0.29472646623952686,
      "grad_norm": 4.113117694854736,
      "learning_rate": 1.8821488417939873e-05,
      "loss": 4.9586,
      "step": 2990
    },
    {
      "epoch": 0.2957121734844751,
      "grad_norm": 4.813814163208008,
      "learning_rate": 1.881754558896008e-05,
      "loss": 4.9269,
      "step": 3000
    },
    {
      "epoch": 0.2966978807294234,
      "grad_norm": 4.095858573913574,
      "learning_rate": 1.8813602759980286e-05,
      "loss": 4.9765,
      "step": 3010
    },
    {
      "epoch": 0.2976835879743716,
      "grad_norm": 4.290462017059326,
      "learning_rate": 1.8809659931000494e-05,
      "loss": 5.1085,
      "step": 3020
    },
    {
      "epoch": 0.29866929521931984,
      "grad_norm": 4.255925178527832,
      "learning_rate": 1.8805717102020702e-05,
      "loss": 4.8682,
      "step": 3030
    },
    {
      "epoch": 0.2996550024642681,
      "grad_norm": 5.260873794555664,
      "learning_rate": 1.880177427304091e-05,
      "loss": 4.9585,
      "step": 3040
    },
    {
      "epoch": 0.30064070970921636,
      "grad_norm": 3.788813591003418,
      "learning_rate": 1.8797831444061115e-05,
      "loss": 4.9469,
      "step": 3050
    },
    {
      "epoch": 0.3016264169541646,
      "grad_norm": 4.121882438659668,
      "learning_rate": 1.879388861508132e-05,
      "loss": 4.9476,
      "step": 3060
    },
    {
      "epoch": 0.3026121241991129,
      "grad_norm": 3.607362985610962,
      "learning_rate": 1.8789945786101528e-05,
      "loss": 5.0552,
      "step": 3070
    },
    {
      "epoch": 0.3035978314440611,
      "grad_norm": 5.442227840423584,
      "learning_rate": 1.8786002957121736e-05,
      "loss": 5.0112,
      "step": 3080
    },
    {
      "epoch": 0.30458353868900934,
      "grad_norm": 4.348607063293457,
      "learning_rate": 1.8782060128141944e-05,
      "loss": 5.0433,
      "step": 3090
    },
    {
      "epoch": 0.3055692459339576,
      "grad_norm": 4.849876880645752,
      "learning_rate": 1.877811729916215e-05,
      "loss": 4.9208,
      "step": 3100
    },
    {
      "epoch": 0.30655495317890585,
      "grad_norm": 3.866450309753418,
      "learning_rate": 1.8774174470182357e-05,
      "loss": 5.0202,
      "step": 3110
    },
    {
      "epoch": 0.30754066042385414,
      "grad_norm": 4.087352275848389,
      "learning_rate": 1.8770231641202566e-05,
      "loss": 5.0029,
      "step": 3120
    },
    {
      "epoch": 0.30852636766880237,
      "grad_norm": 4.707335948944092,
      "learning_rate": 1.876628881222277e-05,
      "loss": 5.1293,
      "step": 3130
    },
    {
      "epoch": 0.3095120749137506,
      "grad_norm": 4.575560569763184,
      "learning_rate": 1.876234598324298e-05,
      "loss": 4.7466,
      "step": 3140
    },
    {
      "epoch": 0.3104977821586989,
      "grad_norm": 3.4100849628448486,
      "learning_rate": 1.8758403154263183e-05,
      "loss": 4.9126,
      "step": 3150
    },
    {
      "epoch": 0.3114834894036471,
      "grad_norm": 4.407261848449707,
      "learning_rate": 1.875446032528339e-05,
      "loss": 4.8966,
      "step": 3160
    },
    {
      "epoch": 0.31246919664859535,
      "grad_norm": 4.440828323364258,
      "learning_rate": 1.87505174963036e-05,
      "loss": 4.9647,
      "step": 3170
    },
    {
      "epoch": 0.31345490389354363,
      "grad_norm": 3.9708499908447266,
      "learning_rate": 1.8746574667323808e-05,
      "loss": 5.0096,
      "step": 3180
    },
    {
      "epoch": 0.31444061113849187,
      "grad_norm": 3.541013479232788,
      "learning_rate": 1.8742631838344013e-05,
      "loss": 5.0255,
      "step": 3190
    },
    {
      "epoch": 0.3154263183834401,
      "grad_norm": 4.307628154754639,
      "learning_rate": 1.873868900936422e-05,
      "loss": 5.0286,
      "step": 3200
    },
    {
      "epoch": 0.3164120256283884,
      "grad_norm": 4.700166702270508,
      "learning_rate": 1.8734746180384426e-05,
      "loss": 5.0662,
      "step": 3210
    },
    {
      "epoch": 0.3173977328733366,
      "grad_norm": 3.945376396179199,
      "learning_rate": 1.8730803351404634e-05,
      "loss": 4.8868,
      "step": 3220
    },
    {
      "epoch": 0.31838344011828484,
      "grad_norm": 4.410877704620361,
      "learning_rate": 1.8726860522424842e-05,
      "loss": 5.0034,
      "step": 3230
    },
    {
      "epoch": 0.31936914736323313,
      "grad_norm": 3.8501498699188232,
      "learning_rate": 1.8722917693445047e-05,
      "loss": 4.9992,
      "step": 3240
    },
    {
      "epoch": 0.32035485460818136,
      "grad_norm": 3.4825825691223145,
      "learning_rate": 1.8718974864465255e-05,
      "loss": 5.0135,
      "step": 3250
    },
    {
      "epoch": 0.32134056185312965,
      "grad_norm": 3.899256944656372,
      "learning_rate": 1.8715032035485463e-05,
      "loss": 4.8993,
      "step": 3260
    },
    {
      "epoch": 0.3223262690980779,
      "grad_norm": 4.746453285217285,
      "learning_rate": 1.871108920650567e-05,
      "loss": 4.8746,
      "step": 3270
    },
    {
      "epoch": 0.3233119763430261,
      "grad_norm": 4.304402828216553,
      "learning_rate": 1.8707146377525876e-05,
      "loss": 4.9034,
      "step": 3280
    },
    {
      "epoch": 0.3242976835879744,
      "grad_norm": 4.273329257965088,
      "learning_rate": 1.8703203548546084e-05,
      "loss": 4.8395,
      "step": 3290
    },
    {
      "epoch": 0.3252833908329226,
      "grad_norm": 3.9408352375030518,
      "learning_rate": 1.869926071956629e-05,
      "loss": 4.8357,
      "step": 3300
    },
    {
      "epoch": 0.32626909807787086,
      "grad_norm": 4.2791829109191895,
      "learning_rate": 1.8695317890586497e-05,
      "loss": 4.8849,
      "step": 3310
    },
    {
      "epoch": 0.32725480532281914,
      "grad_norm": 4.16843843460083,
      "learning_rate": 1.8691375061606706e-05,
      "loss": 5.1035,
      "step": 3320
    },
    {
      "epoch": 0.3282405125677674,
      "grad_norm": 3.6288673877716064,
      "learning_rate": 1.868743223262691e-05,
      "loss": 4.9886,
      "step": 3330
    },
    {
      "epoch": 0.3292262198127156,
      "grad_norm": 5.048831462860107,
      "learning_rate": 1.868348940364712e-05,
      "loss": 4.7085,
      "step": 3340
    },
    {
      "epoch": 0.3302119270576639,
      "grad_norm": 3.6355042457580566,
      "learning_rate": 1.8679546574667323e-05,
      "loss": 4.8732,
      "step": 3350
    },
    {
      "epoch": 0.3311976343026121,
      "grad_norm": 5.184969902038574,
      "learning_rate": 1.867560374568753e-05,
      "loss": 4.941,
      "step": 3360
    },
    {
      "epoch": 0.33218334154756035,
      "grad_norm": 3.9576377868652344,
      "learning_rate": 1.867166091670774e-05,
      "loss": 4.7617,
      "step": 3370
    },
    {
      "epoch": 0.33316904879250864,
      "grad_norm": 5.041083335876465,
      "learning_rate": 1.8667718087727948e-05,
      "loss": 4.9237,
      "step": 3380
    },
    {
      "epoch": 0.33415475603745687,
      "grad_norm": 3.550276041030884,
      "learning_rate": 1.8663775258748153e-05,
      "loss": 5.0233,
      "step": 3390
    },
    {
      "epoch": 0.3351404632824051,
      "grad_norm": 3.9061224460601807,
      "learning_rate": 1.865983242976836e-05,
      "loss": 4.8234,
      "step": 3400
    },
    {
      "epoch": 0.3361261705273534,
      "grad_norm": 4.053264141082764,
      "learning_rate": 1.865588960078857e-05,
      "loss": 4.8934,
      "step": 3410
    },
    {
      "epoch": 0.3371118777723016,
      "grad_norm": 3.804766893386841,
      "learning_rate": 1.8651946771808774e-05,
      "loss": 4.9689,
      "step": 3420
    },
    {
      "epoch": 0.3380975850172499,
      "grad_norm": 3.579172134399414,
      "learning_rate": 1.8648003942828982e-05,
      "loss": 4.7954,
      "step": 3430
    },
    {
      "epoch": 0.33908329226219813,
      "grad_norm": 4.413037300109863,
      "learning_rate": 1.8644061113849187e-05,
      "loss": 4.9587,
      "step": 3440
    },
    {
      "epoch": 0.34006899950714636,
      "grad_norm": 4.352381706237793,
      "learning_rate": 1.8640118284869395e-05,
      "loss": 4.9009,
      "step": 3450
    },
    {
      "epoch": 0.34105470675209465,
      "grad_norm": 3.999781608581543,
      "learning_rate": 1.8636175455889603e-05,
      "loss": 4.8748,
      "step": 3460
    },
    {
      "epoch": 0.3420404139970429,
      "grad_norm": 3.5066092014312744,
      "learning_rate": 1.863223262690981e-05,
      "loss": 4.9467,
      "step": 3470
    },
    {
      "epoch": 0.3430261212419911,
      "grad_norm": 4.129651069641113,
      "learning_rate": 1.8628289797930016e-05,
      "loss": 4.8837,
      "step": 3480
    },
    {
      "epoch": 0.3440118284869394,
      "grad_norm": 4.33369255065918,
      "learning_rate": 1.8624346968950224e-05,
      "loss": 4.8766,
      "step": 3490
    },
    {
      "epoch": 0.34499753573188763,
      "grad_norm": 3.9497878551483154,
      "learning_rate": 1.862040413997043e-05,
      "loss": 4.9904,
      "step": 3500
    },
    {
      "epoch": 0.34598324297683586,
      "grad_norm": 4.56289005279541,
      "learning_rate": 1.8616461310990637e-05,
      "loss": 4.9394,
      "step": 3510
    },
    {
      "epoch": 0.34696895022178414,
      "grad_norm": 3.85766339302063,
      "learning_rate": 1.8612518482010845e-05,
      "loss": 4.8727,
      "step": 3520
    },
    {
      "epoch": 0.3479546574667324,
      "grad_norm": 4.608785629272461,
      "learning_rate": 1.860857565303105e-05,
      "loss": 5.0175,
      "step": 3530
    },
    {
      "epoch": 0.3489403647116806,
      "grad_norm": 3.7646028995513916,
      "learning_rate": 1.860463282405126e-05,
      "loss": 4.7565,
      "step": 3540
    },
    {
      "epoch": 0.3499260719566289,
      "grad_norm": 4.93874454498291,
      "learning_rate": 1.8600689995071467e-05,
      "loss": 4.9721,
      "step": 3550
    },
    {
      "epoch": 0.3509117792015771,
      "grad_norm": 3.5292699337005615,
      "learning_rate": 1.8596747166091675e-05,
      "loss": 4.8547,
      "step": 3560
    },
    {
      "epoch": 0.3518974864465254,
      "grad_norm": 3.4945294857025146,
      "learning_rate": 1.859280433711188e-05,
      "loss": 4.972,
      "step": 3570
    },
    {
      "epoch": 0.35288319369147364,
      "grad_norm": 3.9075229167938232,
      "learning_rate": 1.8588861508132084e-05,
      "loss": 4.8957,
      "step": 3580
    },
    {
      "epoch": 0.35386890093642187,
      "grad_norm": 3.6519923210144043,
      "learning_rate": 1.8584918679152293e-05,
      "loss": 4.8329,
      "step": 3590
    },
    {
      "epoch": 0.35485460818137016,
      "grad_norm": 4.062036037445068,
      "learning_rate": 1.85809758501725e-05,
      "loss": 4.8261,
      "step": 3600
    },
    {
      "epoch": 0.3558403154263184,
      "grad_norm": 3.9328970909118652,
      "learning_rate": 1.857703302119271e-05,
      "loss": 4.8432,
      "step": 3610
    },
    {
      "epoch": 0.3568260226712666,
      "grad_norm": 4.094681262969971,
      "learning_rate": 1.8573090192212914e-05,
      "loss": 5.02,
      "step": 3620
    },
    {
      "epoch": 0.3578117299162149,
      "grad_norm": 4.87980842590332,
      "learning_rate": 1.8569147363233122e-05,
      "loss": 4.9066,
      "step": 3630
    },
    {
      "epoch": 0.35879743716116314,
      "grad_norm": 4.357671737670898,
      "learning_rate": 1.856520453425333e-05,
      "loss": 4.9618,
      "step": 3640
    },
    {
      "epoch": 0.35978314440611137,
      "grad_norm": 3.4645395278930664,
      "learning_rate": 1.8561261705273535e-05,
      "loss": 4.8824,
      "step": 3650
    },
    {
      "epoch": 0.36076885165105965,
      "grad_norm": 4.289275169372559,
      "learning_rate": 1.8557318876293743e-05,
      "loss": 4.8357,
      "step": 3660
    },
    {
      "epoch": 0.3617545588960079,
      "grad_norm": 3.4682769775390625,
      "learning_rate": 1.8553376047313948e-05,
      "loss": 4.8648,
      "step": 3670
    },
    {
      "epoch": 0.3627402661409561,
      "grad_norm": 5.328495502471924,
      "learning_rate": 1.8549433218334156e-05,
      "loss": 4.7377,
      "step": 3680
    },
    {
      "epoch": 0.3637259733859044,
      "grad_norm": 4.102939605712891,
      "learning_rate": 1.8545490389354364e-05,
      "loss": 4.9466,
      "step": 3690
    },
    {
      "epoch": 0.36471168063085263,
      "grad_norm": 3.854370594024658,
      "learning_rate": 1.8541547560374572e-05,
      "loss": 4.7331,
      "step": 3700
    },
    {
      "epoch": 0.36569738787580086,
      "grad_norm": 4.352614879608154,
      "learning_rate": 1.8537604731394777e-05,
      "loss": 4.9608,
      "step": 3710
    },
    {
      "epoch": 0.36668309512074915,
      "grad_norm": 4.27982234954834,
      "learning_rate": 1.8533661902414982e-05,
      "loss": 4.826,
      "step": 3720
    },
    {
      "epoch": 0.3676688023656974,
      "grad_norm": 4.270840644836426,
      "learning_rate": 1.852971907343519e-05,
      "loss": 4.8882,
      "step": 3730
    },
    {
      "epoch": 0.36865450961064566,
      "grad_norm": 3.81618332862854,
      "learning_rate": 1.85257762444554e-05,
      "loss": 4.8482,
      "step": 3740
    },
    {
      "epoch": 0.3696402168555939,
      "grad_norm": 4.074657917022705,
      "learning_rate": 1.8521833415475606e-05,
      "loss": 4.7629,
      "step": 3750
    },
    {
      "epoch": 0.3706259241005421,
      "grad_norm": 4.882518291473389,
      "learning_rate": 1.851789058649581e-05,
      "loss": 4.911,
      "step": 3760
    },
    {
      "epoch": 0.3716116313454904,
      "grad_norm": 4.725922107696533,
      "learning_rate": 1.851394775751602e-05,
      "loss": 4.9446,
      "step": 3770
    },
    {
      "epoch": 0.37259733859043864,
      "grad_norm": 4.25604248046875,
      "learning_rate": 1.8510004928536228e-05,
      "loss": 4.8211,
      "step": 3780
    },
    {
      "epoch": 0.3735830458353869,
      "grad_norm": 4.24944543838501,
      "learning_rate": 1.8506062099556432e-05,
      "loss": 4.6855,
      "step": 3790
    },
    {
      "epoch": 0.37456875308033516,
      "grad_norm": 3.8788750171661377,
      "learning_rate": 1.850211927057664e-05,
      "loss": 4.8634,
      "step": 3800
    },
    {
      "epoch": 0.3755544603252834,
      "grad_norm": 4.791170120239258,
      "learning_rate": 1.8498176441596845e-05,
      "loss": 4.814,
      "step": 3810
    },
    {
      "epoch": 0.3765401675702316,
      "grad_norm": 4.546018600463867,
      "learning_rate": 1.8494233612617054e-05,
      "loss": 4.9263,
      "step": 3820
    },
    {
      "epoch": 0.3775258748151799,
      "grad_norm": 3.9441967010498047,
      "learning_rate": 1.8490290783637262e-05,
      "loss": 5.056,
      "step": 3830
    },
    {
      "epoch": 0.37851158206012814,
      "grad_norm": 4.90690803527832,
      "learning_rate": 1.848634795465747e-05,
      "loss": 4.8459,
      "step": 3840
    },
    {
      "epoch": 0.37949728930507637,
      "grad_norm": 4.345434665679932,
      "learning_rate": 1.8482405125677675e-05,
      "loss": 4.9361,
      "step": 3850
    },
    {
      "epoch": 0.38048299655002465,
      "grad_norm": 4.542006492614746,
      "learning_rate": 1.8478462296697883e-05,
      "loss": 4.8357,
      "step": 3860
    },
    {
      "epoch": 0.3814687037949729,
      "grad_norm": 3.8453097343444824,
      "learning_rate": 1.8474519467718088e-05,
      "loss": 4.772,
      "step": 3870
    },
    {
      "epoch": 0.3824544110399211,
      "grad_norm": 4.061624526977539,
      "learning_rate": 1.8470576638738296e-05,
      "loss": 4.6823,
      "step": 3880
    },
    {
      "epoch": 0.3834401182848694,
      "grad_norm": 3.8301687240600586,
      "learning_rate": 1.8466633809758504e-05,
      "loss": 4.8575,
      "step": 3890
    },
    {
      "epoch": 0.38442582552981763,
      "grad_norm": 4.692713737487793,
      "learning_rate": 1.846269098077871e-05,
      "loss": 4.8963,
      "step": 3900
    },
    {
      "epoch": 0.3854115327747659,
      "grad_norm": 4.094476699829102,
      "learning_rate": 1.8458748151798917e-05,
      "loss": 4.8028,
      "step": 3910
    },
    {
      "epoch": 0.38639724001971415,
      "grad_norm": 4.722322463989258,
      "learning_rate": 1.8454805322819125e-05,
      "loss": 4.8041,
      "step": 3920
    },
    {
      "epoch": 0.3873829472646624,
      "grad_norm": 4.630298137664795,
      "learning_rate": 1.8450862493839333e-05,
      "loss": 4.817,
      "step": 3930
    },
    {
      "epoch": 0.38836865450961067,
      "grad_norm": 4.193833351135254,
      "learning_rate": 1.8446919664859538e-05,
      "loss": 4.747,
      "step": 3940
    },
    {
      "epoch": 0.3893543617545589,
      "grad_norm": 3.787868022918701,
      "learning_rate": 1.8442976835879743e-05,
      "loss": 4.9079,
      "step": 3950
    },
    {
      "epoch": 0.39034006899950713,
      "grad_norm": 4.359759330749512,
      "learning_rate": 1.843903400689995e-05,
      "loss": 4.8733,
      "step": 3960
    },
    {
      "epoch": 0.3913257762444554,
      "grad_norm": 5.683457374572754,
      "learning_rate": 1.843509117792016e-05,
      "loss": 4.7911,
      "step": 3970
    },
    {
      "epoch": 0.39231148348940365,
      "grad_norm": 3.9088408946990967,
      "learning_rate": 1.8431148348940368e-05,
      "loss": 5.0055,
      "step": 3980
    },
    {
      "epoch": 0.3932971907343519,
      "grad_norm": 3.8774242401123047,
      "learning_rate": 1.8427205519960572e-05,
      "loss": 4.7374,
      "step": 3990
    },
    {
      "epoch": 0.39428289797930016,
      "grad_norm": 4.060990333557129,
      "learning_rate": 1.842326269098078e-05,
      "loss": 4.9258,
      "step": 4000
    },
    {
      "epoch": 0.3952686052242484,
      "grad_norm": 3.7701122760772705,
      "learning_rate": 1.841931986200099e-05,
      "loss": 4.8626,
      "step": 4010
    },
    {
      "epoch": 0.3962543124691966,
      "grad_norm": 3.6774942874908447,
      "learning_rate": 1.8415377033021194e-05,
      "loss": 4.7552,
      "step": 4020
    },
    {
      "epoch": 0.3972400197141449,
      "grad_norm": 5.385162353515625,
      "learning_rate": 1.8411434204041402e-05,
      "loss": 4.8354,
      "step": 4030
    },
    {
      "epoch": 0.39822572695909314,
      "grad_norm": 3.7041194438934326,
      "learning_rate": 1.8407491375061607e-05,
      "loss": 4.8815,
      "step": 4040
    },
    {
      "epoch": 0.3992114342040414,
      "grad_norm": 4.5218186378479,
      "learning_rate": 1.8403548546081815e-05,
      "loss": 4.7107,
      "step": 4050
    },
    {
      "epoch": 0.40019714144898966,
      "grad_norm": 4.746909141540527,
      "learning_rate": 1.8399605717102023e-05,
      "loss": 4.6477,
      "step": 4060
    },
    {
      "epoch": 0.4011828486939379,
      "grad_norm": 3.9782302379608154,
      "learning_rate": 1.839566288812223e-05,
      "loss": 4.7658,
      "step": 4070
    },
    {
      "epoch": 0.4021685559388862,
      "grad_norm": 4.148649215698242,
      "learning_rate": 1.8391720059142436e-05,
      "loss": 4.9445,
      "step": 4080
    },
    {
      "epoch": 0.4031542631838344,
      "grad_norm": 3.8386659622192383,
      "learning_rate": 1.838777723016264e-05,
      "loss": 4.7482,
      "step": 4090
    },
    {
      "epoch": 0.40413997042878264,
      "grad_norm": 5.366136074066162,
      "learning_rate": 1.838383440118285e-05,
      "loss": 4.7577,
      "step": 4100
    },
    {
      "epoch": 0.4051256776737309,
      "grad_norm": 3.8898799419403076,
      "learning_rate": 1.8379891572203057e-05,
      "loss": 4.959,
      "step": 4110
    },
    {
      "epoch": 0.40611138491867915,
      "grad_norm": 3.4550974369049072,
      "learning_rate": 1.8375948743223265e-05,
      "loss": 4.9068,
      "step": 4120
    },
    {
      "epoch": 0.4070970921636274,
      "grad_norm": 3.541886329650879,
      "learning_rate": 1.837200591424347e-05,
      "loss": 4.7705,
      "step": 4130
    },
    {
      "epoch": 0.40808279940857567,
      "grad_norm": 4.512409687042236,
      "learning_rate": 1.8368063085263678e-05,
      "loss": 4.8663,
      "step": 4140
    },
    {
      "epoch": 0.4090685066535239,
      "grad_norm": 3.8605830669403076,
      "learning_rate": 1.8364120256283886e-05,
      "loss": 4.8483,
      "step": 4150
    },
    {
      "epoch": 0.41005421389847213,
      "grad_norm": 4.497884273529053,
      "learning_rate": 1.836017742730409e-05,
      "loss": 4.7449,
      "step": 4160
    },
    {
      "epoch": 0.4110399211434204,
      "grad_norm": 3.836915969848633,
      "learning_rate": 1.83562345983243e-05,
      "loss": 4.9884,
      "step": 4170
    },
    {
      "epoch": 0.41202562838836865,
      "grad_norm": 4.390355587005615,
      "learning_rate": 1.8352291769344504e-05,
      "loss": 4.8955,
      "step": 4180
    },
    {
      "epoch": 0.4130113356333169,
      "grad_norm": 4.006931304931641,
      "learning_rate": 1.8348348940364712e-05,
      "loss": 4.7489,
      "step": 4190
    },
    {
      "epoch": 0.41399704287826516,
      "grad_norm": 3.4768035411834717,
      "learning_rate": 1.834440611138492e-05,
      "loss": 4.7316,
      "step": 4200
    },
    {
      "epoch": 0.4149827501232134,
      "grad_norm": 4.287578105926514,
      "learning_rate": 1.834046328240513e-05,
      "loss": 4.8405,
      "step": 4210
    },
    {
      "epoch": 0.4159684573681617,
      "grad_norm": 4.86144495010376,
      "learning_rate": 1.8336520453425333e-05,
      "loss": 4.6384,
      "step": 4220
    },
    {
      "epoch": 0.4169541646131099,
      "grad_norm": 4.181312561035156,
      "learning_rate": 1.833257762444554e-05,
      "loss": 4.836,
      "step": 4230
    },
    {
      "epoch": 0.41793987185805814,
      "grad_norm": 4.373114109039307,
      "learning_rate": 1.8328634795465746e-05,
      "loss": 4.8073,
      "step": 4240
    },
    {
      "epoch": 0.41892557910300643,
      "grad_norm": 4.064973831176758,
      "learning_rate": 1.8324691966485955e-05,
      "loss": 4.9286,
      "step": 4250
    },
    {
      "epoch": 0.41991128634795466,
      "grad_norm": 3.9852471351623535,
      "learning_rate": 1.8320749137506163e-05,
      "loss": 4.5908,
      "step": 4260
    },
    {
      "epoch": 0.4208969935929029,
      "grad_norm": 5.387211799621582,
      "learning_rate": 1.8316806308526368e-05,
      "loss": 4.5109,
      "step": 4270
    },
    {
      "epoch": 0.4218827008378512,
      "grad_norm": 3.9088315963745117,
      "learning_rate": 1.8312863479546576e-05,
      "loss": 4.8285,
      "step": 4280
    },
    {
      "epoch": 0.4228684080827994,
      "grad_norm": 4.247275352478027,
      "learning_rate": 1.8308920650566784e-05,
      "loss": 4.763,
      "step": 4290
    },
    {
      "epoch": 0.42385411532774764,
      "grad_norm": 3.7995805740356445,
      "learning_rate": 1.8304977821586992e-05,
      "loss": 4.8736,
      "step": 4300
    },
    {
      "epoch": 0.4248398225726959,
      "grad_norm": 4.714889049530029,
      "learning_rate": 1.8301034992607197e-05,
      "loss": 4.7723,
      "step": 4310
    },
    {
      "epoch": 0.42582552981764416,
      "grad_norm": 5.268800735473633,
      "learning_rate": 1.8297092163627405e-05,
      "loss": 4.6858,
      "step": 4320
    },
    {
      "epoch": 0.4268112370625924,
      "grad_norm": 4.6208109855651855,
      "learning_rate": 1.829314933464761e-05,
      "loss": 4.6813,
      "step": 4330
    },
    {
      "epoch": 0.42779694430754067,
      "grad_norm": 3.737661838531494,
      "learning_rate": 1.8289206505667818e-05,
      "loss": 4.8275,
      "step": 4340
    },
    {
      "epoch": 0.4287826515524889,
      "grad_norm": 4.187406063079834,
      "learning_rate": 1.8285263676688026e-05,
      "loss": 4.8091,
      "step": 4350
    },
    {
      "epoch": 0.4297683587974372,
      "grad_norm": 4.876672744750977,
      "learning_rate": 1.828132084770823e-05,
      "loss": 4.8323,
      "step": 4360
    },
    {
      "epoch": 0.4307540660423854,
      "grad_norm": 4.536261558532715,
      "learning_rate": 1.827737801872844e-05,
      "loss": 4.7733,
      "step": 4370
    },
    {
      "epoch": 0.43173977328733365,
      "grad_norm": 3.452359437942505,
      "learning_rate": 1.8273435189748644e-05,
      "loss": 4.886,
      "step": 4380
    },
    {
      "epoch": 0.43272548053228194,
      "grad_norm": 4.540109634399414,
      "learning_rate": 1.8269492360768852e-05,
      "loss": 4.7125,
      "step": 4390
    },
    {
      "epoch": 0.43371118777723017,
      "grad_norm": 4.640280246734619,
      "learning_rate": 1.826554953178906e-05,
      "loss": 4.9851,
      "step": 4400
    },
    {
      "epoch": 0.4346968950221784,
      "grad_norm": 3.679086208343506,
      "learning_rate": 1.826160670280927e-05,
      "loss": 4.7606,
      "step": 4410
    },
    {
      "epoch": 0.4356826022671267,
      "grad_norm": 5.4464592933654785,
      "learning_rate": 1.8257663873829473e-05,
      "loss": 4.8094,
      "step": 4420
    },
    {
      "epoch": 0.4366683095120749,
      "grad_norm": 4.675045490264893,
      "learning_rate": 1.825372104484968e-05,
      "loss": 4.9531,
      "step": 4430
    },
    {
      "epoch": 0.43765401675702315,
      "grad_norm": 4.0682501792907715,
      "learning_rate": 1.824977821586989e-05,
      "loss": 4.8652,
      "step": 4440
    },
    {
      "epoch": 0.43863972400197143,
      "grad_norm": 5.9431843757629395,
      "learning_rate": 1.8245835386890095e-05,
      "loss": 4.6918,
      "step": 4450
    },
    {
      "epoch": 0.43962543124691966,
      "grad_norm": 5.956040859222412,
      "learning_rate": 1.8241892557910303e-05,
      "loss": 4.8562,
      "step": 4460
    },
    {
      "epoch": 0.4406111384918679,
      "grad_norm": 4.744897842407227,
      "learning_rate": 1.8237949728930507e-05,
      "loss": 4.7996,
      "step": 4470
    },
    {
      "epoch": 0.4415968457368162,
      "grad_norm": 4.244429111480713,
      "learning_rate": 1.8234006899950716e-05,
      "loss": 4.8094,
      "step": 4480
    },
    {
      "epoch": 0.4425825529817644,
      "grad_norm": 4.057453632354736,
      "learning_rate": 1.8230064070970924e-05,
      "loss": 4.7689,
      "step": 4490
    },
    {
      "epoch": 0.44356826022671264,
      "grad_norm": 3.8896570205688477,
      "learning_rate": 1.8226121241991132e-05,
      "loss": 4.808,
      "step": 4500
    },
    {
      "epoch": 0.4445539674716609,
      "grad_norm": 4.132015705108643,
      "learning_rate": 1.8222178413011337e-05,
      "loss": 4.7525,
      "step": 4510
    },
    {
      "epoch": 0.44553967471660916,
      "grad_norm": 4.705348014831543,
      "learning_rate": 1.8218235584031545e-05,
      "loss": 4.601,
      "step": 4520
    },
    {
      "epoch": 0.44652538196155744,
      "grad_norm": 4.437334060668945,
      "learning_rate": 1.821429275505175e-05,
      "loss": 4.8497,
      "step": 4530
    },
    {
      "epoch": 0.4475110892065057,
      "grad_norm": 4.6853179931640625,
      "learning_rate": 1.8210349926071958e-05,
      "loss": 4.7713,
      "step": 4540
    },
    {
      "epoch": 0.4484967964514539,
      "grad_norm": 4.399407386779785,
      "learning_rate": 1.8206407097092166e-05,
      "loss": 4.7499,
      "step": 4550
    },
    {
      "epoch": 0.4494825036964022,
      "grad_norm": 4.489371299743652,
      "learning_rate": 1.820246426811237e-05,
      "loss": 4.8939,
      "step": 4560
    },
    {
      "epoch": 0.4504682109413504,
      "grad_norm": 3.8174943923950195,
      "learning_rate": 1.819852143913258e-05,
      "loss": 4.708,
      "step": 4570
    },
    {
      "epoch": 0.45145391818629865,
      "grad_norm": 4.482558250427246,
      "learning_rate": 1.8194578610152787e-05,
      "loss": 4.8082,
      "step": 4580
    },
    {
      "epoch": 0.45243962543124694,
      "grad_norm": 5.0607991218566895,
      "learning_rate": 1.8190635781172996e-05,
      "loss": 4.6914,
      "step": 4590
    },
    {
      "epoch": 0.45342533267619517,
      "grad_norm": 4.624401092529297,
      "learning_rate": 1.81866929521932e-05,
      "loss": 4.7053,
      "step": 4600
    },
    {
      "epoch": 0.4544110399211434,
      "grad_norm": 4.620789051055908,
      "learning_rate": 1.8182750123213405e-05,
      "loss": 4.8239,
      "step": 4610
    },
    {
      "epoch": 0.4553967471660917,
      "grad_norm": 4.465173721313477,
      "learning_rate": 1.8178807294233613e-05,
      "loss": 4.7372,
      "step": 4620
    },
    {
      "epoch": 0.4563824544110399,
      "grad_norm": 4.2748332023620605,
      "learning_rate": 1.817486446525382e-05,
      "loss": 4.7348,
      "step": 4630
    },
    {
      "epoch": 0.45736816165598815,
      "grad_norm": 3.61393141746521,
      "learning_rate": 1.817092163627403e-05,
      "loss": 4.5963,
      "step": 4640
    },
    {
      "epoch": 0.45835386890093643,
      "grad_norm": 6.332948207855225,
      "learning_rate": 1.8166978807294234e-05,
      "loss": 4.73,
      "step": 4650
    },
    {
      "epoch": 0.45933957614588466,
      "grad_norm": 5.079867839813232,
      "learning_rate": 1.8163035978314443e-05,
      "loss": 4.7838,
      "step": 4660
    },
    {
      "epoch": 0.4603252833908329,
      "grad_norm": 3.8706400394439697,
      "learning_rate": 1.815909314933465e-05,
      "loss": 4.881,
      "step": 4670
    },
    {
      "epoch": 0.4613109906357812,
      "grad_norm": 5.251516819000244,
      "learning_rate": 1.8155150320354856e-05,
      "loss": 4.9558,
      "step": 4680
    },
    {
      "epoch": 0.4622966978807294,
      "grad_norm": 3.989978075027466,
      "learning_rate": 1.8151207491375064e-05,
      "loss": 4.8294,
      "step": 4690
    },
    {
      "epoch": 0.4632824051256777,
      "grad_norm": 4.204742431640625,
      "learning_rate": 1.814726466239527e-05,
      "loss": 4.7491,
      "step": 4700
    },
    {
      "epoch": 0.46426811237062593,
      "grad_norm": 3.3350586891174316,
      "learning_rate": 1.8143321833415477e-05,
      "loss": 4.6571,
      "step": 4710
    },
    {
      "epoch": 0.46525381961557416,
      "grad_norm": 4.412384986877441,
      "learning_rate": 1.8139379004435685e-05,
      "loss": 4.7303,
      "step": 4720
    },
    {
      "epoch": 0.46623952686052245,
      "grad_norm": 4.264808654785156,
      "learning_rate": 1.8135436175455893e-05,
      "loss": 4.7818,
      "step": 4730
    },
    {
      "epoch": 0.4672252341054707,
      "grad_norm": 4.48480224609375,
      "learning_rate": 1.8131493346476098e-05,
      "loss": 4.8107,
      "step": 4740
    },
    {
      "epoch": 0.4682109413504189,
      "grad_norm": 3.8314433097839355,
      "learning_rate": 1.8127550517496303e-05,
      "loss": 4.6021,
      "step": 4750
    },
    {
      "epoch": 0.4691966485953672,
      "grad_norm": 4.186056613922119,
      "learning_rate": 1.812360768851651e-05,
      "loss": 4.7054,
      "step": 4760
    },
    {
      "epoch": 0.4701823558403154,
      "grad_norm": 4.195452690124512,
      "learning_rate": 1.811966485953672e-05,
      "loss": 4.6589,
      "step": 4770
    },
    {
      "epoch": 0.47116806308526366,
      "grad_norm": 5.121286869049072,
      "learning_rate": 1.8115722030556927e-05,
      "loss": 4.7271,
      "step": 4780
    },
    {
      "epoch": 0.47215377033021194,
      "grad_norm": 4.663058280944824,
      "learning_rate": 1.8111779201577132e-05,
      "loss": 4.6894,
      "step": 4790
    },
    {
      "epoch": 0.47313947757516017,
      "grad_norm": 3.8244247436523438,
      "learning_rate": 1.810783637259734e-05,
      "loss": 4.6715,
      "step": 4800
    },
    {
      "epoch": 0.4741251848201084,
      "grad_norm": 4.403789520263672,
      "learning_rate": 1.810389354361755e-05,
      "loss": 4.8725,
      "step": 4810
    },
    {
      "epoch": 0.4751108920650567,
      "grad_norm": 3.94608736038208,
      "learning_rate": 1.8099950714637757e-05,
      "loss": 4.7698,
      "step": 4820
    },
    {
      "epoch": 0.4760965993100049,
      "grad_norm": 4.256720066070557,
      "learning_rate": 1.809600788565796e-05,
      "loss": 4.712,
      "step": 4830
    },
    {
      "epoch": 0.4770823065549532,
      "grad_norm": 3.9979405403137207,
      "learning_rate": 1.8092065056678166e-05,
      "loss": 4.6299,
      "step": 4840
    },
    {
      "epoch": 0.47806801379990144,
      "grad_norm": 4.455038070678711,
      "learning_rate": 1.8088122227698374e-05,
      "loss": 4.6247,
      "step": 4850
    },
    {
      "epoch": 0.47905372104484967,
      "grad_norm": 4.980525016784668,
      "learning_rate": 1.8084179398718583e-05,
      "loss": 4.7641,
      "step": 4860
    },
    {
      "epoch": 0.48003942828979795,
      "grad_norm": 4.8384480476379395,
      "learning_rate": 1.808023656973879e-05,
      "loss": 4.7767,
      "step": 4870
    },
    {
      "epoch": 0.4810251355347462,
      "grad_norm": 4.131738662719727,
      "learning_rate": 1.8076293740758996e-05,
      "loss": 4.6601,
      "step": 4880
    },
    {
      "epoch": 0.4820108427796944,
      "grad_norm": 4.6256632804870605,
      "learning_rate": 1.8072350911779204e-05,
      "loss": 4.8096,
      "step": 4890
    },
    {
      "epoch": 0.4829965500246427,
      "grad_norm": 4.215279579162598,
      "learning_rate": 1.806840808279941e-05,
      "loss": 4.6962,
      "step": 4900
    },
    {
      "epoch": 0.48398225726959093,
      "grad_norm": 4.707536697387695,
      "learning_rate": 1.8064465253819617e-05,
      "loss": 4.7035,
      "step": 4910
    },
    {
      "epoch": 0.48496796451453916,
      "grad_norm": 4.689214706420898,
      "learning_rate": 1.8060522424839825e-05,
      "loss": 4.7019,
      "step": 4920
    },
    {
      "epoch": 0.48595367175948745,
      "grad_norm": 5.059239387512207,
      "learning_rate": 1.805657959586003e-05,
      "loss": 4.6757,
      "step": 4930
    },
    {
      "epoch": 0.4869393790044357,
      "grad_norm": 3.3821675777435303,
      "learning_rate": 1.8052636766880238e-05,
      "loss": 4.6455,
      "step": 4940
    },
    {
      "epoch": 0.4879250862493839,
      "grad_norm": 4.328376293182373,
      "learning_rate": 1.8048693937900446e-05,
      "loss": 4.7659,
      "step": 4950
    },
    {
      "epoch": 0.4889107934943322,
      "grad_norm": 4.033332347869873,
      "learning_rate": 1.8044751108920654e-05,
      "loss": 4.7574,
      "step": 4960
    },
    {
      "epoch": 0.4898965007392804,
      "grad_norm": 4.521473407745361,
      "learning_rate": 1.804080827994086e-05,
      "loss": 4.7108,
      "step": 4970
    },
    {
      "epoch": 0.49088220798422866,
      "grad_norm": 4.701443672180176,
      "learning_rate": 1.8036865450961064e-05,
      "loss": 4.6369,
      "step": 4980
    },
    {
      "epoch": 0.49186791522917694,
      "grad_norm": 4.432566165924072,
      "learning_rate": 1.8032922621981272e-05,
      "loss": 4.7957,
      "step": 4990
    },
    {
      "epoch": 0.4928536224741252,
      "grad_norm": 4.003012657165527,
      "learning_rate": 1.802897979300148e-05,
      "loss": 4.669,
      "step": 5000
    },
    {
      "epoch": 0.49383932971907346,
      "grad_norm": 4.683012962341309,
      "learning_rate": 1.802503696402169e-05,
      "loss": 4.774,
      "step": 5010
    },
    {
      "epoch": 0.4948250369640217,
      "grad_norm": 4.602590084075928,
      "learning_rate": 1.8021094135041893e-05,
      "loss": 4.5799,
      "step": 5020
    },
    {
      "epoch": 0.4958107442089699,
      "grad_norm": 4.878795623779297,
      "learning_rate": 1.80171513060621e-05,
      "loss": 4.7103,
      "step": 5030
    },
    {
      "epoch": 0.4967964514539182,
      "grad_norm": 4.139692783355713,
      "learning_rate": 1.801320847708231e-05,
      "loss": 4.6237,
      "step": 5040
    },
    {
      "epoch": 0.49778215869886644,
      "grad_norm": 4.177364826202393,
      "learning_rate": 1.8009265648102514e-05,
      "loss": 4.7364,
      "step": 5050
    },
    {
      "epoch": 0.49876786594381467,
      "grad_norm": 5.508412837982178,
      "learning_rate": 1.8005322819122722e-05,
      "loss": 4.6871,
      "step": 5060
    },
    {
      "epoch": 0.49975357318876296,
      "grad_norm": 5.47540283203125,
      "learning_rate": 1.8001379990142927e-05,
      "loss": 4.6725,
      "step": 5070
    },
    {
      "epoch": 0.5007392804337112,
      "grad_norm": 4.121697902679443,
      "learning_rate": 1.7997437161163135e-05,
      "loss": 4.6606,
      "step": 5080
    },
    {
      "epoch": 0.5017249876786595,
      "grad_norm": 4.419307231903076,
      "learning_rate": 1.7993494332183344e-05,
      "loss": 4.8403,
      "step": 5090
    },
    {
      "epoch": 0.5027106949236076,
      "grad_norm": 4.547325134277344,
      "learning_rate": 1.7989551503203552e-05,
      "loss": 4.7746,
      "step": 5100
    },
    {
      "epoch": 0.5036964021685559,
      "grad_norm": 4.438759803771973,
      "learning_rate": 1.7985608674223757e-05,
      "loss": 4.6973,
      "step": 5110
    },
    {
      "epoch": 0.5046821094135042,
      "grad_norm": 5.223434925079346,
      "learning_rate": 1.798166584524396e-05,
      "loss": 4.4586,
      "step": 5120
    },
    {
      "epoch": 0.5056678166584524,
      "grad_norm": 3.5720765590667725,
      "learning_rate": 1.797772301626417e-05,
      "loss": 4.8263,
      "step": 5130
    },
    {
      "epoch": 0.5066535239034007,
      "grad_norm": 4.570855140686035,
      "learning_rate": 1.7973780187284378e-05,
      "loss": 4.6721,
      "step": 5140
    },
    {
      "epoch": 0.507639231148349,
      "grad_norm": 3.319467306137085,
      "learning_rate": 1.7969837358304586e-05,
      "loss": 4.6902,
      "step": 5150
    },
    {
      "epoch": 0.5086249383932971,
      "grad_norm": 4.30423641204834,
      "learning_rate": 1.796589452932479e-05,
      "loss": 4.7336,
      "step": 5160
    },
    {
      "epoch": 0.5096106456382454,
      "grad_norm": 4.943424224853516,
      "learning_rate": 1.7961951700345e-05,
      "loss": 4.7043,
      "step": 5170
    },
    {
      "epoch": 0.5105963528831937,
      "grad_norm": 4.495391368865967,
      "learning_rate": 1.7958008871365207e-05,
      "loss": 4.683,
      "step": 5180
    },
    {
      "epoch": 0.5115820601281419,
      "grad_norm": 5.4516282081604,
      "learning_rate": 1.7954066042385415e-05,
      "loss": 4.6972,
      "step": 5190
    },
    {
      "epoch": 0.5125677673730902,
      "grad_norm": 4.262962341308594,
      "learning_rate": 1.795012321340562e-05,
      "loss": 4.6284,
      "step": 5200
    },
    {
      "epoch": 0.5135534746180385,
      "grad_norm": 4.615508079528809,
      "learning_rate": 1.7946180384425825e-05,
      "loss": 4.7001,
      "step": 5210
    },
    {
      "epoch": 0.5145391818629866,
      "grad_norm": 4.45475435256958,
      "learning_rate": 1.7942237555446033e-05,
      "loss": 4.8008,
      "step": 5220
    },
    {
      "epoch": 0.5155248891079349,
      "grad_norm": 5.025050640106201,
      "learning_rate": 1.793829472646624e-05,
      "loss": 4.6936,
      "step": 5230
    },
    {
      "epoch": 0.5165105963528832,
      "grad_norm": 4.413112163543701,
      "learning_rate": 1.793435189748645e-05,
      "loss": 4.8731,
      "step": 5240
    },
    {
      "epoch": 0.5174963035978315,
      "grad_norm": 4.56916618347168,
      "learning_rate": 1.7930409068506654e-05,
      "loss": 4.8034,
      "step": 5250
    },
    {
      "epoch": 0.5184820108427797,
      "grad_norm": 4.389017581939697,
      "learning_rate": 1.7926466239526862e-05,
      "loss": 4.5938,
      "step": 5260
    },
    {
      "epoch": 0.519467718087728,
      "grad_norm": 3.779353141784668,
      "learning_rate": 1.7922523410547067e-05,
      "loss": 4.534,
      "step": 5270
    },
    {
      "epoch": 0.5204534253326762,
      "grad_norm": 4.40644645690918,
      "learning_rate": 1.7918580581567275e-05,
      "loss": 4.6855,
      "step": 5280
    },
    {
      "epoch": 0.5214391325776244,
      "grad_norm": 5.004255771636963,
      "learning_rate": 1.7914637752587484e-05,
      "loss": 4.6987,
      "step": 5290
    },
    {
      "epoch": 0.5224248398225727,
      "grad_norm": 3.890868902206421,
      "learning_rate": 1.791069492360769e-05,
      "loss": 4.6604,
      "step": 5300
    },
    {
      "epoch": 0.523410547067521,
      "grad_norm": 4.624227523803711,
      "learning_rate": 1.7906752094627897e-05,
      "loss": 4.8414,
      "step": 5310
    },
    {
      "epoch": 0.5243962543124692,
      "grad_norm": 3.891794443130493,
      "learning_rate": 1.7902809265648105e-05,
      "loss": 4.562,
      "step": 5320
    },
    {
      "epoch": 0.5253819615574175,
      "grad_norm": 4.482423782348633,
      "learning_rate": 1.7898866436668313e-05,
      "loss": 4.7111,
      "step": 5330
    },
    {
      "epoch": 0.5263676688023657,
      "grad_norm": 3.942577600479126,
      "learning_rate": 1.7894923607688518e-05,
      "loss": 4.6635,
      "step": 5340
    },
    {
      "epoch": 0.5273533760473139,
      "grad_norm": 4.779184818267822,
      "learning_rate": 1.7890980778708726e-05,
      "loss": 4.4991,
      "step": 5350
    },
    {
      "epoch": 0.5283390832922622,
      "grad_norm": 4.7607879638671875,
      "learning_rate": 1.788703794972893e-05,
      "loss": 4.6924,
      "step": 5360
    },
    {
      "epoch": 0.5293247905372105,
      "grad_norm": 3.9554085731506348,
      "learning_rate": 1.788309512074914e-05,
      "loss": 4.7327,
      "step": 5370
    },
    {
      "epoch": 0.5303104977821587,
      "grad_norm": 3.9586758613586426,
      "learning_rate": 1.7879152291769347e-05,
      "loss": 4.6956,
      "step": 5380
    },
    {
      "epoch": 0.531296205027107,
      "grad_norm": 4.573141574859619,
      "learning_rate": 1.7875209462789552e-05,
      "loss": 4.6649,
      "step": 5390
    },
    {
      "epoch": 0.5322819122720552,
      "grad_norm": 4.881220817565918,
      "learning_rate": 1.787126663380976e-05,
      "loss": 4.6955,
      "step": 5400
    },
    {
      "epoch": 0.5332676195170034,
      "grad_norm": 4.480893135070801,
      "learning_rate": 1.7867323804829968e-05,
      "loss": 4.7025,
      "step": 5410
    },
    {
      "epoch": 0.5342533267619517,
      "grad_norm": 4.062025547027588,
      "learning_rate": 1.7863380975850173e-05,
      "loss": 4.6008,
      "step": 5420
    },
    {
      "epoch": 0.5352390340069,
      "grad_norm": 4.568418979644775,
      "learning_rate": 1.785943814687038e-05,
      "loss": 4.65,
      "step": 5430
    },
    {
      "epoch": 0.5362247412518482,
      "grad_norm": 3.4631547927856445,
      "learning_rate": 1.785549531789059e-05,
      "loss": 4.605,
      "step": 5440
    },
    {
      "epoch": 0.5372104484967964,
      "grad_norm": 4.981983184814453,
      "learning_rate": 1.7851552488910794e-05,
      "loss": 4.6549,
      "step": 5450
    },
    {
      "epoch": 0.5381961557417447,
      "grad_norm": 3.878525733947754,
      "learning_rate": 1.7847609659931002e-05,
      "loss": 4.7015,
      "step": 5460
    },
    {
      "epoch": 0.5391818629866929,
      "grad_norm": 5.143031597137451,
      "learning_rate": 1.784366683095121e-05,
      "loss": 4.6896,
      "step": 5470
    },
    {
      "epoch": 0.5401675702316412,
      "grad_norm": 4.645218849182129,
      "learning_rate": 1.7839724001971415e-05,
      "loss": 4.6217,
      "step": 5480
    },
    {
      "epoch": 0.5411532774765895,
      "grad_norm": 4.041226863861084,
      "learning_rate": 1.7835781172991623e-05,
      "loss": 4.8167,
      "step": 5490
    },
    {
      "epoch": 0.5421389847215377,
      "grad_norm": 4.020968437194824,
      "learning_rate": 1.7831838344011828e-05,
      "loss": 4.6049,
      "step": 5500
    },
    {
      "epoch": 0.5431246919664859,
      "grad_norm": 5.033664703369141,
      "learning_rate": 1.7827895515032036e-05,
      "loss": 4.7712,
      "step": 5510
    },
    {
      "epoch": 0.5441103992114342,
      "grad_norm": 3.687445878982544,
      "learning_rate": 1.7823952686052245e-05,
      "loss": 4.7999,
      "step": 5520
    },
    {
      "epoch": 0.5450961064563824,
      "grad_norm": 3.679943084716797,
      "learning_rate": 1.7820009857072453e-05,
      "loss": 4.6043,
      "step": 5530
    },
    {
      "epoch": 0.5460818137013307,
      "grad_norm": 4.924044132232666,
      "learning_rate": 1.7816067028092658e-05,
      "loss": 4.8696,
      "step": 5540
    },
    {
      "epoch": 0.547067520946279,
      "grad_norm": 4.270914554595947,
      "learning_rate": 1.7812124199112866e-05,
      "loss": 4.6299,
      "step": 5550
    },
    {
      "epoch": 0.5480532281912273,
      "grad_norm": 4.912529468536377,
      "learning_rate": 1.7808181370133074e-05,
      "loss": 4.6207,
      "step": 5560
    },
    {
      "epoch": 0.5490389354361754,
      "grad_norm": 4.5233893394470215,
      "learning_rate": 1.780423854115328e-05,
      "loss": 4.6983,
      "step": 5570
    },
    {
      "epoch": 0.5500246426811237,
      "grad_norm": 4.293355941772461,
      "learning_rate": 1.7800295712173487e-05,
      "loss": 4.6998,
      "step": 5580
    },
    {
      "epoch": 0.551010349926072,
      "grad_norm": 4.375978946685791,
      "learning_rate": 1.7796352883193692e-05,
      "loss": 4.7836,
      "step": 5590
    },
    {
      "epoch": 0.5519960571710202,
      "grad_norm": 3.8980164527893066,
      "learning_rate": 1.77924100542139e-05,
      "loss": 4.5597,
      "step": 5600
    },
    {
      "epoch": 0.5529817644159685,
      "grad_norm": 4.538483142852783,
      "learning_rate": 1.7788467225234108e-05,
      "loss": 4.6178,
      "step": 5610
    },
    {
      "epoch": 0.5539674716609168,
      "grad_norm": 4.834667682647705,
      "learning_rate": 1.7784524396254316e-05,
      "loss": 4.7519,
      "step": 5620
    },
    {
      "epoch": 0.5549531789058649,
      "grad_norm": 4.069507598876953,
      "learning_rate": 1.778058156727452e-05,
      "loss": 4.68,
      "step": 5630
    },
    {
      "epoch": 0.5559388861508132,
      "grad_norm": 3.8088228702545166,
      "learning_rate": 1.7776638738294726e-05,
      "loss": 4.6637,
      "step": 5640
    },
    {
      "epoch": 0.5569245933957615,
      "grad_norm": 5.738439083099365,
      "learning_rate": 1.7772695909314934e-05,
      "loss": 4.4525,
      "step": 5650
    },
    {
      "epoch": 0.5579103006407097,
      "grad_norm": 3.6112430095672607,
      "learning_rate": 1.7768753080335142e-05,
      "loss": 4.5864,
      "step": 5660
    },
    {
      "epoch": 0.558896007885658,
      "grad_norm": 4.247114658355713,
      "learning_rate": 1.776481025135535e-05,
      "loss": 4.6894,
      "step": 5670
    },
    {
      "epoch": 0.5598817151306063,
      "grad_norm": 4.614151477813721,
      "learning_rate": 1.7760867422375555e-05,
      "loss": 4.6222,
      "step": 5680
    },
    {
      "epoch": 0.5608674223755544,
      "grad_norm": 4.711928844451904,
      "learning_rate": 1.7756924593395763e-05,
      "loss": 4.6418,
      "step": 5690
    },
    {
      "epoch": 0.5618531296205027,
      "grad_norm": 4.886808395385742,
      "learning_rate": 1.775298176441597e-05,
      "loss": 4.6653,
      "step": 5700
    },
    {
      "epoch": 0.562838836865451,
      "grad_norm": 4.227883815765381,
      "learning_rate": 1.7749038935436176e-05,
      "loss": 4.6137,
      "step": 5710
    },
    {
      "epoch": 0.5638245441103992,
      "grad_norm": 4.927032947540283,
      "learning_rate": 1.7745096106456385e-05,
      "loss": 4.6416,
      "step": 5720
    },
    {
      "epoch": 0.5648102513553475,
      "grad_norm": 3.9745566844940186,
      "learning_rate": 1.774115327747659e-05,
      "loss": 4.5968,
      "step": 5730
    },
    {
      "epoch": 0.5657959586002957,
      "grad_norm": 3.8460910320281982,
      "learning_rate": 1.7737210448496798e-05,
      "loss": 4.7634,
      "step": 5740
    },
    {
      "epoch": 0.5667816658452439,
      "grad_norm": 3.3741636276245117,
      "learning_rate": 1.7733267619517006e-05,
      "loss": 4.5482,
      "step": 5750
    },
    {
      "epoch": 0.5677673730901922,
      "grad_norm": 4.047481536865234,
      "learning_rate": 1.7729324790537214e-05,
      "loss": 4.7248,
      "step": 5760
    },
    {
      "epoch": 0.5687530803351405,
      "grad_norm": 3.7661349773406982,
      "learning_rate": 1.772538196155742e-05,
      "loss": 4.6185,
      "step": 5770
    },
    {
      "epoch": 0.5697387875800887,
      "grad_norm": 4.255718231201172,
      "learning_rate": 1.7721439132577627e-05,
      "loss": 4.6785,
      "step": 5780
    },
    {
      "epoch": 0.570724494825037,
      "grad_norm": 4.661497116088867,
      "learning_rate": 1.771749630359783e-05,
      "loss": 4.6985,
      "step": 5790
    },
    {
      "epoch": 0.5717102020699852,
      "grad_norm": 5.319787502288818,
      "learning_rate": 1.771355347461804e-05,
      "loss": 4.4755,
      "step": 5800
    },
    {
      "epoch": 0.5726959093149334,
      "grad_norm": 4.461150646209717,
      "learning_rate": 1.7709610645638248e-05,
      "loss": 4.6618,
      "step": 5810
    },
    {
      "epoch": 0.5736816165598817,
      "grad_norm": 3.9111969470977783,
      "learning_rate": 1.7705667816658453e-05,
      "loss": 4.5851,
      "step": 5820
    },
    {
      "epoch": 0.57466732380483,
      "grad_norm": 5.0350661277771,
      "learning_rate": 1.770172498767866e-05,
      "loss": 4.6735,
      "step": 5830
    },
    {
      "epoch": 0.5756530310497782,
      "grad_norm": 5.026872158050537,
      "learning_rate": 1.769778215869887e-05,
      "loss": 4.6131,
      "step": 5840
    },
    {
      "epoch": 0.5766387382947264,
      "grad_norm": 4.2976393699646,
      "learning_rate": 1.7693839329719077e-05,
      "loss": 4.6163,
      "step": 5850
    },
    {
      "epoch": 0.5776244455396747,
      "grad_norm": 4.300207614898682,
      "learning_rate": 1.7689896500739282e-05,
      "loss": 4.7369,
      "step": 5860
    },
    {
      "epoch": 0.578610152784623,
      "grad_norm": 3.613375186920166,
      "learning_rate": 1.7685953671759487e-05,
      "loss": 4.6712,
      "step": 5870
    },
    {
      "epoch": 0.5795958600295712,
      "grad_norm": 3.3827898502349854,
      "learning_rate": 1.7682010842779695e-05,
      "loss": 4.7437,
      "step": 5880
    },
    {
      "epoch": 0.5805815672745195,
      "grad_norm": 4.327151298522949,
      "learning_rate": 1.7678068013799903e-05,
      "loss": 4.7086,
      "step": 5890
    },
    {
      "epoch": 0.5815672745194678,
      "grad_norm": 4.391021728515625,
      "learning_rate": 1.767412518482011e-05,
      "loss": 4.5404,
      "step": 5900
    },
    {
      "epoch": 0.582552981764416,
      "grad_norm": 3.995824098587036,
      "learning_rate": 1.7670182355840316e-05,
      "loss": 4.6215,
      "step": 5910
    },
    {
      "epoch": 0.5835386890093642,
      "grad_norm": 4.333681583404541,
      "learning_rate": 1.7666239526860524e-05,
      "loss": 4.6486,
      "step": 5920
    },
    {
      "epoch": 0.5845243962543125,
      "grad_norm": 4.394413471221924,
      "learning_rate": 1.766229669788073e-05,
      "loss": 4.6357,
      "step": 5930
    },
    {
      "epoch": 0.5855101034992607,
      "grad_norm": 4.664545059204102,
      "learning_rate": 1.7658353868900937e-05,
      "loss": 4.5545,
      "step": 5940
    },
    {
      "epoch": 0.586495810744209,
      "grad_norm": 4.165056228637695,
      "learning_rate": 1.7654411039921146e-05,
      "loss": 4.6604,
      "step": 5950
    },
    {
      "epoch": 0.5874815179891573,
      "grad_norm": 4.567380428314209,
      "learning_rate": 1.765046821094135e-05,
      "loss": 4.6489,
      "step": 5960
    },
    {
      "epoch": 0.5884672252341054,
      "grad_norm": 4.223341941833496,
      "learning_rate": 1.764652538196156e-05,
      "loss": 4.6707,
      "step": 5970
    },
    {
      "epoch": 0.5894529324790537,
      "grad_norm": 4.023601055145264,
      "learning_rate": 1.7642582552981767e-05,
      "loss": 4.6142,
      "step": 5980
    },
    {
      "epoch": 0.590438639724002,
      "grad_norm": 4.075451850891113,
      "learning_rate": 1.7638639724001975e-05,
      "loss": 4.604,
      "step": 5990
    },
    {
      "epoch": 0.5914243469689502,
      "grad_norm": 5.003436088562012,
      "learning_rate": 1.763469689502218e-05,
      "loss": 4.703,
      "step": 6000
    },
    {
      "epoch": 0.5924100542138985,
      "grad_norm": 3.8373658657073975,
      "learning_rate": 1.7630754066042385e-05,
      "loss": 4.6534,
      "step": 6010
    },
    {
      "epoch": 0.5933957614588468,
      "grad_norm": 3.7166895866394043,
      "learning_rate": 1.7626811237062593e-05,
      "loss": 4.7155,
      "step": 6020
    },
    {
      "epoch": 0.5943814687037949,
      "grad_norm": 5.625546932220459,
      "learning_rate": 1.76228684080828e-05,
      "loss": 4.6578,
      "step": 6030
    },
    {
      "epoch": 0.5953671759487432,
      "grad_norm": 3.6482818126678467,
      "learning_rate": 1.761892557910301e-05,
      "loss": 4.6291,
      "step": 6040
    },
    {
      "epoch": 0.5963528831936915,
      "grad_norm": 3.8627138137817383,
      "learning_rate": 1.7614982750123214e-05,
      "loss": 4.5766,
      "step": 6050
    },
    {
      "epoch": 0.5973385904386397,
      "grad_norm": 4.7673563957214355,
      "learning_rate": 1.7611039921143422e-05,
      "loss": 4.5818,
      "step": 6060
    },
    {
      "epoch": 0.598324297683588,
      "grad_norm": 4.990085124969482,
      "learning_rate": 1.760709709216363e-05,
      "loss": 4.5792,
      "step": 6070
    },
    {
      "epoch": 0.5993100049285363,
      "grad_norm": 4.521392822265625,
      "learning_rate": 1.7603154263183835e-05,
      "loss": 4.5664,
      "step": 6080
    },
    {
      "epoch": 0.6002957121734844,
      "grad_norm": 5.192636966705322,
      "learning_rate": 1.7599211434204043e-05,
      "loss": 4.5439,
      "step": 6090
    },
    {
      "epoch": 0.6012814194184327,
      "grad_norm": 5.1416144371032715,
      "learning_rate": 1.7595268605224248e-05,
      "loss": 4.6408,
      "step": 6100
    },
    {
      "epoch": 0.602267126663381,
      "grad_norm": 4.776566028594971,
      "learning_rate": 1.7591325776244456e-05,
      "loss": 4.5196,
      "step": 6110
    },
    {
      "epoch": 0.6032528339083292,
      "grad_norm": 3.792879819869995,
      "learning_rate": 1.7587382947264664e-05,
      "loss": 4.5595,
      "step": 6120
    },
    {
      "epoch": 0.6042385411532775,
      "grad_norm": 4.534742832183838,
      "learning_rate": 1.7583440118284873e-05,
      "loss": 4.6907,
      "step": 6130
    },
    {
      "epoch": 0.6052242483982258,
      "grad_norm": 3.8166160583496094,
      "learning_rate": 1.7579497289305077e-05,
      "loss": 4.7492,
      "step": 6140
    },
    {
      "epoch": 0.6062099556431739,
      "grad_norm": 3.4328370094299316,
      "learning_rate": 1.7575554460325286e-05,
      "loss": 4.6463,
      "step": 6150
    },
    {
      "epoch": 0.6071956628881222,
      "grad_norm": 4.972326278686523,
      "learning_rate": 1.757161163134549e-05,
      "loss": 4.5048,
      "step": 6160
    },
    {
      "epoch": 0.6081813701330705,
      "grad_norm": 4.918020248413086,
      "learning_rate": 1.75676688023657e-05,
      "loss": 4.5864,
      "step": 6170
    },
    {
      "epoch": 0.6091670773780187,
      "grad_norm": 5.011263370513916,
      "learning_rate": 1.7563725973385907e-05,
      "loss": 4.5334,
      "step": 6180
    },
    {
      "epoch": 0.610152784622967,
      "grad_norm": 4.24698543548584,
      "learning_rate": 1.755978314440611e-05,
      "loss": 4.6376,
      "step": 6190
    },
    {
      "epoch": 0.6111384918679152,
      "grad_norm": 4.757246971130371,
      "learning_rate": 1.755584031542632e-05,
      "loss": 4.4444,
      "step": 6200
    },
    {
      "epoch": 0.6121241991128635,
      "grad_norm": 3.6395576000213623,
      "learning_rate": 1.7551897486446528e-05,
      "loss": 4.4675,
      "step": 6210
    },
    {
      "epoch": 0.6131099063578117,
      "grad_norm": 4.877581596374512,
      "learning_rate": 1.7547954657466736e-05,
      "loss": 4.6095,
      "step": 6220
    },
    {
      "epoch": 0.61409561360276,
      "grad_norm": 6.790589332580566,
      "learning_rate": 1.754401182848694e-05,
      "loss": 4.6228,
      "step": 6230
    },
    {
      "epoch": 0.6150813208477083,
      "grad_norm": 4.063344955444336,
      "learning_rate": 1.7540068999507146e-05,
      "loss": 4.7346,
      "step": 6240
    },
    {
      "epoch": 0.6160670280926565,
      "grad_norm": 4.277778625488281,
      "learning_rate": 1.7536126170527354e-05,
      "loss": 4.5941,
      "step": 6250
    },
    {
      "epoch": 0.6170527353376047,
      "grad_norm": 4.368870258331299,
      "learning_rate": 1.7532183341547562e-05,
      "loss": 4.662,
      "step": 6260
    },
    {
      "epoch": 0.618038442582553,
      "grad_norm": 4.176484107971191,
      "learning_rate": 1.752824051256777e-05,
      "loss": 4.5571,
      "step": 6270
    },
    {
      "epoch": 0.6190241498275012,
      "grad_norm": 3.8563952445983887,
      "learning_rate": 1.7524297683587975e-05,
      "loss": 4.6328,
      "step": 6280
    },
    {
      "epoch": 0.6200098570724495,
      "grad_norm": 3.9391446113586426,
      "learning_rate": 1.7520354854608183e-05,
      "loss": 4.6117,
      "step": 6290
    },
    {
      "epoch": 0.6209955643173978,
      "grad_norm": 4.2398457527160645,
      "learning_rate": 1.7516412025628388e-05,
      "loss": 4.6876,
      "step": 6300
    },
    {
      "epoch": 0.621981271562346,
      "grad_norm": 4.106077671051025,
      "learning_rate": 1.7512469196648596e-05,
      "loss": 4.6418,
      "step": 6310
    },
    {
      "epoch": 0.6229669788072942,
      "grad_norm": 3.978623867034912,
      "learning_rate": 1.7508526367668804e-05,
      "loss": 4.605,
      "step": 6320
    },
    {
      "epoch": 0.6239526860522425,
      "grad_norm": 5.182901382446289,
      "learning_rate": 1.750458353868901e-05,
      "loss": 4.4918,
      "step": 6330
    },
    {
      "epoch": 0.6249383932971907,
      "grad_norm": 4.350689888000488,
      "learning_rate": 1.7500640709709217e-05,
      "loss": 4.5948,
      "step": 6340
    },
    {
      "epoch": 0.625924100542139,
      "grad_norm": 4.968380928039551,
      "learning_rate": 1.7496697880729425e-05,
      "loss": 4.6066,
      "step": 6350
    },
    {
      "epoch": 0.6269098077870873,
      "grad_norm": 5.3265700340271,
      "learning_rate": 1.7492755051749634e-05,
      "loss": 4.4999,
      "step": 6360
    },
    {
      "epoch": 0.6278955150320354,
      "grad_norm": 4.564316272735596,
      "learning_rate": 1.748881222276984e-05,
      "loss": 4.6066,
      "step": 6370
    },
    {
      "epoch": 0.6288812222769837,
      "grad_norm": 3.9059741497039795,
      "learning_rate": 1.7484869393790043e-05,
      "loss": 4.5436,
      "step": 6380
    },
    {
      "epoch": 0.629866929521932,
      "grad_norm": 4.2330803871154785,
      "learning_rate": 1.748092656481025e-05,
      "loss": 4.5561,
      "step": 6390
    },
    {
      "epoch": 0.6308526367668802,
      "grad_norm": 4.526771068572998,
      "learning_rate": 1.747698373583046e-05,
      "loss": 4.6401,
      "step": 6400
    },
    {
      "epoch": 0.6318383440118285,
      "grad_norm": 4.834403991699219,
      "learning_rate": 1.7473040906850668e-05,
      "loss": 4.5216,
      "step": 6410
    },
    {
      "epoch": 0.6328240512567768,
      "grad_norm": 4.698257923126221,
      "learning_rate": 1.7469098077870873e-05,
      "loss": 4.6916,
      "step": 6420
    },
    {
      "epoch": 0.6338097585017249,
      "grad_norm": 5.4723591804504395,
      "learning_rate": 1.746515524889108e-05,
      "loss": 4.5075,
      "step": 6430
    },
    {
      "epoch": 0.6347954657466732,
      "grad_norm": 4.095367908477783,
      "learning_rate": 1.746121241991129e-05,
      "loss": 4.57,
      "step": 6440
    },
    {
      "epoch": 0.6357811729916215,
      "grad_norm": 4.750513553619385,
      "learning_rate": 1.7457269590931494e-05,
      "loss": 4.4987,
      "step": 6450
    },
    {
      "epoch": 0.6367668802365697,
      "grad_norm": 4.084737300872803,
      "learning_rate": 1.7453326761951702e-05,
      "loss": 4.7069,
      "step": 6460
    },
    {
      "epoch": 0.637752587481518,
      "grad_norm": 4.090275287628174,
      "learning_rate": 1.7449383932971907e-05,
      "loss": 4.6076,
      "step": 6470
    },
    {
      "epoch": 0.6387382947264663,
      "grad_norm": 5.908038139343262,
      "learning_rate": 1.7445441103992115e-05,
      "loss": 4.7091,
      "step": 6480
    },
    {
      "epoch": 0.6397240019714144,
      "grad_norm": 3.798161506652832,
      "learning_rate": 1.7441498275012323e-05,
      "loss": 4.5793,
      "step": 6490
    },
    {
      "epoch": 0.6407097092163627,
      "grad_norm": 3.9494121074676514,
      "learning_rate": 1.743755544603253e-05,
      "loss": 4.4392,
      "step": 6500
    },
    {
      "epoch": 0.641695416461311,
      "grad_norm": 4.4295125007629395,
      "learning_rate": 1.7433612617052736e-05,
      "loss": 4.5572,
      "step": 6510
    },
    {
      "epoch": 0.6426811237062593,
      "grad_norm": 3.76711106300354,
      "learning_rate": 1.7429669788072944e-05,
      "loss": 4.4723,
      "step": 6520
    },
    {
      "epoch": 0.6436668309512075,
      "grad_norm": 4.229037284851074,
      "learning_rate": 1.742572695909315e-05,
      "loss": 4.7018,
      "step": 6530
    },
    {
      "epoch": 0.6446525381961558,
      "grad_norm": 4.200605869293213,
      "learning_rate": 1.7421784130113357e-05,
      "loss": 4.5817,
      "step": 6540
    },
    {
      "epoch": 0.645638245441104,
      "grad_norm": 4.652637958526611,
      "learning_rate": 1.7417841301133565e-05,
      "loss": 4.4989,
      "step": 6550
    },
    {
      "epoch": 0.6466239526860522,
      "grad_norm": 4.479511260986328,
      "learning_rate": 1.741389847215377e-05,
      "loss": 4.6203,
      "step": 6560
    },
    {
      "epoch": 0.6476096599310005,
      "grad_norm": 3.847964286804199,
      "learning_rate": 1.740995564317398e-05,
      "loss": 4.52,
      "step": 6570
    },
    {
      "epoch": 0.6485953671759488,
      "grad_norm": 4.884064674377441,
      "learning_rate": 1.7406012814194187e-05,
      "loss": 4.428,
      "step": 6580
    },
    {
      "epoch": 0.649581074420897,
      "grad_norm": 4.961979389190674,
      "learning_rate": 1.7402069985214395e-05,
      "loss": 4.4949,
      "step": 6590
    },
    {
      "epoch": 0.6505667816658453,
      "grad_norm": 5.070387363433838,
      "learning_rate": 1.73981271562346e-05,
      "loss": 4.6836,
      "step": 6600
    },
    {
      "epoch": 0.6515524889107935,
      "grad_norm": 3.562107563018799,
      "learning_rate": 1.7394184327254808e-05,
      "loss": 4.5689,
      "step": 6610
    },
    {
      "epoch": 0.6525381961557417,
      "grad_norm": 4.529172420501709,
      "learning_rate": 1.7390241498275012e-05,
      "loss": 4.5158,
      "step": 6620
    },
    {
      "epoch": 0.65352390340069,
      "grad_norm": 4.252549171447754,
      "learning_rate": 1.738629866929522e-05,
      "loss": 4.5431,
      "step": 6630
    },
    {
      "epoch": 0.6545096106456383,
      "grad_norm": 3.4688448905944824,
      "learning_rate": 1.738235584031543e-05,
      "loss": 4.6113,
      "step": 6640
    },
    {
      "epoch": 0.6554953178905865,
      "grad_norm": 4.880143165588379,
      "learning_rate": 1.7378413011335634e-05,
      "loss": 4.6048,
      "step": 6650
    },
    {
      "epoch": 0.6564810251355347,
      "grad_norm": 4.8560895919799805,
      "learning_rate": 1.7374470182355842e-05,
      "loss": 4.4661,
      "step": 6660
    },
    {
      "epoch": 0.657466732380483,
      "grad_norm": 4.2252349853515625,
      "learning_rate": 1.7370527353376047e-05,
      "loss": 4.4759,
      "step": 6670
    },
    {
      "epoch": 0.6584524396254312,
      "grad_norm": 3.608139753341675,
      "learning_rate": 1.7366584524396255e-05,
      "loss": 4.5831,
      "step": 6680
    },
    {
      "epoch": 0.6594381468703795,
      "grad_norm": 6.075026512145996,
      "learning_rate": 1.7362641695416463e-05,
      "loss": 4.5565,
      "step": 6690
    },
    {
      "epoch": 0.6604238541153278,
      "grad_norm": 3.5324673652648926,
      "learning_rate": 1.735869886643667e-05,
      "loss": 4.5838,
      "step": 6700
    },
    {
      "epoch": 0.661409561360276,
      "grad_norm": 4.924741268157959,
      "learning_rate": 1.7354756037456876e-05,
      "loss": 4.6594,
      "step": 6710
    },
    {
      "epoch": 0.6623952686052242,
      "grad_norm": 6.057603359222412,
      "learning_rate": 1.7350813208477084e-05,
      "loss": 4.5242,
      "step": 6720
    },
    {
      "epoch": 0.6633809758501725,
      "grad_norm": 5.294975757598877,
      "learning_rate": 1.7346870379497292e-05,
      "loss": 4.335,
      "step": 6730
    },
    {
      "epoch": 0.6643666830951207,
      "grad_norm": 4.0342302322387695,
      "learning_rate": 1.7342927550517497e-05,
      "loss": 4.6267,
      "step": 6740
    },
    {
      "epoch": 0.665352390340069,
      "grad_norm": 4.7935075759887695,
      "learning_rate": 1.7338984721537705e-05,
      "loss": 4.5524,
      "step": 6750
    },
    {
      "epoch": 0.6663380975850173,
      "grad_norm": 6.593470573425293,
      "learning_rate": 1.733504189255791e-05,
      "loss": 4.4919,
      "step": 6760
    },
    {
      "epoch": 0.6673238048299655,
      "grad_norm": 5.339974880218506,
      "learning_rate": 1.7331099063578118e-05,
      "loss": 4.6393,
      "step": 6770
    },
    {
      "epoch": 0.6683095120749137,
      "grad_norm": 3.6158483028411865,
      "learning_rate": 1.7327156234598326e-05,
      "loss": 4.6018,
      "step": 6780
    },
    {
      "epoch": 0.669295219319862,
      "grad_norm": 4.996316432952881,
      "learning_rate": 1.7323213405618535e-05,
      "loss": 4.4907,
      "step": 6790
    },
    {
      "epoch": 0.6702809265648102,
      "grad_norm": 4.631869792938232,
      "learning_rate": 1.731927057663874e-05,
      "loss": 4.5539,
      "step": 6800
    },
    {
      "epoch": 0.6712666338097585,
      "grad_norm": 4.43354606628418,
      "learning_rate": 1.7315327747658948e-05,
      "loss": 4.4745,
      "step": 6810
    },
    {
      "epoch": 0.6722523410547068,
      "grad_norm": 4.508332252502441,
      "learning_rate": 1.7311384918679152e-05,
      "loss": 4.475,
      "step": 6820
    },
    {
      "epoch": 0.6732380482996551,
      "grad_norm": 4.074427127838135,
      "learning_rate": 1.730744208969936e-05,
      "loss": 4.4312,
      "step": 6830
    },
    {
      "epoch": 0.6742237555446032,
      "grad_norm": 3.987015962600708,
      "learning_rate": 1.730349926071957e-05,
      "loss": 4.5596,
      "step": 6840
    },
    {
      "epoch": 0.6752094627895515,
      "grad_norm": 4.344300746917725,
      "learning_rate": 1.7299556431739774e-05,
      "loss": 4.7081,
      "step": 6850
    },
    {
      "epoch": 0.6761951700344998,
      "grad_norm": 4.563751220703125,
      "learning_rate": 1.7295613602759982e-05,
      "loss": 4.4884,
      "step": 6860
    },
    {
      "epoch": 0.677180877279448,
      "grad_norm": 5.587255001068115,
      "learning_rate": 1.729167077378019e-05,
      "loss": 4.5843,
      "step": 6870
    },
    {
      "epoch": 0.6781665845243963,
      "grad_norm": 4.271036624908447,
      "learning_rate": 1.7287727944800398e-05,
      "loss": 4.5468,
      "step": 6880
    },
    {
      "epoch": 0.6791522917693446,
      "grad_norm": 5.397618293762207,
      "learning_rate": 1.7283785115820603e-05,
      "loss": 4.5731,
      "step": 6890
    },
    {
      "epoch": 0.6801379990142927,
      "grad_norm": 4.660338401794434,
      "learning_rate": 1.7279842286840808e-05,
      "loss": 4.6078,
      "step": 6900
    },
    {
      "epoch": 0.681123706259241,
      "grad_norm": 3.8909451961517334,
      "learning_rate": 1.7275899457861016e-05,
      "loss": 4.6076,
      "step": 6910
    },
    {
      "epoch": 0.6821094135041893,
      "grad_norm": 4.4310197830200195,
      "learning_rate": 1.7271956628881224e-05,
      "loss": 4.5951,
      "step": 6920
    },
    {
      "epoch": 0.6830951207491375,
      "grad_norm": 4.22012186050415,
      "learning_rate": 1.7268013799901432e-05,
      "loss": 4.6238,
      "step": 6930
    },
    {
      "epoch": 0.6840808279940858,
      "grad_norm": 4.800903797149658,
      "learning_rate": 1.7264070970921637e-05,
      "loss": 4.4525,
      "step": 6940
    },
    {
      "epoch": 0.685066535239034,
      "grad_norm": 4.1669487953186035,
      "learning_rate": 1.7260128141941845e-05,
      "loss": 4.4826,
      "step": 6950
    },
    {
      "epoch": 0.6860522424839822,
      "grad_norm": 5.198056221008301,
      "learning_rate": 1.7256185312962053e-05,
      "loss": 4.4481,
      "step": 6960
    },
    {
      "epoch": 0.6870379497289305,
      "grad_norm": 3.9867260456085205,
      "learning_rate": 1.7252242483982258e-05,
      "loss": 4.5872,
      "step": 6970
    },
    {
      "epoch": 0.6880236569738788,
      "grad_norm": 4.863420486450195,
      "learning_rate": 1.7248299655002466e-05,
      "loss": 4.6434,
      "step": 6980
    },
    {
      "epoch": 0.689009364218827,
      "grad_norm": 4.1974592208862305,
      "learning_rate": 1.724435682602267e-05,
      "loss": 4.5221,
      "step": 6990
    },
    {
      "epoch": 0.6899950714637753,
      "grad_norm": 4.044414043426514,
      "learning_rate": 1.724041399704288e-05,
      "loss": 4.5464,
      "step": 7000
    },
    {
      "epoch": 0.6909807787087235,
      "grad_norm": 5.012508392333984,
      "learning_rate": 1.7236471168063088e-05,
      "loss": 4.6757,
      "step": 7010
    },
    {
      "epoch": 0.6919664859536717,
      "grad_norm": 4.780646800994873,
      "learning_rate": 1.7232528339083296e-05,
      "loss": 4.4522,
      "step": 7020
    },
    {
      "epoch": 0.69295219319862,
      "grad_norm": 4.560245513916016,
      "learning_rate": 1.72285855101035e-05,
      "loss": 4.6722,
      "step": 7030
    },
    {
      "epoch": 0.6939379004435683,
      "grad_norm": 4.852388858795166,
      "learning_rate": 1.7224642681123705e-05,
      "loss": 4.5336,
      "step": 7040
    },
    {
      "epoch": 0.6949236076885165,
      "grad_norm": 4.617619037628174,
      "learning_rate": 1.7220699852143913e-05,
      "loss": 4.5049,
      "step": 7050
    },
    {
      "epoch": 0.6959093149334648,
      "grad_norm": 4.454949378967285,
      "learning_rate": 1.721675702316412e-05,
      "loss": 4.5074,
      "step": 7060
    },
    {
      "epoch": 0.696895022178413,
      "grad_norm": 4.3477044105529785,
      "learning_rate": 1.721281419418433e-05,
      "loss": 4.4764,
      "step": 7070
    },
    {
      "epoch": 0.6978807294233612,
      "grad_norm": 4.086779594421387,
      "learning_rate": 1.7208871365204535e-05,
      "loss": 4.5845,
      "step": 7080
    },
    {
      "epoch": 0.6988664366683095,
      "grad_norm": 4.5640082359313965,
      "learning_rate": 1.7204928536224743e-05,
      "loss": 4.6699,
      "step": 7090
    },
    {
      "epoch": 0.6998521439132578,
      "grad_norm": 5.5287981033325195,
      "learning_rate": 1.720098570724495e-05,
      "loss": 4.4681,
      "step": 7100
    },
    {
      "epoch": 0.700837851158206,
      "grad_norm": 4.037373065948486,
      "learning_rate": 1.719704287826516e-05,
      "loss": 4.6445,
      "step": 7110
    },
    {
      "epoch": 0.7018235584031542,
      "grad_norm": 3.8234596252441406,
      "learning_rate": 1.7193100049285364e-05,
      "loss": 4.5999,
      "step": 7120
    },
    {
      "epoch": 0.7028092656481025,
      "grad_norm": 4.020842552185059,
      "learning_rate": 1.718915722030557e-05,
      "loss": 4.5665,
      "step": 7130
    },
    {
      "epoch": 0.7037949728930508,
      "grad_norm": 3.4288623332977295,
      "learning_rate": 1.7185214391325777e-05,
      "loss": 4.5781,
      "step": 7140
    },
    {
      "epoch": 0.704780680137999,
      "grad_norm": 4.547270774841309,
      "learning_rate": 1.7181271562345985e-05,
      "loss": 4.3564,
      "step": 7150
    },
    {
      "epoch": 0.7057663873829473,
      "grad_norm": 4.383988857269287,
      "learning_rate": 1.7177328733366193e-05,
      "loss": 4.561,
      "step": 7160
    },
    {
      "epoch": 0.7067520946278956,
      "grad_norm": 4.987183570861816,
      "learning_rate": 1.7173385904386398e-05,
      "loss": 4.5865,
      "step": 7170
    },
    {
      "epoch": 0.7077378018728437,
      "grad_norm": 4.548421382904053,
      "learning_rate": 1.7169837358304583e-05,
      "loss": 4.4965,
      "step": 7180
    },
    {
      "epoch": 0.708723509117792,
      "grad_norm": 4.563070297241211,
      "learning_rate": 1.716589452932479e-05,
      "loss": 4.6654,
      "step": 7190
    },
    {
      "epoch": 0.7097092163627403,
      "grad_norm": 4.682849884033203,
      "learning_rate": 1.7161951700345e-05,
      "loss": 4.5678,
      "step": 7200
    },
    {
      "epoch": 0.7106949236076885,
      "grad_norm": 4.426487922668457,
      "learning_rate": 1.7158008871365205e-05,
      "loss": 4.3978,
      "step": 7210
    },
    {
      "epoch": 0.7116806308526368,
      "grad_norm": 4.938043117523193,
      "learning_rate": 1.7154066042385413e-05,
      "loss": 4.6414,
      "step": 7220
    },
    {
      "epoch": 0.7126663380975851,
      "grad_norm": 5.273311138153076,
      "learning_rate": 1.715012321340562e-05,
      "loss": 4.376,
      "step": 7230
    },
    {
      "epoch": 0.7136520453425332,
      "grad_norm": 3.8490591049194336,
      "learning_rate": 1.714618038442583e-05,
      "loss": 4.696,
      "step": 7240
    },
    {
      "epoch": 0.7146377525874815,
      "grad_norm": 4.39672327041626,
      "learning_rate": 1.7142237555446034e-05,
      "loss": 4.5201,
      "step": 7250
    },
    {
      "epoch": 0.7156234598324298,
      "grad_norm": 5.346808910369873,
      "learning_rate": 1.713829472646624e-05,
      "loss": 4.4554,
      "step": 7260
    },
    {
      "epoch": 0.716609167077378,
      "grad_norm": 6.088338851928711,
      "learning_rate": 1.7134351897486447e-05,
      "loss": 4.5677,
      "step": 7270
    },
    {
      "epoch": 0.7175948743223263,
      "grad_norm": 4.196001052856445,
      "learning_rate": 1.7130409068506655e-05,
      "loss": 4.5288,
      "step": 7280
    },
    {
      "epoch": 0.7185805815672746,
      "grad_norm": 4.093610763549805,
      "learning_rate": 1.7126466239526863e-05,
      "loss": 4.4263,
      "step": 7290
    },
    {
      "epoch": 0.7195662888122227,
      "grad_norm": 3.393770694732666,
      "learning_rate": 1.7122523410547068e-05,
      "loss": 4.5194,
      "step": 7300
    },
    {
      "epoch": 0.720551996057171,
      "grad_norm": 5.565793514251709,
      "learning_rate": 1.7118580581567276e-05,
      "loss": 4.4607,
      "step": 7310
    },
    {
      "epoch": 0.7215377033021193,
      "grad_norm": 4.246342658996582,
      "learning_rate": 1.711463775258748e-05,
      "loss": 4.4583,
      "step": 7320
    },
    {
      "epoch": 0.7225234105470675,
      "grad_norm": 5.089433193206787,
      "learning_rate": 1.711069492360769e-05,
      "loss": 4.4926,
      "step": 7330
    },
    {
      "epoch": 0.7235091177920158,
      "grad_norm": 4.050384044647217,
      "learning_rate": 1.7106752094627897e-05,
      "loss": 4.4665,
      "step": 7340
    },
    {
      "epoch": 0.724494825036964,
      "grad_norm": 4.815736293792725,
      "learning_rate": 1.7102809265648102e-05,
      "loss": 4.5356,
      "step": 7350
    },
    {
      "epoch": 0.7254805322819122,
      "grad_norm": 4.609645366668701,
      "learning_rate": 1.709886643666831e-05,
      "loss": 4.5277,
      "step": 7360
    },
    {
      "epoch": 0.7264662395268605,
      "grad_norm": 3.58893084526062,
      "learning_rate": 1.709492360768852e-05,
      "loss": 4.4682,
      "step": 7370
    },
    {
      "epoch": 0.7274519467718088,
      "grad_norm": 3.702880859375,
      "learning_rate": 1.7090980778708727e-05,
      "loss": 4.6111,
      "step": 7380
    },
    {
      "epoch": 0.728437654016757,
      "grad_norm": 4.029083728790283,
      "learning_rate": 1.708703794972893e-05,
      "loss": 4.6858,
      "step": 7390
    },
    {
      "epoch": 0.7294233612617053,
      "grad_norm": 4.84650993347168,
      "learning_rate": 1.7083095120749136e-05,
      "loss": 4.6388,
      "step": 7400
    },
    {
      "epoch": 0.7304090685066535,
      "grad_norm": 4.122178554534912,
      "learning_rate": 1.7079152291769345e-05,
      "loss": 4.5836,
      "step": 7410
    },
    {
      "epoch": 0.7313947757516017,
      "grad_norm": 5.679856777191162,
      "learning_rate": 1.7075209462789553e-05,
      "loss": 4.5273,
      "step": 7420
    },
    {
      "epoch": 0.73238048299655,
      "grad_norm": 4.282087326049805,
      "learning_rate": 1.707126663380976e-05,
      "loss": 4.5114,
      "step": 7430
    },
    {
      "epoch": 0.7333661902414983,
      "grad_norm": 4.316591262817383,
      "learning_rate": 1.7067323804829966e-05,
      "loss": 4.5963,
      "step": 7440
    },
    {
      "epoch": 0.7343518974864465,
      "grad_norm": 3.988945960998535,
      "learning_rate": 1.7063380975850174e-05,
      "loss": 4.5444,
      "step": 7450
    },
    {
      "epoch": 0.7353376047313948,
      "grad_norm": 4.33768367767334,
      "learning_rate": 1.7059438146870382e-05,
      "loss": 4.5507,
      "step": 7460
    },
    {
      "epoch": 0.736323311976343,
      "grad_norm": 3.731109857559204,
      "learning_rate": 1.7055495317890587e-05,
      "loss": 4.3678,
      "step": 7470
    },
    {
      "epoch": 0.7373090192212913,
      "grad_norm": 3.6379735469818115,
      "learning_rate": 1.7051552488910795e-05,
      "loss": 4.6652,
      "step": 7480
    },
    {
      "epoch": 0.7382947264662395,
      "grad_norm": 4.024075508117676,
      "learning_rate": 1.7047609659931e-05,
      "loss": 4.5332,
      "step": 7490
    },
    {
      "epoch": 0.7392804337111878,
      "grad_norm": 4.627487659454346,
      "learning_rate": 1.7043666830951208e-05,
      "loss": 4.6877,
      "step": 7500
    },
    {
      "epoch": 0.7402661409561361,
      "grad_norm": 4.404199600219727,
      "learning_rate": 1.7039724001971416e-05,
      "loss": 4.5512,
      "step": 7510
    },
    {
      "epoch": 0.7412518482010843,
      "grad_norm": 4.126681327819824,
      "learning_rate": 1.7035781172991624e-05,
      "loss": 4.5415,
      "step": 7520
    },
    {
      "epoch": 0.7422375554460325,
      "grad_norm": 4.966951370239258,
      "learning_rate": 1.703183834401183e-05,
      "loss": 4.6142,
      "step": 7530
    },
    {
      "epoch": 0.7432232626909808,
      "grad_norm": 3.6505210399627686,
      "learning_rate": 1.7027895515032037e-05,
      "loss": 4.4373,
      "step": 7540
    },
    {
      "epoch": 0.744208969935929,
      "grad_norm": 5.464383602142334,
      "learning_rate": 1.7023952686052242e-05,
      "loss": 4.5506,
      "step": 7550
    },
    {
      "epoch": 0.7451946771808773,
      "grad_norm": 5.713836193084717,
      "learning_rate": 1.702000985707245e-05,
      "loss": 4.4585,
      "step": 7560
    },
    {
      "epoch": 0.7461803844258256,
      "grad_norm": 4.268540859222412,
      "learning_rate": 1.701606702809266e-05,
      "loss": 4.7622,
      "step": 7570
    },
    {
      "epoch": 0.7471660916707737,
      "grad_norm": 4.413543701171875,
      "learning_rate": 1.7012124199112863e-05,
      "loss": 4.409,
      "step": 7580
    },
    {
      "epoch": 0.748151798915722,
      "grad_norm": 3.85125732421875,
      "learning_rate": 1.700818137013307e-05,
      "loss": 4.6245,
      "step": 7590
    },
    {
      "epoch": 0.7491375061606703,
      "grad_norm": 5.014560222625732,
      "learning_rate": 1.700423854115328e-05,
      "loss": 4.482,
      "step": 7600
    },
    {
      "epoch": 0.7501232134056185,
      "grad_norm": 5.452763557434082,
      "learning_rate": 1.7000295712173488e-05,
      "loss": 4.4823,
      "step": 7610
    },
    {
      "epoch": 0.7511089206505668,
      "grad_norm": 4.534891605377197,
      "learning_rate": 1.6996352883193693e-05,
      "loss": 4.6016,
      "step": 7620
    },
    {
      "epoch": 0.7520946278955151,
      "grad_norm": 5.103949069976807,
      "learning_rate": 1.69924100542139e-05,
      "loss": 4.3765,
      "step": 7630
    },
    {
      "epoch": 0.7530803351404632,
      "grad_norm": 4.300394535064697,
      "learning_rate": 1.6988467225234106e-05,
      "loss": 4.5072,
      "step": 7640
    },
    {
      "epoch": 0.7540660423854115,
      "grad_norm": 5.296239852905273,
      "learning_rate": 1.6984524396254314e-05,
      "loss": 4.4298,
      "step": 7650
    },
    {
      "epoch": 0.7550517496303598,
      "grad_norm": 4.89744234085083,
      "learning_rate": 1.6980581567274522e-05,
      "loss": 4.4813,
      "step": 7660
    },
    {
      "epoch": 0.756037456875308,
      "grad_norm": 3.8016164302825928,
      "learning_rate": 1.6976638738294727e-05,
      "loss": 4.469,
      "step": 7670
    },
    {
      "epoch": 0.7570231641202563,
      "grad_norm": 4.123938083648682,
      "learning_rate": 1.6972695909314935e-05,
      "loss": 4.5295,
      "step": 7680
    },
    {
      "epoch": 0.7580088713652046,
      "grad_norm": 5.317567348480225,
      "learning_rate": 1.696875308033514e-05,
      "loss": 4.541,
      "step": 7690
    },
    {
      "epoch": 0.7589945786101527,
      "grad_norm": 5.226110458374023,
      "learning_rate": 1.6964810251355348e-05,
      "loss": 4.755,
      "step": 7700
    },
    {
      "epoch": 0.759980285855101,
      "grad_norm": 4.8416242599487305,
      "learning_rate": 1.6960867422375556e-05,
      "loss": 4.5628,
      "step": 7710
    },
    {
      "epoch": 0.7609659931000493,
      "grad_norm": 3.3188602924346924,
      "learning_rate": 1.6956924593395764e-05,
      "loss": 4.6083,
      "step": 7720
    },
    {
      "epoch": 0.7619517003449975,
      "grad_norm": 4.437368869781494,
      "learning_rate": 1.695298176441597e-05,
      "loss": 4.466,
      "step": 7730
    },
    {
      "epoch": 0.7629374075899458,
      "grad_norm": 4.322661876678467,
      "learning_rate": 1.6949038935436177e-05,
      "loss": 4.53,
      "step": 7740
    },
    {
      "epoch": 0.7639231148348941,
      "grad_norm": 4.164970874786377,
      "learning_rate": 1.6945096106456385e-05,
      "loss": 4.5895,
      "step": 7750
    },
    {
      "epoch": 0.7649088220798422,
      "grad_norm": 4.515751361846924,
      "learning_rate": 1.694115327747659e-05,
      "loss": 4.3776,
      "step": 7760
    },
    {
      "epoch": 0.7658945293247905,
      "grad_norm": 3.8075578212738037,
      "learning_rate": 1.69372104484968e-05,
      "loss": 4.4739,
      "step": 7770
    },
    {
      "epoch": 0.7668802365697388,
      "grad_norm": 4.435269832611084,
      "learning_rate": 1.6933267619517003e-05,
      "loss": 4.3018,
      "step": 7780
    },
    {
      "epoch": 0.7678659438146871,
      "grad_norm": 4.12326192855835,
      "learning_rate": 1.692932479053721e-05,
      "loss": 4.3091,
      "step": 7790
    },
    {
      "epoch": 0.7688516510596353,
      "grad_norm": 3.762413740158081,
      "learning_rate": 1.692538196155742e-05,
      "loss": 4.5194,
      "step": 7800
    },
    {
      "epoch": 0.7698373583045836,
      "grad_norm": 3.7134034633636475,
      "learning_rate": 1.6921439132577628e-05,
      "loss": 4.5841,
      "step": 7810
    },
    {
      "epoch": 0.7708230655495318,
      "grad_norm": 3.5518434047698975,
      "learning_rate": 1.6917496303597833e-05,
      "loss": 4.5436,
      "step": 7820
    },
    {
      "epoch": 0.77180877279448,
      "grad_norm": 4.247130393981934,
      "learning_rate": 1.691355347461804e-05,
      "loss": 4.518,
      "step": 7830
    },
    {
      "epoch": 0.7727944800394283,
      "grad_norm": 3.4161534309387207,
      "learning_rate": 1.6909610645638245e-05,
      "loss": 4.4773,
      "step": 7840
    },
    {
      "epoch": 0.7737801872843766,
      "grad_norm": 3.734706401824951,
      "learning_rate": 1.6905667816658454e-05,
      "loss": 4.4321,
      "step": 7850
    },
    {
      "epoch": 0.7747658945293248,
      "grad_norm": 5.095702171325684,
      "learning_rate": 1.6901724987678662e-05,
      "loss": 4.4578,
      "step": 7860
    },
    {
      "epoch": 0.775751601774273,
      "grad_norm": 3.8463313579559326,
      "learning_rate": 1.6897782158698867e-05,
      "loss": 4.6323,
      "step": 7870
    },
    {
      "epoch": 0.7767373090192213,
      "grad_norm": 3.8926918506622314,
      "learning_rate": 1.6893839329719075e-05,
      "loss": 4.3757,
      "step": 7880
    },
    {
      "epoch": 0.7777230162641695,
      "grad_norm": 4.597101211547852,
      "learning_rate": 1.6889896500739283e-05,
      "loss": 4.4006,
      "step": 7890
    },
    {
      "epoch": 0.7787087235091178,
      "grad_norm": 5.069626331329346,
      "learning_rate": 1.688595367175949e-05,
      "loss": 4.6114,
      "step": 7900
    },
    {
      "epoch": 0.7796944307540661,
      "grad_norm": 4.555321216583252,
      "learning_rate": 1.6882010842779696e-05,
      "loss": 4.3815,
      "step": 7910
    },
    {
      "epoch": 0.7806801379990143,
      "grad_norm": 4.243745803833008,
      "learning_rate": 1.68780680137999e-05,
      "loss": 4.4597,
      "step": 7920
    },
    {
      "epoch": 0.7816658452439625,
      "grad_norm": 4.429755687713623,
      "learning_rate": 1.687412518482011e-05,
      "loss": 4.4306,
      "step": 7930
    },
    {
      "epoch": 0.7826515524889108,
      "grad_norm": 3.9142558574676514,
      "learning_rate": 1.6870182355840317e-05,
      "loss": 4.4143,
      "step": 7940
    },
    {
      "epoch": 0.783637259733859,
      "grad_norm": 3.8608779907226562,
      "learning_rate": 1.6866239526860525e-05,
      "loss": 4.5366,
      "step": 7950
    },
    {
      "epoch": 0.7846229669788073,
      "grad_norm": 4.875969409942627,
      "learning_rate": 1.686229669788073e-05,
      "loss": 4.4669,
      "step": 7960
    },
    {
      "epoch": 0.7856086742237556,
      "grad_norm": 4.949875831604004,
      "learning_rate": 1.685835386890094e-05,
      "loss": 4.5425,
      "step": 7970
    },
    {
      "epoch": 0.7865943814687038,
      "grad_norm": 4.905105113983154,
      "learning_rate": 1.6854411039921146e-05,
      "loss": 4.4162,
      "step": 7980
    },
    {
      "epoch": 0.787580088713652,
      "grad_norm": 4.1795759201049805,
      "learning_rate": 1.685046821094135e-05,
      "loss": 4.5184,
      "step": 7990
    },
    {
      "epoch": 0.7885657959586003,
      "grad_norm": 4.166109561920166,
      "learning_rate": 1.684652538196156e-05,
      "loss": 4.5131,
      "step": 8000
    },
    {
      "epoch": 0.7895515032035485,
      "grad_norm": 4.308537006378174,
      "learning_rate": 1.6842582552981764e-05,
      "loss": 4.318,
      "step": 8010
    },
    {
      "epoch": 0.7905372104484968,
      "grad_norm": 4.547513484954834,
      "learning_rate": 1.6838639724001972e-05,
      "loss": 4.4807,
      "step": 8020
    },
    {
      "epoch": 0.7915229176934451,
      "grad_norm": 5.13068151473999,
      "learning_rate": 1.683469689502218e-05,
      "loss": 4.4409,
      "step": 8030
    },
    {
      "epoch": 0.7925086249383932,
      "grad_norm": 5.451699256896973,
      "learning_rate": 1.683075406604239e-05,
      "loss": 4.5005,
      "step": 8040
    },
    {
      "epoch": 0.7934943321833415,
      "grad_norm": 5.863656997680664,
      "learning_rate": 1.6826811237062594e-05,
      "loss": 4.4718,
      "step": 8050
    },
    {
      "epoch": 0.7944800394282898,
      "grad_norm": 4.0416259765625,
      "learning_rate": 1.68228684080828e-05,
      "loss": 4.3974,
      "step": 8060
    },
    {
      "epoch": 0.795465746673238,
      "grad_norm": 3.7725584506988525,
      "learning_rate": 1.6818925579103007e-05,
      "loss": 4.643,
      "step": 8070
    },
    {
      "epoch": 0.7964514539181863,
      "grad_norm": 4.5384063720703125,
      "learning_rate": 1.6814982750123215e-05,
      "loss": 4.4476,
      "step": 8080
    },
    {
      "epoch": 0.7974371611631346,
      "grad_norm": 5.017953395843506,
      "learning_rate": 1.6811039921143423e-05,
      "loss": 4.3762,
      "step": 8090
    },
    {
      "epoch": 0.7984228684080829,
      "grad_norm": 3.619776725769043,
      "learning_rate": 1.6807097092163628e-05,
      "loss": 4.5503,
      "step": 8100
    },
    {
      "epoch": 0.799408575653031,
      "grad_norm": 4.115339279174805,
      "learning_rate": 1.6803154263183836e-05,
      "loss": 4.4641,
      "step": 8110
    },
    {
      "epoch": 0.8003942828979793,
      "grad_norm": 5.026993274688721,
      "learning_rate": 1.6799211434204044e-05,
      "loss": 4.4358,
      "step": 8120
    },
    {
      "epoch": 0.8013799901429276,
      "grad_norm": 4.039222240447998,
      "learning_rate": 1.6795268605224252e-05,
      "loss": 4.4398,
      "step": 8130
    },
    {
      "epoch": 0.8023656973878758,
      "grad_norm": 5.27006721496582,
      "learning_rate": 1.6791325776244457e-05,
      "loss": 4.583,
      "step": 8140
    },
    {
      "epoch": 0.8033514046328241,
      "grad_norm": 4.307122230529785,
      "learning_rate": 1.6787382947264662e-05,
      "loss": 4.4771,
      "step": 8150
    },
    {
      "epoch": 0.8043371118777723,
      "grad_norm": 4.1506428718566895,
      "learning_rate": 1.678344011828487e-05,
      "loss": 4.5088,
      "step": 8160
    },
    {
      "epoch": 0.8053228191227205,
      "grad_norm": 4.508208751678467,
      "learning_rate": 1.6779497289305078e-05,
      "loss": 4.5503,
      "step": 8170
    },
    {
      "epoch": 0.8063085263676688,
      "grad_norm": 3.64788818359375,
      "learning_rate": 1.6775554460325286e-05,
      "loss": 4.5716,
      "step": 8180
    },
    {
      "epoch": 0.8072942336126171,
      "grad_norm": 5.083389759063721,
      "learning_rate": 1.677161163134549e-05,
      "loss": 4.3802,
      "step": 8190
    },
    {
      "epoch": 0.8082799408575653,
      "grad_norm": 4.393739223480225,
      "learning_rate": 1.67676688023657e-05,
      "loss": 4.5268,
      "step": 8200
    },
    {
      "epoch": 0.8092656481025136,
      "grad_norm": 4.128270149230957,
      "learning_rate": 1.6763725973385904e-05,
      "loss": 4.4612,
      "step": 8210
    },
    {
      "epoch": 0.8102513553474618,
      "grad_norm": 5.655118942260742,
      "learning_rate": 1.6759783144406112e-05,
      "loss": 4.3489,
      "step": 8220
    },
    {
      "epoch": 0.81123706259241,
      "grad_norm": 4.2808661460876465,
      "learning_rate": 1.675584031542632e-05,
      "loss": 4.6018,
      "step": 8230
    },
    {
      "epoch": 0.8122227698373583,
      "grad_norm": 4.533838748931885,
      "learning_rate": 1.6751897486446525e-05,
      "loss": 4.5606,
      "step": 8240
    },
    {
      "epoch": 0.8132084770823066,
      "grad_norm": 4.458531856536865,
      "learning_rate": 1.6747954657466734e-05,
      "loss": 4.5081,
      "step": 8250
    },
    {
      "epoch": 0.8141941843272548,
      "grad_norm": 4.176660060882568,
      "learning_rate": 1.6744011828486942e-05,
      "loss": 4.4801,
      "step": 8260
    },
    {
      "epoch": 0.815179891572203,
      "grad_norm": 4.540898323059082,
      "learning_rate": 1.674006899950715e-05,
      "loss": 4.5553,
      "step": 8270
    },
    {
      "epoch": 0.8161655988171513,
      "grad_norm": 3.6692440509796143,
      "learning_rate": 1.6736126170527355e-05,
      "loss": 4.5222,
      "step": 8280
    },
    {
      "epoch": 0.8171513060620995,
      "grad_norm": 5.033674240112305,
      "learning_rate": 1.673218334154756e-05,
      "loss": 4.3916,
      "step": 8290
    },
    {
      "epoch": 0.8181370133070478,
      "grad_norm": 4.570968151092529,
      "learning_rate": 1.6728240512567768e-05,
      "loss": 4.4006,
      "step": 8300
    },
    {
      "epoch": 0.8191227205519961,
      "grad_norm": 4.305530071258545,
      "learning_rate": 1.6724297683587976e-05,
      "loss": 4.5773,
      "step": 8310
    },
    {
      "epoch": 0.8201084277969443,
      "grad_norm": 4.135795593261719,
      "learning_rate": 1.6720354854608184e-05,
      "loss": 4.6301,
      "step": 8320
    },
    {
      "epoch": 0.8210941350418925,
      "grad_norm": 4.206915855407715,
      "learning_rate": 1.671641202562839e-05,
      "loss": 4.4498,
      "step": 8330
    },
    {
      "epoch": 0.8220798422868408,
      "grad_norm": 4.502058982849121,
      "learning_rate": 1.6712469196648597e-05,
      "loss": 4.4029,
      "step": 8340
    },
    {
      "epoch": 0.823065549531789,
      "grad_norm": 4.21694803237915,
      "learning_rate": 1.6708526367668805e-05,
      "loss": 4.5331,
      "step": 8350
    },
    {
      "epoch": 0.8240512567767373,
      "grad_norm": 3.464580774307251,
      "learning_rate": 1.670458353868901e-05,
      "loss": 4.3955,
      "step": 8360
    },
    {
      "epoch": 0.8250369640216856,
      "grad_norm": 4.365930557250977,
      "learning_rate": 1.6700640709709218e-05,
      "loss": 4.4429,
      "step": 8370
    },
    {
      "epoch": 0.8260226712666338,
      "grad_norm": 4.160543441772461,
      "learning_rate": 1.6696697880729423e-05,
      "loss": 4.4554,
      "step": 8380
    },
    {
      "epoch": 0.827008378511582,
      "grad_norm": 5.011847019195557,
      "learning_rate": 1.669275505174963e-05,
      "loss": 4.4258,
      "step": 8390
    },
    {
      "epoch": 0.8279940857565303,
      "grad_norm": 4.5906982421875,
      "learning_rate": 1.668881222276984e-05,
      "loss": 4.4346,
      "step": 8400
    },
    {
      "epoch": 0.8289797930014786,
      "grad_norm": 5.171726226806641,
      "learning_rate": 1.6684869393790047e-05,
      "loss": 4.4892,
      "step": 8410
    },
    {
      "epoch": 0.8299655002464268,
      "grad_norm": 4.575148582458496,
      "learning_rate": 1.6680926564810252e-05,
      "loss": 4.4112,
      "step": 8420
    },
    {
      "epoch": 0.8309512074913751,
      "grad_norm": 4.133979797363281,
      "learning_rate": 1.6676983735830457e-05,
      "loss": 4.4828,
      "step": 8430
    },
    {
      "epoch": 0.8319369147363234,
      "grad_norm": 3.9941585063934326,
      "learning_rate": 1.6673040906850665e-05,
      "loss": 4.5203,
      "step": 8440
    },
    {
      "epoch": 0.8329226219812715,
      "grad_norm": 4.582649230957031,
      "learning_rate": 1.6669098077870873e-05,
      "loss": 4.5345,
      "step": 8450
    },
    {
      "epoch": 0.8339083292262198,
      "grad_norm": 4.730890274047852,
      "learning_rate": 1.666515524889108e-05,
      "loss": 4.5684,
      "step": 8460
    },
    {
      "epoch": 0.8348940364711681,
      "grad_norm": 5.502623081207275,
      "learning_rate": 1.6661212419911286e-05,
      "loss": 4.4047,
      "step": 8470
    },
    {
      "epoch": 0.8358797437161163,
      "grad_norm": 5.03304386138916,
      "learning_rate": 1.6657269590931495e-05,
      "loss": 4.3663,
      "step": 8480
    },
    {
      "epoch": 0.8368654509610646,
      "grad_norm": 4.307651042938232,
      "learning_rate": 1.6653326761951703e-05,
      "loss": 4.5782,
      "step": 8490
    },
    {
      "epoch": 0.8378511582060129,
      "grad_norm": 3.4071977138519287,
      "learning_rate": 1.664938393297191e-05,
      "loss": 4.3474,
      "step": 8500
    },
    {
      "epoch": 0.838836865450961,
      "grad_norm": 4.289849281311035,
      "learning_rate": 1.6645441103992116e-05,
      "loss": 4.3994,
      "step": 8510
    },
    {
      "epoch": 0.8398225726959093,
      "grad_norm": 4.135201930999756,
      "learning_rate": 1.664149827501232e-05,
      "loss": 4.3633,
      "step": 8520
    },
    {
      "epoch": 0.8408082799408576,
      "grad_norm": 4.5986104011535645,
      "learning_rate": 1.663755544603253e-05,
      "loss": 4.5159,
      "step": 8530
    },
    {
      "epoch": 0.8417939871858058,
      "grad_norm": 5.425565242767334,
      "learning_rate": 1.6633612617052737e-05,
      "loss": 4.2665,
      "step": 8540
    },
    {
      "epoch": 0.8427796944307541,
      "grad_norm": 4.070878505706787,
      "learning_rate": 1.6629669788072945e-05,
      "loss": 4.3099,
      "step": 8550
    },
    {
      "epoch": 0.8437654016757024,
      "grad_norm": 4.5605902671813965,
      "learning_rate": 1.662572695909315e-05,
      "loss": 4.3814,
      "step": 8560
    },
    {
      "epoch": 0.8447511089206505,
      "grad_norm": 3.7329463958740234,
      "learning_rate": 1.6621784130113358e-05,
      "loss": 4.2823,
      "step": 8570
    },
    {
      "epoch": 0.8457368161655988,
      "grad_norm": 4.34826135635376,
      "learning_rate": 1.6617841301133563e-05,
      "loss": 4.4436,
      "step": 8580
    },
    {
      "epoch": 0.8467225234105471,
      "grad_norm": 6.476024150848389,
      "learning_rate": 1.661389847215377e-05,
      "loss": 4.2856,
      "step": 8590
    },
    {
      "epoch": 0.8477082306554953,
      "grad_norm": 5.137244701385498,
      "learning_rate": 1.660995564317398e-05,
      "loss": 4.3895,
      "step": 8600
    },
    {
      "epoch": 0.8486939379004436,
      "grad_norm": 3.9476189613342285,
      "learning_rate": 1.6606012814194184e-05,
      "loss": 4.3876,
      "step": 8610
    },
    {
      "epoch": 0.8496796451453918,
      "grad_norm": 4.382721424102783,
      "learning_rate": 1.6602069985214392e-05,
      "loss": 4.4864,
      "step": 8620
    },
    {
      "epoch": 0.85066535239034,
      "grad_norm": 4.224106788635254,
      "learning_rate": 1.65981271562346e-05,
      "loss": 4.3611,
      "step": 8630
    },
    {
      "epoch": 0.8516510596352883,
      "grad_norm": 3.442864418029785,
      "learning_rate": 1.659418432725481e-05,
      "loss": 4.5269,
      "step": 8640
    },
    {
      "epoch": 0.8526367668802366,
      "grad_norm": 4.350409507751465,
      "learning_rate": 1.6590241498275013e-05,
      "loss": 4.5232,
      "step": 8650
    },
    {
      "epoch": 0.8536224741251848,
      "grad_norm": 4.840267658233643,
      "learning_rate": 1.658629866929522e-05,
      "loss": 4.5158,
      "step": 8660
    },
    {
      "epoch": 0.8546081813701331,
      "grad_norm": 4.791681289672852,
      "learning_rate": 1.6582355840315426e-05,
      "loss": 4.4621,
      "step": 8670
    },
    {
      "epoch": 0.8555938886150813,
      "grad_norm": 4.017481327056885,
      "learning_rate": 1.6578413011335635e-05,
      "loss": 4.3665,
      "step": 8680
    },
    {
      "epoch": 0.8565795958600295,
      "grad_norm": 4.5007243156433105,
      "learning_rate": 1.6574470182355843e-05,
      "loss": 4.5478,
      "step": 8690
    },
    {
      "epoch": 0.8575653031049778,
      "grad_norm": 4.61100959777832,
      "learning_rate": 1.6570527353376047e-05,
      "loss": 4.3168,
      "step": 8700
    },
    {
      "epoch": 0.8585510103499261,
      "grad_norm": 5.610187530517578,
      "learning_rate": 1.6566584524396256e-05,
      "loss": 4.3572,
      "step": 8710
    },
    {
      "epoch": 0.8595367175948744,
      "grad_norm": 4.670156478881836,
      "learning_rate": 1.6562641695416464e-05,
      "loss": 4.4345,
      "step": 8720
    },
    {
      "epoch": 0.8605224248398226,
      "grad_norm": 4.180234909057617,
      "learning_rate": 1.655869886643667e-05,
      "loss": 4.5084,
      "step": 8730
    },
    {
      "epoch": 0.8615081320847708,
      "grad_norm": 4.530508995056152,
      "learning_rate": 1.6554756037456877e-05,
      "loss": 4.5033,
      "step": 8740
    },
    {
      "epoch": 0.8624938393297191,
      "grad_norm": 4.496294021606445,
      "learning_rate": 1.6550813208477085e-05,
      "loss": 4.484,
      "step": 8750
    },
    {
      "epoch": 0.8634795465746673,
      "grad_norm": 4.904247760772705,
      "learning_rate": 1.654687037949729e-05,
      "loss": 4.3027,
      "step": 8760
    },
    {
      "epoch": 0.8644652538196156,
      "grad_norm": 6.283519744873047,
      "learning_rate": 1.6542927550517498e-05,
      "loss": 4.4916,
      "step": 8770
    },
    {
      "epoch": 0.8654509610645639,
      "grad_norm": 5.306303977966309,
      "learning_rate": 1.6538984721537706e-05,
      "loss": 4.5564,
      "step": 8780
    },
    {
      "epoch": 0.866436668309512,
      "grad_norm": 3.8196029663085938,
      "learning_rate": 1.653504189255791e-05,
      "loss": 4.3896,
      "step": 8790
    },
    {
      "epoch": 0.8674223755544603,
      "grad_norm": 3.669806718826294,
      "learning_rate": 1.653109906357812e-05,
      "loss": 4.385,
      "step": 8800
    },
    {
      "epoch": 0.8684080827994086,
      "grad_norm": 4.673031330108643,
      "learning_rate": 1.6527156234598324e-05,
      "loss": 4.4928,
      "step": 8810
    },
    {
      "epoch": 0.8693937900443568,
      "grad_norm": 4.794321537017822,
      "learning_rate": 1.6523213405618532e-05,
      "loss": 4.4205,
      "step": 8820
    },
    {
      "epoch": 0.8703794972893051,
      "grad_norm": 3.8281748294830322,
      "learning_rate": 1.651927057663874e-05,
      "loss": 4.4077,
      "step": 8830
    },
    {
      "epoch": 0.8713652045342534,
      "grad_norm": 3.9069533348083496,
      "learning_rate": 1.651532774765895e-05,
      "loss": 4.4446,
      "step": 8840
    },
    {
      "epoch": 0.8723509117792015,
      "grad_norm": 3.637928009033203,
      "learning_rate": 1.6511384918679153e-05,
      "loss": 4.494,
      "step": 8850
    },
    {
      "epoch": 0.8733366190241498,
      "grad_norm": 4.670320987701416,
      "learning_rate": 1.650744208969936e-05,
      "loss": 4.3361,
      "step": 8860
    },
    {
      "epoch": 0.8743223262690981,
      "grad_norm": 4.9050188064575195,
      "learning_rate": 1.6503499260719566e-05,
      "loss": 4.3194,
      "step": 8870
    },
    {
      "epoch": 0.8753080335140463,
      "grad_norm": 4.2055253982543945,
      "learning_rate": 1.6499556431739774e-05,
      "loss": 4.6876,
      "step": 8880
    },
    {
      "epoch": 0.8762937407589946,
      "grad_norm": 4.820836544036865,
      "learning_rate": 1.6495613602759983e-05,
      "loss": 4.3828,
      "step": 8890
    },
    {
      "epoch": 0.8772794480039429,
      "grad_norm": 4.088197708129883,
      "learning_rate": 1.6491670773780187e-05,
      "loss": 4.344,
      "step": 8900
    },
    {
      "epoch": 0.878265155248891,
      "grad_norm": 4.0759477615356445,
      "learning_rate": 1.6487727944800396e-05,
      "loss": 4.5242,
      "step": 8910
    },
    {
      "epoch": 0.8792508624938393,
      "grad_norm": 4.101762771606445,
      "learning_rate": 1.6483785115820604e-05,
      "loss": 4.3554,
      "step": 8920
    },
    {
      "epoch": 0.8802365697387876,
      "grad_norm": 4.6070780754089355,
      "learning_rate": 1.6479842286840812e-05,
      "loss": 4.2759,
      "step": 8930
    },
    {
      "epoch": 0.8812222769837358,
      "grad_norm": 4.003029823303223,
      "learning_rate": 1.6475899457861017e-05,
      "loss": 4.5516,
      "step": 8940
    },
    {
      "epoch": 0.8822079842286841,
      "grad_norm": 5.822605609893799,
      "learning_rate": 1.647195662888122e-05,
      "loss": 4.4761,
      "step": 8950
    },
    {
      "epoch": 0.8831936914736324,
      "grad_norm": 4.431909084320068,
      "learning_rate": 1.646801379990143e-05,
      "loss": 4.4221,
      "step": 8960
    },
    {
      "epoch": 0.8841793987185805,
      "grad_norm": 4.88311767578125,
      "learning_rate": 1.6464070970921638e-05,
      "loss": 4.3997,
      "step": 8970
    },
    {
      "epoch": 0.8851651059635288,
      "grad_norm": 4.0705037117004395,
      "learning_rate": 1.6460128141941846e-05,
      "loss": 4.3768,
      "step": 8980
    },
    {
      "epoch": 0.8861508132084771,
      "grad_norm": 5.6986260414123535,
      "learning_rate": 1.645618531296205e-05,
      "loss": 4.4778,
      "step": 8990
    },
    {
      "epoch": 0.8871365204534253,
      "grad_norm": 4.538078308105469,
      "learning_rate": 1.645224248398226e-05,
      "loss": 4.3843,
      "step": 9000
    },
    {
      "epoch": 0.8881222276983736,
      "grad_norm": 5.251614093780518,
      "learning_rate": 1.6448299655002467e-05,
      "loss": 4.3865,
      "step": 9010
    },
    {
      "epoch": 0.8891079349433219,
      "grad_norm": 5.305327892303467,
      "learning_rate": 1.6444356826022672e-05,
      "loss": 4.473,
      "step": 9020
    },
    {
      "epoch": 0.89009364218827,
      "grad_norm": 4.955812454223633,
      "learning_rate": 1.644041399704288e-05,
      "loss": 4.434,
      "step": 9030
    },
    {
      "epoch": 0.8910793494332183,
      "grad_norm": 4.138523578643799,
      "learning_rate": 1.6436471168063085e-05,
      "loss": 4.3838,
      "step": 9040
    },
    {
      "epoch": 0.8920650566781666,
      "grad_norm": 4.723174571990967,
      "learning_rate": 1.6432528339083293e-05,
      "loss": 4.3598,
      "step": 9050
    },
    {
      "epoch": 0.8930507639231149,
      "grad_norm": 4.532871723175049,
      "learning_rate": 1.64285855101035e-05,
      "loss": 4.4333,
      "step": 9060
    },
    {
      "epoch": 0.8940364711680631,
      "grad_norm": 5.11651086807251,
      "learning_rate": 1.642464268112371e-05,
      "loss": 4.3327,
      "step": 9070
    },
    {
      "epoch": 0.8950221784130113,
      "grad_norm": 4.574418067932129,
      "learning_rate": 1.6420699852143914e-05,
      "loss": 4.4421,
      "step": 9080
    },
    {
      "epoch": 0.8960078856579596,
      "grad_norm": 5.042479515075684,
      "learning_rate": 1.6416757023164123e-05,
      "loss": 4.4781,
      "step": 9090
    },
    {
      "epoch": 0.8969935929029078,
      "grad_norm": 4.396965026855469,
      "learning_rate": 1.6412814194184327e-05,
      "loss": 4.3942,
      "step": 9100
    },
    {
      "epoch": 0.8979793001478561,
      "grad_norm": 4.446270942687988,
      "learning_rate": 1.6408871365204536e-05,
      "loss": 4.2754,
      "step": 9110
    },
    {
      "epoch": 0.8989650073928044,
      "grad_norm": 3.8587565422058105,
      "learning_rate": 1.6404928536224744e-05,
      "loss": 4.3897,
      "step": 9120
    },
    {
      "epoch": 0.8999507146377526,
      "grad_norm": 4.700434684753418,
      "learning_rate": 1.640098570724495e-05,
      "loss": 4.5252,
      "step": 9130
    },
    {
      "epoch": 0.9009364218827008,
      "grad_norm": 4.805492877960205,
      "learning_rate": 1.6397042878265157e-05,
      "loss": 4.3265,
      "step": 9140
    },
    {
      "epoch": 0.9019221291276491,
      "grad_norm": 4.70494270324707,
      "learning_rate": 1.6393100049285365e-05,
      "loss": 4.4482,
      "step": 9150
    },
    {
      "epoch": 0.9029078363725973,
      "grad_norm": 4.759113788604736,
      "learning_rate": 1.6389157220305573e-05,
      "loss": 4.2478,
      "step": 9160
    },
    {
      "epoch": 0.9038935436175456,
      "grad_norm": 4.402041912078857,
      "learning_rate": 1.6385214391325778e-05,
      "loss": 4.3817,
      "step": 9170
    },
    {
      "epoch": 0.9048792508624939,
      "grad_norm": 3.500760793685913,
      "learning_rate": 1.6381271562345983e-05,
      "loss": 4.4956,
      "step": 9180
    },
    {
      "epoch": 0.905864958107442,
      "grad_norm": 4.2578606605529785,
      "learning_rate": 1.637732873336619e-05,
      "loss": 4.3933,
      "step": 9190
    },
    {
      "epoch": 0.9068506653523903,
      "grad_norm": 5.471297740936279,
      "learning_rate": 1.63733859043864e-05,
      "loss": 4.312,
      "step": 9200
    },
    {
      "epoch": 0.9078363725973386,
      "grad_norm": 4.232886791229248,
      "learning_rate": 1.6369443075406607e-05,
      "loss": 4.5389,
      "step": 9210
    },
    {
      "epoch": 0.9088220798422868,
      "grad_norm": 3.476999282836914,
      "learning_rate": 1.6365500246426812e-05,
      "loss": 4.4503,
      "step": 9220
    },
    {
      "epoch": 0.9098077870872351,
      "grad_norm": 4.391975402832031,
      "learning_rate": 1.636155741744702e-05,
      "loss": 4.4622,
      "step": 9230
    },
    {
      "epoch": 0.9107934943321834,
      "grad_norm": 4.102422714233398,
      "learning_rate": 1.6357614588467225e-05,
      "loss": 4.398,
      "step": 9240
    },
    {
      "epoch": 0.9117792015771315,
      "grad_norm": 4.066939353942871,
      "learning_rate": 1.6353671759487433e-05,
      "loss": 4.4331,
      "step": 9250
    },
    {
      "epoch": 0.9127649088220798,
      "grad_norm": 5.3320231437683105,
      "learning_rate": 1.634972893050764e-05,
      "loss": 4.3386,
      "step": 9260
    },
    {
      "epoch": 0.9137506160670281,
      "grad_norm": 3.841113328933716,
      "learning_rate": 1.6345786101527846e-05,
      "loss": 4.3128,
      "step": 9270
    },
    {
      "epoch": 0.9147363233119763,
      "grad_norm": 5.005311489105225,
      "learning_rate": 1.6341843272548054e-05,
      "loss": 4.4623,
      "step": 9280
    },
    {
      "epoch": 0.9157220305569246,
      "grad_norm": 3.921928644180298,
      "learning_rate": 1.6337900443568262e-05,
      "loss": 4.4169,
      "step": 9290
    },
    {
      "epoch": 0.9167077378018729,
      "grad_norm": 4.492448806762695,
      "learning_rate": 1.633395761458847e-05,
      "loss": 4.3918,
      "step": 9300
    },
    {
      "epoch": 0.917693445046821,
      "grad_norm": 4.530555725097656,
      "learning_rate": 1.6330014785608675e-05,
      "loss": 4.483,
      "step": 9310
    },
    {
      "epoch": 0.9186791522917693,
      "grad_norm": 4.694797039031982,
      "learning_rate": 1.632607195662888e-05,
      "loss": 4.546,
      "step": 9320
    },
    {
      "epoch": 0.9196648595367176,
      "grad_norm": 4.031548023223877,
      "learning_rate": 1.632212912764909e-05,
      "loss": 4.2957,
      "step": 9330
    },
    {
      "epoch": 0.9206505667816658,
      "grad_norm": 3.9459009170532227,
      "learning_rate": 1.6318186298669297e-05,
      "loss": 4.2323,
      "step": 9340
    },
    {
      "epoch": 0.9216362740266141,
      "grad_norm": 4.820214748382568,
      "learning_rate": 1.6314243469689505e-05,
      "loss": 4.4234,
      "step": 9350
    },
    {
      "epoch": 0.9226219812715624,
      "grad_norm": 3.7837729454040527,
      "learning_rate": 1.631030064070971e-05,
      "loss": 4.4494,
      "step": 9360
    },
    {
      "epoch": 0.9236076885165106,
      "grad_norm": 4.439566135406494,
      "learning_rate": 1.6306357811729918e-05,
      "loss": 4.5058,
      "step": 9370
    },
    {
      "epoch": 0.9245933957614588,
      "grad_norm": 6.628385066986084,
      "learning_rate": 1.6302414982750126e-05,
      "loss": 4.2984,
      "step": 9380
    },
    {
      "epoch": 0.9255791030064071,
      "grad_norm": 4.673073768615723,
      "learning_rate": 1.629847215377033e-05,
      "loss": 4.2059,
      "step": 9390
    },
    {
      "epoch": 0.9265648102513554,
      "grad_norm": 4.110708713531494,
      "learning_rate": 1.629452932479054e-05,
      "loss": 4.3062,
      "step": 9400
    },
    {
      "epoch": 0.9275505174963036,
      "grad_norm": 4.110527515411377,
      "learning_rate": 1.6290586495810744e-05,
      "loss": 4.3729,
      "step": 9410
    },
    {
      "epoch": 0.9285362247412519,
      "grad_norm": 4.412356853485107,
      "learning_rate": 1.6286643666830952e-05,
      "loss": 4.4257,
      "step": 9420
    },
    {
      "epoch": 0.9295219319862001,
      "grad_norm": 4.694524765014648,
      "learning_rate": 1.628270083785116e-05,
      "loss": 4.2674,
      "step": 9430
    },
    {
      "epoch": 0.9305076392311483,
      "grad_norm": 4.529065132141113,
      "learning_rate": 1.6278758008871368e-05,
      "loss": 4.4559,
      "step": 9440
    },
    {
      "epoch": 0.9314933464760966,
      "grad_norm": 5.509743690490723,
      "learning_rate": 1.6274815179891573e-05,
      "loss": 4.3887,
      "step": 9450
    },
    {
      "epoch": 0.9324790537210449,
      "grad_norm": 5.829843044281006,
      "learning_rate": 1.627087235091178e-05,
      "loss": 4.3906,
      "step": 9460
    },
    {
      "epoch": 0.9334647609659931,
      "grad_norm": 4.65142822265625,
      "learning_rate": 1.6266929521931986e-05,
      "loss": 4.4207,
      "step": 9470
    },
    {
      "epoch": 0.9344504682109414,
      "grad_norm": 3.8724026679992676,
      "learning_rate": 1.6262986692952194e-05,
      "loss": 4.531,
      "step": 9480
    },
    {
      "epoch": 0.9354361754558896,
      "grad_norm": 4.327146053314209,
      "learning_rate": 1.6259043863972402e-05,
      "loss": 4.2767,
      "step": 9490
    },
    {
      "epoch": 0.9364218827008378,
      "grad_norm": 4.221595764160156,
      "learning_rate": 1.6255101034992607e-05,
      "loss": 4.4219,
      "step": 9500
    },
    {
      "epoch": 0.9374075899457861,
      "grad_norm": 3.8527956008911133,
      "learning_rate": 1.6251158206012815e-05,
      "loss": 4.3999,
      "step": 9510
    },
    {
      "epoch": 0.9383932971907344,
      "grad_norm": 5.017287731170654,
      "learning_rate": 1.6247215377033024e-05,
      "loss": 4.2865,
      "step": 9520
    },
    {
      "epoch": 0.9393790044356826,
      "grad_norm": 5.1457695960998535,
      "learning_rate": 1.6243272548053232e-05,
      "loss": 4.4888,
      "step": 9530
    },
    {
      "epoch": 0.9403647116806308,
      "grad_norm": 5.501786231994629,
      "learning_rate": 1.6239329719073437e-05,
      "loss": 4.3333,
      "step": 9540
    },
    {
      "epoch": 0.9413504189255791,
      "grad_norm": 4.053833961486816,
      "learning_rate": 1.623538689009364e-05,
      "loss": 4.2504,
      "step": 9550
    },
    {
      "epoch": 0.9423361261705273,
      "grad_norm": 3.802968740463257,
      "learning_rate": 1.623144406111385e-05,
      "loss": 4.3872,
      "step": 9560
    },
    {
      "epoch": 0.9433218334154756,
      "grad_norm": 4.85194730758667,
      "learning_rate": 1.6227501232134058e-05,
      "loss": 4.3361,
      "step": 9570
    },
    {
      "epoch": 0.9443075406604239,
      "grad_norm": 5.100406646728516,
      "learning_rate": 1.6223558403154266e-05,
      "loss": 4.2725,
      "step": 9580
    },
    {
      "epoch": 0.9452932479053721,
      "grad_norm": 4.13508415222168,
      "learning_rate": 1.621961557417447e-05,
      "loss": 4.5217,
      "step": 9590
    },
    {
      "epoch": 0.9462789551503203,
      "grad_norm": 4.088593482971191,
      "learning_rate": 1.621567274519468e-05,
      "loss": 4.5099,
      "step": 9600
    },
    {
      "epoch": 0.9472646623952686,
      "grad_norm": 4.893286228179932,
      "learning_rate": 1.6211729916214884e-05,
      "loss": 4.3333,
      "step": 9610
    },
    {
      "epoch": 0.9482503696402168,
      "grad_norm": 4.107420444488525,
      "learning_rate": 1.6207787087235092e-05,
      "loss": 4.4613,
      "step": 9620
    },
    {
      "epoch": 0.9492360768851651,
      "grad_norm": 4.057967185974121,
      "learning_rate": 1.62038442582553e-05,
      "loss": 4.3654,
      "step": 9630
    },
    {
      "epoch": 0.9502217841301134,
      "grad_norm": 4.20274543762207,
      "learning_rate": 1.6199901429275505e-05,
      "loss": 4.3535,
      "step": 9640
    },
    {
      "epoch": 0.9512074913750616,
      "grad_norm": 5.439868927001953,
      "learning_rate": 1.6195958600295713e-05,
      "loss": 4.3876,
      "step": 9650
    },
    {
      "epoch": 0.9521931986200098,
      "grad_norm": 4.886075496673584,
      "learning_rate": 1.619201577131592e-05,
      "loss": 4.4505,
      "step": 9660
    },
    {
      "epoch": 0.9531789058649581,
      "grad_norm": 4.355121612548828,
      "learning_rate": 1.618807294233613e-05,
      "loss": 4.4294,
      "step": 9670
    },
    {
      "epoch": 0.9541646131099064,
      "grad_norm": 4.177900791168213,
      "learning_rate": 1.6184130113356334e-05,
      "loss": 4.2706,
      "step": 9680
    },
    {
      "epoch": 0.9551503203548546,
      "grad_norm": 4.609010696411133,
      "learning_rate": 1.6180187284376542e-05,
      "loss": 4.514,
      "step": 9690
    },
    {
      "epoch": 0.9561360275998029,
      "grad_norm": 4.653597354888916,
      "learning_rate": 1.6176244455396747e-05,
      "loss": 4.2786,
      "step": 9700
    },
    {
      "epoch": 0.9571217348447512,
      "grad_norm": 4.021420478820801,
      "learning_rate": 1.6172301626416955e-05,
      "loss": 4.4044,
      "step": 9710
    },
    {
      "epoch": 0.9581074420896993,
      "grad_norm": 4.634300231933594,
      "learning_rate": 1.6168358797437163e-05,
      "loss": 4.5743,
      "step": 9720
    },
    {
      "epoch": 0.9590931493346476,
      "grad_norm": 4.279201507568359,
      "learning_rate": 1.6164415968457368e-05,
      "loss": 4.3051,
      "step": 9730
    },
    {
      "epoch": 0.9600788565795959,
      "grad_norm": 3.5778298377990723,
      "learning_rate": 1.6160473139477576e-05,
      "loss": 4.2735,
      "step": 9740
    },
    {
      "epoch": 0.9610645638245441,
      "grad_norm": 4.936531066894531,
      "learning_rate": 1.6156530310497785e-05,
      "loss": 4.3624,
      "step": 9750
    },
    {
      "epoch": 0.9620502710694924,
      "grad_norm": 3.3117356300354004,
      "learning_rate": 1.615258748151799e-05,
      "loss": 4.4147,
      "step": 9760
    },
    {
      "epoch": 0.9630359783144407,
      "grad_norm": 4.561831951141357,
      "learning_rate": 1.6148644652538198e-05,
      "loss": 4.3813,
      "step": 9770
    },
    {
      "epoch": 0.9640216855593888,
      "grad_norm": 5.2585673332214355,
      "learning_rate": 1.6144701823558406e-05,
      "loss": 4.4185,
      "step": 9780
    },
    {
      "epoch": 0.9650073928043371,
      "grad_norm": 4.422088146209717,
      "learning_rate": 1.614075899457861e-05,
      "loss": 4.3343,
      "step": 9790
    },
    {
      "epoch": 0.9659931000492854,
      "grad_norm": 4.432077407836914,
      "learning_rate": 1.613681616559882e-05,
      "loss": 4.3602,
      "step": 9800
    },
    {
      "epoch": 0.9669788072942336,
      "grad_norm": 3.7823617458343506,
      "learning_rate": 1.6132873336619027e-05,
      "loss": 4.1458,
      "step": 9810
    },
    {
      "epoch": 0.9679645145391819,
      "grad_norm": 4.739285469055176,
      "learning_rate": 1.6128930507639232e-05,
      "loss": 4.2873,
      "step": 9820
    },
    {
      "epoch": 0.9689502217841302,
      "grad_norm": 4.707936763763428,
      "learning_rate": 1.612498767865944e-05,
      "loss": 4.3792,
      "step": 9830
    },
    {
      "epoch": 0.9699359290290783,
      "grad_norm": 5.78579044342041,
      "learning_rate": 1.6121044849679645e-05,
      "loss": 4.2883,
      "step": 9840
    },
    {
      "epoch": 0.9709216362740266,
      "grad_norm": 3.9615695476531982,
      "learning_rate": 1.6117102020699853e-05,
      "loss": 4.3506,
      "step": 9850
    },
    {
      "epoch": 0.9719073435189749,
      "grad_norm": 4.850016117095947,
      "learning_rate": 1.611315919172006e-05,
      "loss": 4.3437,
      "step": 9860
    },
    {
      "epoch": 0.9728930507639231,
      "grad_norm": 4.368075847625732,
      "learning_rate": 1.610921636274027e-05,
      "loss": 4.4867,
      "step": 9870
    },
    {
      "epoch": 0.9738787580088714,
      "grad_norm": 3.5626542568206787,
      "learning_rate": 1.6105273533760474e-05,
      "loss": 4.2372,
      "step": 9880
    },
    {
      "epoch": 0.9748644652538196,
      "grad_norm": 4.499643325805664,
      "learning_rate": 1.6101330704780682e-05,
      "loss": 4.4662,
      "step": 9890
    },
    {
      "epoch": 0.9758501724987678,
      "grad_norm": 5.907257080078125,
      "learning_rate": 1.609738787580089e-05,
      "loss": 4.3246,
      "step": 9900
    },
    {
      "epoch": 0.9768358797437161,
      "grad_norm": 3.5794882774353027,
      "learning_rate": 1.6093445046821095e-05,
      "loss": 4.4036,
      "step": 9910
    },
    {
      "epoch": 0.9778215869886644,
      "grad_norm": 3.9258878231048584,
      "learning_rate": 1.6089502217841303e-05,
      "loss": 4.4219,
      "step": 9920
    },
    {
      "epoch": 0.9788072942336126,
      "grad_norm": 5.308004856109619,
      "learning_rate": 1.6085559388861508e-05,
      "loss": 4.233,
      "step": 9930
    },
    {
      "epoch": 0.9797930014785609,
      "grad_norm": 4.2683587074279785,
      "learning_rate": 1.6081616559881716e-05,
      "loss": 4.4043,
      "step": 9940
    },
    {
      "epoch": 0.9807787087235091,
      "grad_norm": 6.547760963439941,
      "learning_rate": 1.6077673730901925e-05,
      "loss": 4.4424,
      "step": 9950
    },
    {
      "epoch": 0.9817644159684573,
      "grad_norm": 6.109828948974609,
      "learning_rate": 1.6073730901922133e-05,
      "loss": 4.3589,
      "step": 9960
    },
    {
      "epoch": 0.9827501232134056,
      "grad_norm": 5.053562641143799,
      "learning_rate": 1.6069788072942337e-05,
      "loss": 4.268,
      "step": 9970
    },
    {
      "epoch": 0.9837358304583539,
      "grad_norm": 5.368679523468018,
      "learning_rate": 1.6065845243962542e-05,
      "loss": 4.3513,
      "step": 9980
    },
    {
      "epoch": 0.9847215377033022,
      "grad_norm": 4.840617656707764,
      "learning_rate": 1.606190241498275e-05,
      "loss": 4.3093,
      "step": 9990
    },
    {
      "epoch": 0.9857072449482503,
      "grad_norm": 4.615786075592041,
      "learning_rate": 1.605795958600296e-05,
      "loss": 4.53,
      "step": 10000
    }
  ],
  "logging_steps": 10,
  "max_steps": 50725,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 524058128547840.0,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
